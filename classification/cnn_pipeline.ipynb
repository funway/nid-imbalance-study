{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/funway/nid-imbalance-study/blob/main/classification/cnn_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNye-IlJ9foa"
      },
      "source": [
        "# CNN å·ç§¯ç¥ç»ç½‘ç»œ\n",
        "ğŸš€ NYIT 880 | ğŸ§‘ğŸ»â€ğŸ’» funway"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJktH4kaJky7"
      },
      "source": [
        "## Modules import & Globals setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qz-PuaY_Jljm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8183ab7-952a-4291-bb9a-6470e149ab74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[06/25/25 06:10:10] ğŸ·ï¸ Label mapping: {'Benign': 0, 'Bot': 1, 'Brute Force -Web': 2, 'Brute Force -XSS': 3, 'DDOS attack-HOIC': 4, 'DDOS attack-LOIC-UDP': 5, 'DDoS attacks-LOIC-HTTP': 6, 'DoS attacks-GoldenEye': 7, 'DoS attacks-Hulk': 8, 'DoS attacks-SlowHTTPTest': 9, 'DoS attacks-Slowloris': 10, 'FTP-BruteForce': 11, 'Infilteration': 12, 'SQL Injection': 13, 'SSH-Bruteforce': 14}\n",
            "å¯¼å…¥ utility.ipynb æ¨¡å—. version 1.0.1\n"
          ]
        }
      ],
      "source": [
        "### Modules ###\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "from io import StringIO\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "## mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "### Globals ###\n",
        "## æ•°æ®æ–‡ä»¶ç›®å½•\n",
        "dataset = 'cse-cic-ids2018'\n",
        "project_folder = Path('/content/drive/MyDrive/NYIT/880')\n",
        "preprocessed_folder = project_folder / 'data/preprocessed'\n",
        "scaled_folder = preprocessed_folder / 'scaled'\n",
        "splits_folder = preprocessed_folder / 'splits'\n",
        "balanced_folder = project_folder / 'data/balanced'\n",
        "cgan_folder = balanced_folder / 'models'\n",
        "model_folder = project_folder / 'data/classification/models'\n",
        "report_folder = project_folder / 'data/classification/reports'\n",
        "\n",
        "## Label åˆ—çš„æ‰€æœ‰å¯èƒ½å€¼(æœ‰åº)\n",
        "unique_labels = ['Benign', 'Bot', 'Brute Force -Web', 'Brute Force -XSS', 'DDOS attack-HOIC', 'DDOS attack-LOIC-UDP', 'DDoS attacks-LOIC-HTTP', 'DoS attacks-GoldenEye', 'DoS attacks-Hulk', 'DoS attacks-SlowHTTPTest', 'DoS attacks-Slowloris', 'FTP-BruteForce', 'Infilteration', 'SQL Injection', 'SSH-Bruteforce']\n",
        "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "print(f\"[{datetime.now().strftime('%x %X')}] ğŸ·ï¸ Label mapping: {label_mapping}\")\n",
        "\n",
        "### å…¨å±€éšæœºæ•°ç§å­ ###\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "op_seed = 42\n",
        "\n",
        "### Utilities ###\n",
        "# å¯¼å…¥ utility.ipynb æ¨¡å—\n",
        "%run /content/drive/MyDrive/NYIT/880/code/utils/utility.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## å¯è°ƒå‚æ•°"
      ],
      "metadata": {
        "id": "OEJ752Ov8lgb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "54NXnZ1cTC5_"
      },
      "outputs": [],
      "source": [
        "# æ˜¯å¦å¼ºåˆ¶é‡æ–°è®­ç»ƒåˆ†ç±»å™¨æ¨¡å‹\n",
        "retrain = False\n",
        "\n",
        "# é€‰æ‹© scaling æ–¹æ³•. å¯é€‰[standard, minmax, robust, l1pminmax]\n",
        "scaling_method = 'l1pminmax'\n",
        "\n",
        "# é€‰æ‹© resample çš„ç›®æ ‡æ¨¡å¼, 0 è¡¨ç¤ºæœªç»è¿‡ resample å¹³è¡¡å¤„ç†\n",
        "resample_scheme = 2\n",
        "\n",
        "# è¿‡é‡‡æ ·æ–¹æ³•\n",
        "cgan_model = 'cgan-b(n128,f70,c15,e100,b512,gen[128,[128, 256, 512],0.0003],dis[64,[256, 128],0.0001])_generator.keras'\n",
        "cgan_filter_strategy = 0\n",
        "cgan_filter_keep_high = True\n",
        "cgan_filter_mark = f\"f{'h' if cgan_filter_keep_high else 'l'}{cgan_filter_strategy}\" if cgan_filter_strategy else ''\n",
        "oversampling_method = cgan_model[:-16] + (f'f{cgan_filter_strategy}' if cgan_filter_strategy else '')\n",
        "\n",
        "# é€‰æ‹© æ¬ é‡‡æ · æ–¹æ³• (NA è¡¨ç¤ºä¸ä½¿ç”¨æ¬ é‡‡æ ·)\n",
        "undersampling_method = 'rus'\n",
        "\n",
        "# é€‰æ‹© åˆ†ç±»å™¨\n",
        "classifier = 'CNN'\n",
        "\n",
        "##########################\n",
        "if resample_scheme == 0:\n",
        "    oversampling_method = 'NA'\n",
        "    undersampling_method = 'NA'\n",
        "##########################\n",
        "\n",
        "# è®­ç»ƒç»“æŸåæœ€ç»ˆæ¨¡å‹ä¿å­˜è·¯å¾„\n",
        "model_file_final = model_folder / f'{dataset}_{scaling_method}_s{resample_scheme}_{oversampling_method}_{undersampling_method}_{classifier}-final.keras'\n",
        "\n",
        "# è®­ç»ƒè¿‡ç¨‹ä¸­æœ€ä½³æ¨¡å‹ä¿å­˜è·¯å¾„\n",
        "model_file_best  = model_folder / f'{dataset}_{scaling_method}_s{resample_scheme}_{oversampling_method}_{undersampling_method}_{classifier}-best.keras'\n",
        "\n",
        "# éœ€è¦è¯„ä¼°çš„æ¨¡å‹\n",
        "models_to_evaluate = [model_file_best, model_file_final]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data"
      ],
      "metadata": {
        "id": "nQz2XyRo7C3y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_4O7q_BTYFy",
        "outputId": "1cb2d115-9a41-4ea4-c8ce-0227f7c66a2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/24/25 23:10:14 PDT] Loading datasets...\n",
            "[06/24/25 23:10:15 PDT] âš ï¸ File not found: /content/drive/MyDrive/NYIT/880/data/balanced/train_X_l1pminmax_s2_cgan-b(n128,f70,c15,e100,b512,gen[128,[128, 256, 512],0.0003],dis[64,[256, 128],0.0001]).npy\n",
            "Load original files\n"
          ]
        }
      ],
      "source": [
        "if resample_scheme == 0:\n",
        "    file_X_train = splits_folder / f'train_X_{scaling_method}.npy'\n",
        "    file_y_train = splits_folder / f'train_y.npy'\n",
        "elif undersampling_method in ['NA', 'rus']:\n",
        "    file_X_train = balanced_folder / f'train_X_{scaling_method}_s{resample_scheme}_{oversampling_method}.npy'\n",
        "    file_y_train = balanced_folder / f'train_y_{scaling_method}_s{resample_scheme}_{oversampling_method}.npy'\n",
        "else:\n",
        "    file_X_train = balanced_folder / f'train_X_{scaling_method}_s{resample_scheme}_{oversampling_method}_{undersampling_method}.npy'\n",
        "    file_y_train = balanced_folder / f'train_y_{scaling_method}_s{resample_scheme}_{oversampling_method}_{undersampling_method}.npy'\n",
        "\n",
        "file_X_valid = splits_folder / f'valid_X_{scaling_method}.npy'\n",
        "file_y_valid = splits_folder / f'valid_y.npy'\n",
        "\n",
        "file_X_test = splits_folder / f'test_X_{scaling_method}.npy'\n",
        "file_y_test = splits_folder / f'test_y.npy'\n",
        "\n",
        "print(f\"[{now()}] Loading datasets...\")\n",
        "if file_X_train.exists():\n",
        "    X_train = np.load(file_X_train)\n",
        "    y_train = np.load(file_y_train)\n",
        "    neet_oversample = False\n",
        "else:\n",
        "    print(f'[{now()}] âš ï¸ File not found: {file_X_train}')\n",
        "    print('Load original files')\n",
        "    X_train = np.load(splits_folder / f'train_X_{scaling_method}.npy')\n",
        "    y_train = np.load(splits_folder / f'train_y.npy')\n",
        "    neet_oversample = True\n",
        "X_valid = np.load(file_X_valid)\n",
        "y_valid = np.load(file_y_valid)\n",
        "X_test = np.load(file_X_test)\n",
        "y_test = np.load(file_y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### é‡é‡‡æ ·å‡½æ•°å®šä¹‰"
      ],
      "metadata": {
        "id": "yMko8ofwMb1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_samples(generator: tf.keras.Model, target_class: int, num_samples: int):\n",
        "    \"\"\"\n",
        "    Generates samples using the generator for a specific target class.\n",
        "\n",
        "    Args:\n",
        "        generator (tensorflow.keras.Model): The generator model.\n",
        "        target_class (int): The target class for which to generate samples.\n",
        "        num_samples (int): The number of samples to generate.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Generated samples as a NumPy array.\n",
        "    \"\"\"\n",
        "    noise_dim = generator.input_shape[0][1]\n",
        "\n",
        "    # éšæœºç”Ÿæˆä¸€ç»„å™ªå£°å‘é‡ shape=(num_samples, noise_dim)\n",
        "    noise = np.random.normal(0, 1, size=(num_samples, noise_dim))\n",
        "\n",
        "    # éšæœºç”Ÿæˆä¸€ç»„ç±»åˆ«æ ‡ç­¾ shape=(num_samples, 1), å…¨éƒ¨ä¸º target_class\n",
        "    labels = np.full((num_samples, 1), fill_value=target_class, dtype=np.int32)\n",
        "\n",
        "    # ä½¿ç”¨ç”Ÿæˆå™¨ç”Ÿæˆæ•°æ®\n",
        "    generated_data = generator.predict([noise, labels], verbose=0)\n",
        "    return generated_data\n",
        "\n",
        "\n",
        "def cgan_undersample(cgan_discriminator: tf.keras.Model, sampling_strategy: dict, X: np.ndarray, y: np.ndarray, keep_high_score=True):\n",
        "    \"\"\"\n",
        "    ä½¿ç”¨ CGAN åˆ¤åˆ«å™¨å¯¹ç›®æ ‡æ•°æ®é›†è¿›è¡Œæ¬ é‡‡æ ·ï¼Œåˆ é™¤è¯„åˆ†ä½çš„æ•°æ®\n",
        "    \"\"\"\n",
        "    print(f'[{now()}] ğŸ“‰ CGAN Undersampling ...')\n",
        "    print(f'  original X.shaep: {X.shape}')\n",
        "    print(f'  original labels_counts: {get_label_counts(y)}')\n",
        "    print(f'  undersample to: {sampling_strategy}')\n",
        "\n",
        "    keep_idxs = []\n",
        "\n",
        "    for cls, target_n in sampling_strategy.items():\n",
        "        idxs = np.where(y == cls)[0]\n",
        "\n",
        "        # nothing to drop if already <= target\n",
        "        if len(idxs) <= target_n:\n",
        "            keep_idxs.extend(idxs.tolist())\n",
        "            print(f'[{now()}] Skipping class [{cls}]: {len(idxs)} â‰¤ {target_n}')\n",
        "            continue\n",
        "\n",
        "        # Score all samples of this cls\n",
        "        X_cls = X[idxs]\n",
        "        y_cls = y[idxs].reshape(-1, 1)\n",
        "        scores = cgan_discriminator([X_cls, y_cls], training=False)\n",
        "        scores = tf.reshape(scores, [-1]).numpy()\n",
        "\n",
        "        if keep_high_score:\n",
        "            # æŒ‰ç…§åˆ¤åˆ«å™¨è¯„åˆ†é™åºæ’åˆ—ï¼Œå–å‰ n ä¸ªä¿ç•™(ä¿ç•™è¯„åˆ†é«˜çš„)\n",
        "            top_idxs = idxs[np.argsort(scores)[::-1][:target_n]]\n",
        "        else:\n",
        "            # æŒ‰ç…§åˆ¤åˆ«å™¨è¯„åˆ†å‡åºæ’åˆ—ï¼Œå–å‰ n ä¸ªä¿ç•™(ä¿ç•™è¯„åˆ†ä½çš„)\n",
        "            top_idxs = idxs[np.argsort(scores)[:target_n]]\n",
        "\n",
        "        keep_idxs.extend(top_idxs.tolist())\n",
        "        print(f'[{now()}] Dropping {len(idxs) - target_n} samples for class [{cls}]: {len(idxs)} -> {target_n}')\n",
        "\n",
        "    # For any classes not in sampling_strategy, keep all\n",
        "    all_classes = set(np.unique(y))\n",
        "    leftover = all_classes - set(sampling_strategy.keys())\n",
        "    for cls in leftover:\n",
        "        keep_idxs.extend(np.where(y == cls)[0].tolist())\n",
        "\n",
        "    # produce final undersampled arrays\n",
        "    keep_idxs = np.sort(keep_idxs)\n",
        "    X_res = X[keep_idxs]\n",
        "    y_res = y[keep_idxs]\n",
        "\n",
        "    print(f'[{now()}] ğŸ“‰ After CGAN Undersampling:')\n",
        "    print(f'  X_res.shape: {X_res.shape}')\n",
        "    print(f'  Labels: {get_label_counts(y_res)}')\n",
        "\n",
        "    return X_res, y_res\n",
        "\n",
        "def cgan_oversample(cgan_generator: tf.keras.Model, sampling_strategy: dict, X: np.ndarray, y: np.ndarray):\n",
        "    current_counts = get_label_counts(y)\n",
        "\n",
        "    print(f'[{now()}] ğŸ“ˆ CGAN Oversampling ...')\n",
        "    print(f'  original X.shaep: {X.shape}')\n",
        "    print(f'  original labels_counts: {current_counts}')\n",
        "    print(f'  oversample to: {sampling_strategy}')\n",
        "\n",
        "    all_X = [X]\n",
        "    all_y = [y]\n",
        "\n",
        "    for cls, desired_n in sampling_strategy.items():\n",
        "        current_n = current_counts.get(cls, 0)\n",
        "        n_to_gen = desired_n - current_n\n",
        "        if n_to_gen > 0:\n",
        "            print(f'[{now()}] Generating {n_to_gen} samples for class [{cls}]: {current_n} -> {desired_n}')\n",
        "            gen_samples = generate_samples(cgan_generator, cls, n_to_gen)\n",
        "            all_X.append(gen_samples)\n",
        "            all_y.append(np.full(n_to_gen, cls, dtype=np.int32))\n",
        "        else:\n",
        "            print(f'[{now()}] Skipping class [{cls}]: {current_n} â‰¥ {desired_n}')\n",
        "\n",
        "    X_res = np.concatenate(all_X)\n",
        "    y_res = np.concatenate(all_y)\n",
        "\n",
        "    print(f'[{now()}] ğŸ“ˆ After CGAN Oversampling:')\n",
        "    print(f'  X_res.shape: {X_res.shape}')\n",
        "    print(f'  Labels: {get_label_counts(y_res)}')\n",
        "\n",
        "    return X_res, y_res"
      ],
      "metadata": {
        "id": "GML7aC9nMlsY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### è¿‡é‡‡æ ·"
      ],
      "metadata": {
        "id": "FWO8ThhnMgRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if neet_oversample:\n",
        "    print(f\"[{now()}] ğŸ“ˆ Oversampling with {oversampling_method}\")\n",
        "\n",
        "    # ROS è¿‡é‡‡æ ·\n",
        "    from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "    # åˆ¤æ–­ oversampling_method å­—ç¬¦ä¸²å¼€å¤´æ˜¯å¦ä¸º ros\n",
        "    if oversampling_method.startswith('ros'):\n",
        "        ros_scheme = int(oversampling_method[3])\n",
        "        oversample_to = ros_schemes[ros_scheme]\n",
        "        print(f'[{now()}] Applying ROS oversampling to: {oversample_to}')\n",
        "\n",
        "        oversampler = RandomOverSampler(sampling_strategy=oversample_to, random_state=op_seed)\n",
        "        X_train, y_train = oversampler.fit_resample(X_train, y_train)\n",
        "\n",
        "        print(f'[{now()}] After ROS oversampling:')\n",
        "        print(f'  X.shape: {X_train.shape}, y.shape: {y_train.shape}')\n",
        "        print(f'  Labels: { {int(k): v for k, v in sorted(Counter(y_train).items())} }\\n')\n",
        "    else:\n",
        "        print(f'[{now()}] No need to apply ROS oversampling.')\n",
        "\n",
        "\n",
        "    # åŠ è½½ CGAN\n",
        "    generator_file = cgan_folder / scaling_method / cgan_model\n",
        "    discriminator_file = cgan_folder / scaling_method / cgan_model.replace('generator', 'discriminator')\n",
        "\n",
        "    print(f\"[{now()}] ğŸ“¡ Loading pre-trained generator from {generator_file}\")\n",
        "    generator = tf.keras.models.load_model(generator_file)\n",
        "\n",
        "    print(f\"[{now()}] ğŸ“¡ Loading pre-trained discriminator from {discriminator_file}\")\n",
        "    discriminator = tf.keras.models.load_model(discriminator_file)\n",
        "\n",
        "\n",
        "    # CGAN æ¬ é‡‡æ · (å¦‚æœéœ€è¦çš„è¯)\n",
        "    if cgan_filter_strategy:\n",
        "        print(f'[{now()}] ğŸŸ¡ Apply CGAN Undersampling.')\n",
        "        X_train, y_train = cgan_undersample(discriminator, cgan_filter_schemes[cgan_filter_strategy], X_train, y_train)\n",
        "\n",
        "\n",
        "    # CGAN è¿‡é‡‡æ ·\n",
        "    print(f'[{now()}] ğŸŸ¢ Apply CGAN Oversampling.')\n",
        "    resample_to = resample_schemes[resample_scheme]\n",
        "    X_train, y_train = cgan_oversample(generator, resample_to, X_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3q8c32TkLn6y",
        "outputId": "889d81ab-21c3-4b3a-9fb8-6b16b81a6f67"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/24/25 23:10:43 PDT] ğŸ“ˆ Oversampling with cgan-b(n128,f70,c15,e100,b512,gen[128,[128, 256, 512],0.0003],dis[64,[256, 128],0.0001])\n",
            "[06/24/25 23:10:44 PDT] No need to apply ROS oversampling.\n",
            "[06/24/25 23:10:44 PDT] ğŸ“¡ Loading pre-trained generator from /content/drive/MyDrive/NYIT/880/data/balanced/models/l1pminmax/cgan-b(n128,f70,c15,e100,b512,gen[128,[128, 256, 512],0.0003],dis[64,[256, 128],0.0001])_generator.keras\n",
            "[06/24/25 23:10:50 PDT] ğŸ“¡ Loading pre-trained discriminator from /content/drive/MyDrive/NYIT/880/data/balanced/models/l1pminmax/cgan-b(n128,f70,c15,e100,b512,gen[128,[128, 256, 512],0.0003],dis[64,[256, 128],0.0001])_discriminator.keras\n",
            "[06/24/25 23:10:51 PDT] ğŸŸ¢ Apply CGAN Oversampling.\n",
            "[06/24/25 23:10:52 PDT] ğŸ“ˆ CGAN Oversampling ...\n",
            "  original X.shaep: (3797547, 70)\n",
            "  original labels_counts: {0: 1600000, 1: 228953, 2: 489, 3: 184, 4: 548809, 5: 1384, 6: 460953, 7: 33206, 8: 369530, 9: 111912, 10: 8792, 11: 154683, 12: 128511, 13: 70, 14: 150071}\n",
            "  oversample to: {0: 800000, 1: 200000, 2: 50000, 3: 50000, 4: 200000, 5: 50000, 6: 200000, 7: 100000, 8: 200000, 9: 110000, 10: 50000, 11: 150000, 12: 120000, 13: 50000, 14: 150000}\n",
            "[06/24/25 23:10:52 PDT] Skipping class [0]: 1600000 â‰¥ 800000\n",
            "[06/24/25 23:10:52 PDT] Skipping class [1]: 228953 â‰¥ 200000\n",
            "[06/24/25 23:10:52 PDT] Generating 49511 samples for class [2]: 489 -> 50000\n",
            "[06/24/25 23:10:56 PDT] Generating 49816 samples for class [3]: 184 -> 50000\n",
            "[06/24/25 23:11:00 PDT] Skipping class [4]: 548809 â‰¥ 200000\n",
            "[06/24/25 23:11:00 PDT] Generating 48616 samples for class [5]: 1384 -> 50000\n",
            "[06/24/25 23:11:04 PDT] Skipping class [6]: 460953 â‰¥ 200000\n",
            "[06/24/25 23:11:04 PDT] Generating 66794 samples for class [7]: 33206 -> 100000\n",
            "[06/24/25 23:11:09 PDT] Skipping class [8]: 369530 â‰¥ 200000\n",
            "[06/24/25 23:11:09 PDT] Skipping class [9]: 111912 â‰¥ 110000\n",
            "[06/24/25 23:11:09 PDT] Generating 41208 samples for class [10]: 8792 -> 50000\n",
            "[06/24/25 23:11:12 PDT] Skipping class [11]: 154683 â‰¥ 150000\n",
            "[06/24/25 23:11:12 PDT] Skipping class [12]: 128511 â‰¥ 120000\n",
            "[06/24/25 23:11:12 PDT] Generating 49930 samples for class [13]: 70 -> 50000\n",
            "[06/24/25 23:11:15 PDT] Skipping class [14]: 150071 â‰¥ 150000\n",
            "[06/24/25 23:11:15 PDT] ğŸ“ˆ After CGAN Oversampling:\n",
            "  X_res.shape: (4103422, 70)\n",
            "  Labels: {0: 1600000, 1: 228953, 2: 50000, 3: 50000, 4: 548809, 5: 50000, 6: 460953, 7: 100000, 8: 369530, 9: 111912, 10: 50000, 11: 154683, 12: 128511, 13: 50000, 14: 150071}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### æ¬ é‡‡æ ·"
      ],
      "metadata": {
        "id": "1t_MCf_bMiUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## å¤„ç† RUS æ¬ é‡‡æ ·\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "label_counts = get_label_counts(y_train)\n",
        "resample_to = resample_schemes[resample_scheme]\n",
        "undersample_to = {\n",
        "    k: resample_to[k]\n",
        "    for k in resample_to\n",
        "    if resample_to[k] < label_counts.get(k, 0)\n",
        "}\n",
        "\n",
        "print(f\"[{now()}] å½“å‰æ ·æœ¬æ•°: {X_train.shape}, {label_counts}\")\n",
        "print(f\"[{now()}] Undersample to: {undersample_to}\")\n",
        "\n",
        "if undersampling_method == 'rus':\n",
        "    print(f\"[{now()}] ğŸ“‰ ä½¿ç”¨ RUS æ¬ é‡‡æ ·\")\n",
        "\n",
        "    rus = RandomUnderSampler(sampling_strategy=undersample_to, random_state=op_seed)\n",
        "    X_train, y_train = rus.fit_resample(X_train, y_train)\n",
        "\n",
        "elif undersampling_method == 'cgan':\n",
        "    print(f\"[{now()}] ğŸ“‰ ä½¿ç”¨ CGAN æ¬ é‡‡æ ·\")\n",
        "    X_train, y_train = cgan_undersample(discriminator, undersample_to, X_train, y_train)\n",
        "\n",
        "elif undersampling_method == 'cganlow':\n",
        "    print(f\"[{now()}] ğŸ“‰ ä½¿ç”¨ CGAN æ¬ é‡‡æ · (ä¿ç•™è¯„åˆ†ä½çš„)\")\n",
        "    X_train, y_train = cgan_undersample(discriminator, undersample_to, X_train, y_train, keep_high_score=False)\n",
        "\n",
        "elif undersampling_method == 'iht':\n",
        "    print(f\"[{now()}] ğŸ“‰ ä½¿ç”¨ IHT æ¬ é‡‡æ ·\")\n",
        "    from imblearn.under_sampling import InstanceHardnessThreshold\n",
        "    from xgboost import XGBClassifier\n",
        "\n",
        "    # ä½¿ç”¨ GPU åŠ é€Ÿçš„ XGBClassifier ä½œä¸º IHT çš„åˆ†ç±»å™¨\n",
        "    iht = InstanceHardnessThreshold(\n",
        "        estimator=XGBClassifier(tree_method='hist', device='cuda', n_estimators=200, max_depth=5, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, eval_metric='mlogloss'),\n",
        "        sampling_strategy={9: 100_000, 12: 110_000},\n",
        "        random_state=op_seed)\n",
        "    X_train, y_train = iht.fit_resample(X_train, y_train)\n",
        "    print(f'[{now()}] After IHT Undersampling: {X_train.shape}, {get_label_counts(y_train)}')\n",
        "\n",
        "    # IHT æ— æ³•ä¿è¯æ¬ é‡‡æ ·åˆ°ç›®æ ‡æ•°é‡ï¼Œæ‰€ä»¥è¦å†è¡¥ä¸€ä¸ª RUS\n",
        "    rus = RandomUnderSampler(sampling_strategy=undersample_to, random_state=op_seed)\n",
        "    X_train, y_train = rus.fit_resample(X_train, y_train)\n",
        "\n",
        "elif undersampling_method == 'NA':\n",
        "    print(f\"[{now()}] ä¸è¿›è¡Œæ¬ é‡‡æ ·\")\n",
        "\n",
        "else:\n",
        "    raise Exception(f'[{now()}] âŒ æœªå®šä¹‰çš„æ¬ é‡‡æ ·æ–¹æ³•: {undersampling_method}')\n",
        "\n",
        "print(f\"[{now()}] æ¬ é‡‡æ ·å®Œæˆ: {X_train.shape}, {get_label_counts(y_train)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SI06twDp53Nm",
        "outputId": "33114e7b-02a5-4cfd-ca52-0b99c55eb673"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/24/25 23:11:17 PDT] å½“å‰æ ·æœ¬æ•°: (4103422, 70), {0: 1600000, 1: 228953, 2: 50000, 3: 50000, 4: 548809, 5: 50000, 6: 460953, 7: 100000, 8: 369530, 9: 111912, 10: 50000, 11: 154683, 12: 128511, 13: 50000, 14: 150071}\n",
            "[06/24/25 23:11:17 PDT] Undersample to: {0: 800000, 1: 200000, 4: 200000, 6: 200000, 8: 200000, 9: 110000, 11: 150000, 12: 120000, 14: 150000}\n",
            "[06/24/25 23:11:17 PDT] ğŸ“‰ ä½¿ç”¨ RUS æ¬ é‡‡æ ·\n",
            "[06/24/25 23:11:18 PDT] æ¬ é‡‡æ ·å®Œæˆ: (2480000, 70), {0: 800000, 1: 200000, 2: 50000, 3: 50000, 4: 200000, 5: 50000, 6: 200000, 7: 100000, 8: 200000, 9: 110000, 10: 50000, 11: 150000, 12: 120000, 13: 50000, 14: 150000}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "report_dataset = ''\n",
        "report_dataset += f'\\nTrain set: {file_X_train.name}, {file_y_train.name} \\n'\n",
        "report_dataset += f'    shape: {X_train.shape}, {y_train.shape} \\n'\n",
        "report_dataset += f'    labels: {get_label_counts(y_train)} \\n'\n",
        "report_dataset += f'\\nValid set: {file_X_valid.name}, {file_y_valid.name} \\n'\n",
        "report_dataset += f'    shape: {X_valid.shape}, {y_valid.shape} \\n'\n",
        "report_dataset += f'    labels: {get_label_counts(y_valid)} \\n'\n",
        "report_dataset += f'\\nTest set: {file_X_test.name}, {file_y_test.name} \\n'\n",
        "report_dataset += f'    shape: {X_test.shape}, {y_test.shape} \\n'\n",
        "report_dataset += f'    labels: {get_label_counts(y_test)} \\n'\n",
        "\n",
        "print(report_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni7rOH24Kf4q",
        "outputId": "a33aede6-f659-4a8d-b623-3f2581bf7d60"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train set: train_X_l1pminmax_s2_cgan-b(n128,f70,c15,e100,b512,gen[128,[128, 256, 512],0.0003],dis[64,[256, 128],0.0001]).npy, train_y_l1pminmax_s2_cgan-b(n128,f70,c15,e100,b512,gen[128,[128, 256, 512],0.0003],dis[64,[256, 128],0.0001]).npy \n",
            "    shape: (2480000, 70), (2480000,) \n",
            "    labels: {0: 800000, 1: 200000, 2: 50000, 3: 50000, 4: 200000, 5: 50000, 6: 200000, 7: 100000, 8: 200000, 9: 110000, 10: 50000, 11: 150000, 12: 120000, 13: 50000, 14: 150000} \n",
            "\n",
            "Valid set: valid_X_l1pminmax.npy, valid_y.npy \n",
            "    shape: (474693, 70), (474693,) \n",
            "    labels: {0: 200000, 1: 28619, 2: 61, 3: 23, 4: 68601, 5: 173, 6: 57619, 7: 4151, 8: 46191, 9: 13989, 10: 1099, 11: 19335, 12: 16064, 13: 9, 14: 18759} \n",
            "\n",
            "Test set: test_X_l1pminmax.npy, test_y.npy \n",
            "    shape: (474694, 70), (474694,) \n",
            "    labels: {0: 200000, 1: 28619, 2: 61, 3: 23, 4: 68602, 5: 173, 6: 57619, 7: 4151, 8: 46191, 9: 13989, 10: 1099, 11: 19336, 12: 16064, 13: 8, 14: 18759} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OzbmCahI4UPt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c4c5844-7bd9-4b5c-d766-20d99cd9e42f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/24/25 23:11:19 PDT] å‡ç»´ä¹‹å: (2480000, 70, 1) (474693, 70, 1) (474694, 70, 1)\n",
            "[06/24/25 23:11:19 PDT] After one-hot: (2480000, 15) (474693, 15) (474694, 15)\n"
          ]
        }
      ],
      "source": [
        "## ç»™ç‰¹å¾çŸ©é˜µXå¢åŠ çº¬åº¦ï¼Œæ ‡ç­¾å‘é‡yè¿›è¡Œç‹¬çƒ­ç¼–ç \n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "if X_train.ndim == 2:  # åªåœ¨åŸå§‹äºŒç»´æ—¶åŠ ç»´åº¦\n",
        "    # ä¸ºæ¯ä¸ªç‰¹å¾çŸ©é˜µå¢åŠ ä¸€ä¸ªçº¬åº¦(CNN å·ç§¯ç¥ç»ç½‘ç»œä¸­çš„é€šé“)\n",
        "    X_train = np.expand_dims(X_train, 2)\n",
        "    X_valid = np.expand_dims(X_valid, 2)\n",
        "    X_test = np.expand_dims(X_test, 2)\n",
        "\n",
        "    # å¯¹æ ‡ç­¾æ•°æ®è¿›è¡Œç‹¬çƒ­ç¼–ç  one-hot\n",
        "    y_train = to_categorical(y_train)\n",
        "    y_valid = to_categorical(y_valid)\n",
        "    y_test = to_categorical(y_test)\n",
        "\n",
        "print(f\"[{now()}] å‡ç»´ä¹‹å:\", X_train.shape, X_valid.shape, X_test.shape)\n",
        "print(f\"[{now()}] After one-hot:\", y_train.shape, y_valid.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uTYOH66Wwtm"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9W3zg2ezZz5V"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "\n",
        "def train_model(x_train, y_train, x_val, y_val, batch_size=256, epochs=100):\n",
        "    \"\"\"æ¨¡å‹è®­ç»ƒå‡½æ•°\n",
        "\n",
        "    Args:\n",
        "        x_train (_type_): è®­ç»ƒé›†ç‰¹å¾çŸ©é˜µ\n",
        "        y_train (_type_): è®­ç»ƒé›†æ ‡ç­¾\n",
        "        x_val (_type_): éªŒè¯é›†ç‰¹å¾çŸ©é˜µ\n",
        "        y_val (_type_): éªŒè¯é›†æ ‡ç­¾\n",
        "        batch_size (int, optional): _description_. Defaults to 256.\n",
        "        epochs (int, optional): _description_. Defaults to 100.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. è¾“å…¥å±‚ï¼ŒæŒ‡å®šè¾“å…¥æ•°æ®çš„å½¢çŠ¶ (features, depth)\n",
        "    input_signal = tf.keras.Input(x_train.shape[1:])\n",
        "\n",
        "    # 2. ç¬¬ä¸€ç»„å·ç§¯å±‚ï¼Œä½¿ç”¨ 32 ä¸ªå·ç§¯æ ¸ï¼Œå¤§å°ä¸º 3ï¼Œæ¿€æ´»å‡½æ•°ä¸º ReLUï¼Œå¡«å……æ–¹å¼ä¸º same, åˆå§‹åŒ–æ–¹æ³•ä¸º he_uniform\n",
        "    x = layers.Conv1D(32, 3, activation='relu', padding='same', kernel_initializer='he_uniform')(input_signal)\n",
        "    x = layers.Conv1D(32, 3, activation='relu', padding='same', kernel_initializer='he_uniform')(x)\n",
        "    x = layers.MaxPooling1D(pool_size=2, strides=2)(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # 3. ç¬¬äºŒç»„å·ç§¯å±‚ï¼Œä½¿ç”¨ 64 ä¸ªå·ç§¯æ ¸ï¼Œå¤§å°ä¸º 3ï¼Œæ¿€æ´»å‡½æ•°ä¸º ReLUï¼Œå¡«å……æ–¹å¼ä¸º same, åˆå§‹åŒ–æ–¹æ³•ä¸º he_uniform\n",
        "    x = layers.Conv1D(64, 3, activation='relu', padding='same', kernel_initializer='he_uniform')(x)\n",
        "    x = layers.Conv1D(64, 3, activation='relu', padding='same', kernel_initializer='he_uniform')(x)\n",
        "    x = layers.MaxPooling1D(pool_size=2, strides=2)(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # 4. å…¨è¿æ¥å±‚, å°†å·ç§¯å±‚çš„è¾“å‡ºå±•å¹³\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(32, activation='relu')(x)\n",
        "    x = layers.Dense(y_train.shape[1], activation='softmax')(x)  # y_train.shape[1] æ˜¯ç±»åˆ«æ•°\n",
        "\n",
        "    # 5. åˆ›å»ºæ¨¡å‹\n",
        "    model = models.Model(inputs=input_signal, outputs=x)\n",
        "\n",
        "    # 6. æ‰“å°æ¨¡å‹ä¿¡æ¯\n",
        "    print(f\"[{now()}] model summary:\")\n",
        "    model.summary()\n",
        "\n",
        "    # 7. å®šä¹‰ä¼˜åŒ–å™¨\n",
        "    # learning_rate æŒ‡å®šæ­¥é•¿å¤§å°ï¼Œè¶Šå¤§åˆ™æ”¶æ•›è¶Šå¿«ï¼Œä½†è¶Šä¸ç¨³å®šã€‚é»˜è®¤å€¼ä¸º 0.001\n",
        "    # å…¶ä»–å‚æ•°ä¿æŒé»˜è®¤å€¼å³å¯\n",
        "    learning_rate = 0.001\n",
        "    nadam = Nadam(learning_rate=learning_rate)\n",
        "\n",
        "    # 8. å®šä¹‰ callbacks æ–¹æ³• (æ¯ä¸ª epoch è®­ç»ƒç»“æŸåè°ƒç”¨)\n",
        "    callbacks = [\n",
        "        # â¹ï¸ æå‰åœæ­¢è®­ç»ƒï¼šå¦‚æœéªŒè¯é›† loss è¿ç»­ 15 è½®æ²¡æœ‰ä¸‹é™ï¼Œå°±åœæ­¢è®­ç»ƒï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆ\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',          # ç›‘æ§æŒ‡æ ‡æ˜¯éªŒè¯é›†çš„ loss\n",
        "            min_delta=1e-6,              #\n",
        "            patience=15,                 # å®¹å¿ 15 ä¸ª epoch ä¸è¿›æ­¥\n",
        "            restore_best_weights=True    # è‡ªåŠ¨å›æ»šåˆ°éªŒè¯é›† loss æœ€ä¼˜çš„æ¨¡å‹å‚æ•°\n",
        "        ),\n",
        "\n",
        "        # ğŸ”½ è‡ªåŠ¨è°ƒæ•´å­¦ä¹ ç‡ï¼šå¦‚æœéªŒè¯é›† loss åœæ» 10 ä¸ª epochï¼Œå°±å°†å­¦ä¹ ç‡ä¹˜ä»¥ 0.1\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',         # ç›‘æ§éªŒè¯é›† loss\n",
        "            factor=0.1,                 # å­¦ä¹ ç‡é™ä½ä¸ºåŸæ¥çš„ 1/10\n",
        "            patience=10                 # å®¹å¿å¤šå°‘ä¸ª epoch æ— è¿›æ­¥\n",
        "        ),\n",
        "\n",
        "        # ğŸ’¾ æ¨¡å‹ä¿å­˜å™¨ï¼šåœ¨éªŒè¯é›† accuracy æœ€ä½³æ—¶ä¿å­˜æ•´ä¸ªæ¨¡å‹åˆ°æ–‡ä»¶\n",
        "        ModelCheckpoint(\n",
        "            filepath=model_file_best,  # æ¨¡å‹ä¿å­˜è·¯å¾„\n",
        "            monitor='val_accuracy',     # ä¿å­˜ä¾æ®æ˜¯éªŒè¯é›†å‡†ç¡®ç‡\n",
        "            save_best_only=True         # åªåœ¨ val_accuracy æœ€ä½³æ—¶æ‰ä¿å­˜\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # 9. ç¼–è¯‘æ¨¡å‹\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=nadam,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # 10. å¼€å§‹è®­ç»ƒæ¨¡å‹\n",
        "    time_start = datetime.now()\n",
        "    print(f\"[{now()}] ğŸ‹ï¸â€â™‚ï¸ Training model...\")\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        batch_size=batch_size, epochs=epochs,\n",
        "                        validation_data=(x_val, y_val),\n",
        "                        verbose=1,\n",
        "                        callbacks=callbacks)\n",
        "    time_end = datetime.now()\n",
        "    print(f\"[{now()}] ğŸ•— Training completed. Time elapsed: {time_end - time_start}\")\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFyUvc4UMLsg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 967
        },
        "outputId": "8d55f689-5c57-4704-8d6a-7e46889ef983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/24/25 23:11:21 PDT] ğŸ¤– build and train model (cse-cic-ids2018_l1pminmax_s2_cgan-b(n128,f70,c15,e100,b512,gen[128,[128, 256, 512],0.0003],dis[64,[256, 128],0.0001])_rus_CNN-final.keras)...\n",
            "[06/24/25 23:11:21 PDT] model summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m, \u001b[38;5;34m1\u001b[0m)          â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m, \u001b[38;5;34m32\u001b[0m)         â”‚           \u001b[38;5;34m128\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m, \u001b[38;5;34m32\u001b[0m)         â”‚         \u001b[38;5;34m3,104\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m32\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout (\u001b[38;5;33mDropout\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m32\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m32\u001b[0m)         â”‚           \u001b[38;5;34m128\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚         \u001b[38;5;34m6,208\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚        \u001b[38;5;34m12,352\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization_1           â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m64\u001b[0m)         â”‚           \u001b[38;5;34m256\u001b[0m â”‚\n",
              "â”‚ (\u001b[38;5;33mBatchNormalization\u001b[0m)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ flatten (\u001b[38;5;33mFlatten\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1088\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             â”‚        \u001b[38;5;34m34,848\u001b[0m â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)             â”‚           \u001b[38;5;34m495\u001b[0m â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
              "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
              "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
              "â”‚ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)          â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,104</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,208</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ batch_normalization_1           â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> â”‚\n",
              "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            â”‚                        â”‚               â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1088</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">34,848</span> â”‚\n",
              "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
              "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">495</span> â”‚\n",
              "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m57,519\u001b[0m (224.68 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">57,519</span> (224.68 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m57,327\u001b[0m (223.93 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">57,327</span> (223.93 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m192\u001b[0m (768.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> (768.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/24/25 23:11:21 PDT] ğŸ‹ï¸â€â™‚ï¸ Training model...\n",
            "Epoch 1/100\n",
            "\u001b[1m9688/9688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 4ms/step - accuracy: 0.9043 - loss: 0.2390 - val_accuracy: 0.9468 - val_loss: 0.1358 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m9688/9688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 3ms/step - accuracy: 0.9251 - loss: 0.1697 - val_accuracy: 0.9495 - val_loss: 0.1349 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m9688/9688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 3ms/step - accuracy: 0.9263 - loss: 0.1668 - val_accuracy: 0.9485 - val_loss: 0.1344 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m9688/9688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 3ms/step - accuracy: 0.9270 - loss: 0.1652 - val_accuracy: 0.9482 - val_loss: 0.1340 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m9688/9688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 3ms/step - accuracy: 0.9274 - loss: 0.1641 - val_accuracy: 0.9490 - val_loss: 0.1336 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m9688/9688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 3ms/step - accuracy: 0.9276 - loss: 0.1635 - val_accuracy: 0.9497 - val_loss: 0.1327 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m9688/9688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 3ms/step - accuracy: 0.9280 - loss: 0.1626 - val_accuracy: 0.9485 - val_loss: 0.1322 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m9675/9688\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9285 - loss: 0.1619"
          ]
        }
      ],
      "source": [
        "model_file = model_file_final\n",
        "\n",
        "if model_file.exists() and not retrain:\n",
        "    print(f\"[{now()}] â­ï¸ æ¨¡å‹å·²å­˜åœ¨ï¼Œä¸é‡æ–°è®­ç»ƒ ({model_file})\")\n",
        "else:\n",
        "    # è®­ç»ƒæ¨¡å‹\n",
        "    print(f\"[{now()}] ğŸ¤– build and train model ({model_file.name})...\")\n",
        "    model, history = train_model(X_train, y_train, X_valid, y_valid)\n",
        "\n",
        "    # æ‰“å°history\n",
        "    print(f\"[{now()}] history:\\n  {history.history}\")\n",
        "\n",
        "    # ä¿å­˜æ¨¡å‹\n",
        "    model.save(model_file)\n",
        "    print(f\"[{now()}] ğŸ’¾ model saved to {model_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruMKfG2mXAWr"
      },
      "source": [
        "## Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# å¯¼å…¥ evaluation.ipynb æ¨¡å—\n",
        "%run /content/drive/MyDrive/NYIT/880/code/utils/evaluation.ipynb\n",
        "\n",
        "def get_model_summary_string(model):\n",
        "    \"\"\"è·å– Keras æ¨¡å‹çš„æ‘˜è¦å­—ç¬¦ä¸²\"\"\"\n",
        "    stream = StringIO()\n",
        "    model.summary(print_fn=lambda x: stream.write(x + '\\n'))\n",
        "    summary_str = stream.getvalue()\n",
        "    stream.close()\n",
        "    return summary_str\n",
        "\n",
        "for model_file in models_to_evaluate:\n",
        "    print(f\"[{now()}] âš™ï¸ Loading model from {model_file}\")\n",
        "    model = models.load_model(model_file)\n",
        "\n",
        "    # æ¨¡å‹ä¿¡æ¯\n",
        "    report_model_info = f\"[{now()}] ğŸ¤– Model: {model_file.name}\\n\"\n",
        "    report_model_info += f\"[{now()}] ================= ğŸ§  Model Info =================\\n\"\n",
        "    report_model_info += f\"Model summary: {get_model_summary_string(model)}\"\n",
        "    print(report_model_info)\n",
        "\n",
        "    # å†…å»ºçš„ evaluate è¯„ä¼°\n",
        "    # report_model_info += f\"[{now()}] ================= ğŸ”¶ tf.keras.Model.evaluate() =================\\n\"\n",
        "    # results = model.evaluate(X_test, y_test, verbose=1, return_dict=True)\n",
        "    # report_model_info += f\"{results}\\n\"\n",
        "\n",
        "    # è·å–é¢„æµ‹ç»“æœï¼ˆæ¦‚ç‡ï¼‰ï¼Œå†è½¬ä¸ºç±»åˆ«ç´¢å¼•\n",
        "    y_pred_probs = model.predict(X_test, verbose=1)  # å¾—åˆ°çš„ä¸€ä¸ªçŸ©é˜µï¼Œæ¯è¡Œä»£è¡¨ä¸€ä¸ªæ ·æœ¬å±äºæ¯ä¸ªç±»åˆ«çš„æ¦‚ç‡. æ¯”å¦‚[0.1, 0.7, 0.2]å°±è¡¨ç¤ºè¯¥æ ·æœ¬æœ‰0.1çš„æ¦‚ç‡å±äºç±»åˆ«0, 0.7çš„æ¦‚ç‡å±äºç±»åˆ«1\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)  # å¯¹æ¯è¡Œæå–å€¼æœ€å¤§çš„åˆ—(æå–å‡ºæ¯ä¸ªæ ·æœ¬çš„é¢„æµ‹ç±»åˆ«)\n",
        "    y_true = np.argmax(y_test, axis=1)  # æå–å‡ºæ¯ä¸ªæ ·æœ¬çš„çœŸå®ç±»åˆ«\n",
        "\n",
        "    # ç”Ÿæˆè¯„ä¼°æŠ¥å‘Š\n",
        "    report_eval = generate_evaluation_report(y_true=y_true, y_predict=y_pred,\n",
        "                                             label_mapping=label_mapping,\n",
        "                                             figure_output=report_folder / 'png' / f'{model_file.stem}_confusion_matrix.png',\n",
        "                                             figure_show=True)\n",
        "    print(report_eval)\n",
        "\n",
        "    # ä¿å­˜æŠ¥å‘Š\n",
        "    report_file = report_folder / f'{model_file.stem}_report.txt'\n",
        "    report_all = report_dataset + '\\n' + report_model_info + '\\n' + report_eval\n",
        "    with open(report_file, 'w') as f:\n",
        "        f.write(report_all)\n",
        "        print(f\"[{now()}] ğŸ’¾ Evaluation report saved to {report_file}\")\n",
        "    print('============================================================')"
      ],
      "metadata": {
        "id": "q3_hHyk9tjyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "print(f'[{now()}] â›” è¿è¡Œç»“æŸ. shutdown now...')\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "YppT-rp0qI8F"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}