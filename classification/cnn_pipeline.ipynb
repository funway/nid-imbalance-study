{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/funway/nid-imbalance-study/blob/main/classification/cnn_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pNye-IlJ9foa"
      },
      "source": [
        "# CNN 卷积神经网络\n",
        "🚀 NYIT 880 | 🧑🏻‍💻 funway"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rJktH4kaJky7"
      },
      "source": [
        "## Modules import & Globals setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qz-PuaY_Jljm",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d8183ab7-952a-4291-bb9a-6470e149ab74"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[06/25/25 06:10:10] 🏷️ Label mapping: {'Benign': 0, 'Bot': 1, 'Brute Force -Web': 2, 'Brute Force -XSS': 3, 'DDOS attack-HOIC': 4, 'DDOS attack-LOIC-UDP': 5, 'DDoS attacks-LOIC-HTTP': 6, 'DoS attacks-GoldenEye': 7, 'DoS attacks-Hulk': 8, 'DoS attacks-SlowHTTPTest': 9, 'DoS attacks-Slowloris': 10, 'FTP-BruteForce': 11, 'Infilteration': 12, 'SQL Injection': 13, 'SSH-Bruteforce': 14}\n",
            "导入 utility.ipynb 模块. version 1.0.1\n"
          ]
        }
      ],
      "source": [
        "### Modules ###\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "from io import StringIO\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "## mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "### Globals ###\n",
        "## 数据文件目录\n",
        "dataset = 'cse-cic-ids2018'\n",
        "project_folder = Path('/content/drive/MyDrive/NYIT/880')\n",
        "preprocessed_folder = project_folder / 'data/preprocessed'\n",
        "scaled_folder = preprocessed_folder / 'scaled'\n",
        "splits_folder = preprocessed_folder / 'splits'\n",
        "balanced_folder = project_folder / 'data/balanced'\n",
        "cgan_folder = balanced_folder / 'models'\n",
        "model_folder = project_folder / 'data/classification/models'\n",
        "report_folder = project_folder / 'data/classification/reports'\n",
        "\n",
        "## Label 列的所有可能值(有序)\n",
        "unique_labels = ['Benign', 'Bot', 'Brute Force -Web', 'Brute Force -XSS', 'DDOS attack-HOIC', 'DDOS attack-LOIC-UDP', 'DDoS attacks-LOIC-HTTP', 'DoS attacks-GoldenEye', 'DoS attacks-Hulk', 'DoS attacks-SlowHTTPTest', 'DoS attacks-Slowloris', 'FTP-BruteForce', 'Infilteration', 'SQL Injection', 'SSH-Bruteforce']\n",
        "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "print(f\"[{datetime.now().strftime('%x %X')}] 🏷️ Label mapping: {label_mapping}\")\n",
        "\n",
        "### 全局随机数种子 ###\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "op_seed = 42\n",
        "\n",
        "### Utilities ###\n",
        "# 导入 utility.ipynb 模块\n",
        "%run /content/drive/MyDrive/NYIT/880/code/utils/utility.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 可调参数"
      ],
      "metadata": {
        "id": "OEJ752Ov8lgb"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "54NXnZ1cTC5_"
      },
      "outputs": [],
      "source": [
        "# 是否强制重新训练分类器模型\n",
        "retrain = False\n",
        "\n",
        "# 选择 scaling 方法. 可选[standard, minmax, robust, l1pminmax]\n",
        "scaling_method = 'l1pminmax'\n",
        "\n",
        "# 选择 resample 的目标模式, 0 表示未经过 resample 平衡处理\n",
        "resample_scheme = 2\n",
        "\n",
        "# 过采样方法\n",
        "cgan_model = 'cgan-b(n128,f70,c15,e100,b512,gen[128,[128, 256, 512],0.0003],dis[64,[256, 128],0.0001])_generator.keras'\n",
        "cgan_filter_strategy = 0\n",
        "cgan_filter_keep_high = True\n",
        "cgan_filter_mark = f\"f{'h' if cgan_filter_keep_high else 'l'}{cgan_filter_strategy}\" if cgan_filter_strategy else ''\n",
        "oversampling_method = cgan_model[:-16] + (f'f{cgan_filter_strategy}' if cgan_filter_strategy else '')\n",
        "\n",
        "# 选择 欠采样 方法 (NA 表示不使用欠采样)\n",
        "undersampling_method = 'rus'\n",
        "\n",
        "# 选择 分类器\n",
        "classifier = 'CNN'\n",
        "\n",
        "##########################\n",
        "if resample_scheme == 0:\n",
        "    oversampling_method = 'NA'\n",
        "    undersampling_method = 'NA'\n",
        "##########################\n",
        "\n",
        "# 训练结束后最终模型保存路径\n",
        "model_file_final = model_folder / f'{dataset}_{scaling_method}_s{resample_scheme}_{oversampling_method}_{undersampling_method}_{classifier}-final.keras'\n",
        "\n",
        "# 训练过程中最佳模型保存路径\n",
        "model_file_best  = model_folder / f'{dataset}_{scaling_method}_s{resample_scheme}_{oversampling_method}_{undersampling_method}_{classifier}-best.keras'\n",
        "\n",
        "# 需要评估的模型\n",
        "models_to_evaluate = [model_file_best, model_file_final]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Loading Data"
      ],
      "metadata": {
        "id": "nQz2XyRo7C3y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W_4O7q_BTYFy",
        "outputId": "1cb2d115-9a41-4ea4-c8ce-0227f7c66a2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/24/25 23:10:14 PDT] Loading datasets...\n",
            "[06/24/25 23:10:15 PDT] ⚠️ File not found: /content/drive/MyDrive/NYIT/880/data/balanced/train_X_l1pminmax_s2_cgan-b(n128,f70,c15,e100,b512,gen[128,[128, 256, 512],0.0003],dis[64,[256, 128],0.0001]).npy\n",
            "Load original files\n"
          ]
        }
      ],
      "source": [
        "if resample_scheme == 0:\n",
        "    file_X_train = splits_folder / f'train_X_{scaling_method}.npy'\n",
        "    file_y_train = splits_folder / f'train_y.npy'\n",
        "elif undersampling_method in ['NA', 'rus']:\n",
        "    file_X_train = balanced_folder / f'train_X_{scaling_method}_s{resample_scheme}_{oversampling_method}.npy'\n",
        "    file_y_train = balanced_folder / f'train_y_{scaling_method}_s{resample_scheme}_{oversampling_method}.npy'\n",
        "else:\n",
        "    file_X_train = balanced_folder / f'train_X_{scaling_method}_s{resample_scheme}_{oversampling_method}_{undersampling_method}.npy'\n",
        "    file_y_train = balanced_folder / f'train_y_{scaling_method}_s{resample_scheme}_{oversampling_method}_{undersampling_method}.npy'\n",
        "\n",
        "file_X_valid = splits_folder / f'valid_X_{scaling_method}.npy'\n",
        "file_y_valid = splits_folder / f'valid_y.npy'\n",
        "\n",
        "file_X_test = splits_folder / f'test_X_{scaling_method}.npy'\n",
        "file_y_test = splits_folder / f'test_y.npy'\n",
        "\n",
        "print(f\"[{now()}] Loading datasets...\")\n",
        "if file_X_train.exists():\n",
        "    X_train = np.load(file_X_train)\n",
        "    y_train = np.load(file_y_train)\n",
        "    neet_oversample = False\n",
        "else:\n",
        "    print(f'[{now()}] ⚠️ File not found: {file_X_train}')\n",
        "    print('Load original files')\n",
        "    X_train = np.load(splits_folder / f'train_X_{scaling_method}.npy')\n",
        "    y_train = np.load(splits_folder / f'train_y.npy')\n",
        "    neet_oversample = True\n",
        "X_valid = np.load(file_X_valid)\n",
        "y_valid = np.load(file_y_valid)\n",
        "X_test = np.load(file_X_test)\n",
        "y_test = np.load(file_y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 重采样函数定义"
      ],
      "metadata": {
        "id": "yMko8ofwMb1I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_samples(generator: tf.keras.Model, target_class: int, num_samples: int):\n",
        "    \"\"\"\n",
        "    Generates samples using the generator for a specific target class.\n",
        "\n",
        "    Args:\n",
        "        generator (tensorflow.keras.Model): The generator model.\n",
        "        target_class (int): The target class for which to generate samples.\n",
        "        num_samples (int): The number of samples to generate.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Generated samples as a NumPy array.\n",
        "    \"\"\"\n",
        "    noise_dim = generator.input_shape[0][1]\n",
        "\n",
        "    # 随机生成一组噪声向量 shape=(num_samples, noise_dim)\n",
        "    noise = np.random.normal(0, 1, size=(num_samples, noise_dim))\n",
        "\n",
        "    # 随机生成一组类别标签 shape=(num_samples, 1), 全部为 target_class\n",
        "    labels = np.full((num_samples, 1), fill_value=target_class, dtype=np.int32)\n",
        "\n",
        "    # 使用生成器生成数据\n",
        "    generated_data = generator.predict([noise, labels], verbose=0)\n",
        "    return generated_data\n",
        "\n",
        "\n",
        "def cgan_undersample(cgan_discriminator: tf.keras.Model, sampling_strategy: dict, X: np.ndarray, y: np.ndarray, keep_high_score=True):\n",
        "    \"\"\"\n",
        "    使用 CGAN 判别器对目标数据集进行欠采样，删除评分低的数据\n",
        "    \"\"\"\n",
        "    print(f'[{now()}] 📉 CGAN Undersampling ...')\n",
        "    print(f'  original X.shaep: {X.shape}')\n",
        "    print(f'  original labels_counts: {get_label_counts(y)}')\n",
        "    print(f'  undersample to: {sampling_strategy}')\n",
        "\n",
        "    keep_idxs = []\n",
        "\n",
        "    for cls, target_n in sampling_strategy.items():\n",
        "        idxs = np.where(y == cls)[0]\n",
        "\n",
        "        # nothing to drop if already <= target\n",
        "        if len(idxs) <= target_n:\n",
        "            keep_idxs.extend(idxs.tolist())\n",
        "            print(f'[{now()}] Skipping class [{cls}]: {len(idxs)} ≤ {target_n}')\n",
        "            continue\n",
        "\n",
        "        # Score all samples of this cls\n",
        "        X_cls = X[idxs]\n",
        "        y_cls = y[idxs].reshape(-1, 1)\n",
        "        scores = cgan_discriminator([X_cls, y_cls], training=False)\n",
        "        scores = tf.reshape(scores, [-1]).numpy()\n",
        "\n",
        "        if keep_high_score:\n",
        "            # 按照判别器评分降序排列，取前 n 个保留(保留评分高的)\n",
        "            top_idxs = idxs[np.argsort(scores)[::-1][:target_n]]\n",
        "        else:\n",
        "            # 按照判别器评分升序排列，取前 n 个保留(保留评分低的)\n",
        "            top_idxs = idxs[np.argsort(scores)[:target_n]]\n",
        "\n",
        "        keep_idxs.extend(top_idxs.tolist())\n",
        "        print(f'[{now()}] Dropping {len(idxs) - target_n} samples for class [{cls}]: {len(idxs)} -> {target_n}')\n",
        "\n",
        "    # For any classes not in sampling_strategy, keep all\n",
        "    all_classes = set(np.unique(y))\n",
        "    leftover = all_classes - set(sampling_strategy.keys())\n",
        "    for cls in leftover:\n",
        "        keep_idxs.extend(np.where(y == cls)[0].tolist())\n",
        "\n",
        "    # produce final undersampled arrays\n",
        "    keep_idxs = np.sort(keep_idxs)\n",
        "    X_res = X[keep_idxs]\n",
        "    y_res = y[keep_idxs]\n",
        "\n",
        "    print(f'[{now()}] 📉 After CGAN Undersampling:')\n",
        "    print(f'  X_res.shape: {X_res.shape}')\n",
        "    print(f'  Labels: {get_label_counts(y_res)}')\n",
        "\n",
        "    return X_res, y_res\n",
        "\n",
        "def cgan_oversample(cgan_generator: tf.keras.Model, sampling_strategy: dict, X: np.ndarray, y: np.ndarray):\n",
        "    current_counts = get_label_counts(y)\n",
        "\n",
        "    print(f'[{now()}] 📈 CGAN Oversampling ...')\n",
        "    print(f'  original X.shaep: {X.shape}')\n",
        "    print(f'  original labels_counts: {current_counts}')\n",
        "    print(f'  oversample to: {sampling_strategy}')\n",
        "\n",
        "    all_X = [X]\n",
        "    all_y = [y]\n",
        "\n",
        "    for cls, desired_n in sampling_strategy.items():\n",
        "        current_n = current_counts.get(cls, 0)\n",
        "        n_to_gen = desired_n - current_n\n",
        "        if n_to_gen > 0:\n",
        "            print(f'[{now()}] Generating {n_to_gen} samples for class [{cls}]: {current_n} -> {desired_n}')\n",
        "            gen_samples = generate_samples(cgan_generator, cls, n_to_gen)\n",
        "            all_X.append(gen_samples)\n",
        "            all_y.append(np.full(n_to_gen, cls, dtype=np.int32))\n",
        "        else:\n",
        "            print(f'[{now()}] Skipping class [{cls}]: {current_n} ≥ {desired_n}')\n",
        "\n",
        "    X_res = np.concatenate(all_X)\n",
        "    y_res = np.concatenate(all_y)\n",
        "\n",
        "    print(f'[{now()}] 📈 After CGAN Oversampling:')\n",
        "    print(f'  X_res.shape: {X_res.shape}')\n",
        "    print(f'  Labels: {get_label_counts(y_res)}')\n",
        "\n",
        "    return X_res, y_res"
      ],
      "metadata": {
        "id": "GML7aC9nMlsY"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 过采样"
      ],
      "metadata": {
        "id": "FWO8ThhnMgRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if neet_oversample:\n",
        "    print(f\"[{now()}] 📈 Oversampling with {oversampling_method}\")\n",
        "\n",
        "    # ROS 过采样\n",
        "    from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "    # 判断 oversampling_method 字符串开头是否为 ros\n",
        "    if oversampling_method.startswith('ros'):\n",
        "        ros_scheme = int(oversampling_method[3])\n",
        "        oversample_to = ros_schemes[ros_scheme]\n",
        "        print(f'[{now()}] Applying ROS oversampling to: {oversample_to}')\n",
        "\n",
        "        oversampler = RandomOverSampler(sampling_strategy=oversample_to, random_state=op_seed)\n",
        "        X_train, y_train = oversampler.fit_resample(X_train, y_train)\n",
        "\n",
        "        print(f'[{now()}] After ROS oversampling:')\n",
        "        print(f'  X.shape: {X_train.shape}, y.shape: {y_train.shape}')\n",
        "        print(f'  Labels: { {int(k): v for k, v in sorted(Counter(y_train).items())} }\\n')\n",
        "    else:\n",
        "        print(f'[{now()}] No need to apply ROS oversampling.')\n",
        "\n",
        "\n",
        "    # 加载 CGAN\n",
        "    generator_file = cgan_folder / scaling_method / cgan_model\n",
        "    discriminator_file = cgan_folder / scaling_method / cgan_model.replace('generator', 'discriminator')\n",
        "\n",
        "    print(f\"[{now()}] 📡 Loading pre-trained generator from {generator_file}\")\n",
        "    generator = tf.keras.models.load_model(generator_file)\n",
        "\n",
        "    print(f\"[{now()}] 📡 Loading pre-trained discriminator from {discriminator_file}\")\n",
        "    discriminator = tf.keras.models.load_model(discriminator_file)\n",
        "\n",
        "\n",
        "    # CGAN 欠采样 (如果需要的话)\n",
        "    if cgan_filter_strategy:\n",
        "        print(f'[{now()}] 🟡 Apply CGAN Undersampling.')\n",
        "        X_train, y_train = cgan_undersample(discriminator, cgan_filter_schemes[cgan_filter_strategy], X_train, y_train)\n",
        "\n",
        "\n",
        "    # CGAN 过采样\n",
        "    print(f'[{now()}] 🟢 Apply CGAN Oversampling.')\n",
        "    resample_to = resample_schemes[resample_scheme]\n",
        "    X_train, y_train = cgan_oversample(generator, resample_to, X_train, y_train)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3q8c32TkLn6y",
        "outputId": "889d81ab-21c3-4b3a-9fb8-6b16b81a6f67"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/24/25 23:10:43 PDT] 📈 Oversampling with cgan-b(n128,f70,c15,e100,b512,gen[128,[128, 256, 512],0.0003],dis[64,[256, 128],0.0001])\n",
            "[06/24/25 23:10:44 PDT] No need to apply ROS oversampling.\n",
            "[06/24/25 23:10:44 PDT] 📡 Loading pre-trained generator from /content/drive/MyDrive/NYIT/880/data/balanced/models/l1pminmax/cgan-b(n128,f70,c15,e100,b512,gen[128,[128, 256, 512],0.0003],dis[64,[256, 128],0.0001])_generator.keras\n",
            "[06/24/25 23:10:50 PDT] 📡 Loading pre-trained discriminator from /content/drive/MyDrive/NYIT/880/data/balanced/models/l1pminmax/cgan-b(n128,f70,c15,e100,b512,gen[128,[128, 256, 512],0.0003],dis[64,[256, 128],0.0001])_discriminator.keras\n",
            "[06/24/25 23:10:51 PDT] 🟢 Apply CGAN Oversampling.\n",
            "[06/24/25 23:10:52 PDT] 📈 CGAN Oversampling ...\n",
            "  original X.shaep: (3797547, 70)\n",
            "  original labels_counts: {0: 1600000, 1: 228953, 2: 489, 3: 184, 4: 548809, 5: 1384, 6: 460953, 7: 33206, 8: 369530, 9: 111912, 10: 8792, 11: 154683, 12: 128511, 13: 70, 14: 150071}\n",
            "  oversample to: {0: 800000, 1: 200000, 2: 50000, 3: 50000, 4: 200000, 5: 50000, 6: 200000, 7: 100000, 8: 200000, 9: 110000, 10: 50000, 11: 150000, 12: 120000, 13: 50000, 14: 150000}\n",
            "[06/24/25 23:10:52 PDT] Skipping class [0]: 1600000 ≥ 800000\n",
            "[06/24/25 23:10:52 PDT] Skipping class [1]: 228953 ≥ 200000\n",
            "[06/24/25 23:10:52 PDT] Generating 49511 samples for class [2]: 489 -> 50000\n",
            "[06/24/25 23:10:56 PDT] Generating 49816 samples for class [3]: 184 -> 50000\n",
            "[06/24/25 23:11:00 PDT] Skipping class [4]: 548809 ≥ 200000\n",
            "[06/24/25 23:11:00 PDT] Generating 48616 samples for class [5]: 1384 -> 50000\n",
            "[06/24/25 23:11:04 PDT] Skipping class [6]: 460953 ≥ 200000\n",
            "[06/24/25 23:11:04 PDT] Generating 66794 samples for class [7]: 33206 -> 100000\n",
            "[06/24/25 23:11:09 PDT] Skipping class [8]: 369530 ≥ 200000\n",
            "[06/24/25 23:11:09 PDT] Skipping class [9]: 111912 ≥ 110000\n",
            "[06/24/25 23:11:09 PDT] Generating 41208 samples for class [10]: 8792 -> 50000\n",
            "[06/24/25 23:11:12 PDT] Skipping class [11]: 154683 ≥ 150000\n",
            "[06/24/25 23:11:12 PDT] Skipping class [12]: 128511 ≥ 120000\n",
            "[06/24/25 23:11:12 PDT] Generating 49930 samples for class [13]: 70 -> 50000\n",
            "[06/24/25 23:11:15 PDT] Skipping class [14]: 150071 ≥ 150000\n",
            "[06/24/25 23:11:15 PDT] 📈 After CGAN Oversampling:\n",
            "  X_res.shape: (4103422, 70)\n",
            "  Labels: {0: 1600000, 1: 228953, 2: 50000, 3: 50000, 4: 548809, 5: 50000, 6: 460953, 7: 100000, 8: 369530, 9: 111912, 10: 50000, 11: 154683, 12: 128511, 13: 50000, 14: 150071}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 欠采样"
      ],
      "metadata": {
        "id": "1t_MCf_bMiUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 处理 RUS 欠采样\n",
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "label_counts = get_label_counts(y_train)\n",
        "resample_to = resample_schemes[resample_scheme]\n",
        "undersample_to = {\n",
        "    k: resample_to[k]\n",
        "    for k in resample_to\n",
        "    if resample_to[k] < label_counts.get(k, 0)\n",
        "}\n",
        "\n",
        "print(f\"[{now()}] 当前样本数: {X_train.shape}, {label_counts}\")\n",
        "print(f\"[{now()}] Undersample to: {undersample_to}\")\n",
        "\n",
        "if undersampling_method == 'rus':\n",
        "    print(f\"[{now()}] 📉 使用 RUS 欠采样\")\n",
        "\n",
        "    rus = RandomUnderSampler(sampling_strategy=undersample_to, random_state=op_seed)\n",
        "    X_train, y_train = rus.fit_resample(X_train, y_train)\n",
        "\n",
        "elif undersampling_method == 'cgan':\n",
        "    print(f\"[{now()}] 📉 使用 CGAN 欠采样\")\n",
        "    X_train, y_train = cgan_undersample(discriminator, undersample_to, X_train, y_train)\n",
        "\n",
        "elif undersampling_method == 'cganlow':\n",
        "    print(f\"[{now()}] 📉 使用 CGAN 欠采样 (保留评分低的)\")\n",
        "    X_train, y_train = cgan_undersample(discriminator, undersample_to, X_train, y_train, keep_high_score=False)\n",
        "\n",
        "elif undersampling_method == 'iht':\n",
        "    print(f\"[{now()}] 📉 使用 IHT 欠采样\")\n",
        "    from imblearn.under_sampling import InstanceHardnessThreshold\n",
        "    from xgboost import XGBClassifier\n",
        "\n",
        "    # 使用 GPU 加速的 XGBClassifier 作为 IHT 的分类器\n",
        "    iht = InstanceHardnessThreshold(\n",
        "        estimator=XGBClassifier(tree_method='hist', device='cuda', n_estimators=200, max_depth=5, learning_rate=0.05, subsample=0.8, colsample_bytree=0.8, eval_metric='mlogloss'),\n",
        "        sampling_strategy={9: 100_000, 12: 110_000},\n",
        "        random_state=op_seed)\n",
        "    X_train, y_train = iht.fit_resample(X_train, y_train)\n",
        "    print(f'[{now()}] After IHT Undersampling: {X_train.shape}, {get_label_counts(y_train)}')\n",
        "\n",
        "    # IHT 无法保证欠采样到目标数量，所以要再补一个 RUS\n",
        "    rus = RandomUnderSampler(sampling_strategy=undersample_to, random_state=op_seed)\n",
        "    X_train, y_train = rus.fit_resample(X_train, y_train)\n",
        "\n",
        "elif undersampling_method == 'NA':\n",
        "    print(f\"[{now()}] 不进行欠采样\")\n",
        "\n",
        "else:\n",
        "    raise Exception(f'[{now()}] ❌ 未定义的欠采样方法: {undersampling_method}')\n",
        "\n",
        "print(f\"[{now()}] 欠采样完成: {X_train.shape}, {get_label_counts(y_train)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SI06twDp53Nm",
        "outputId": "33114e7b-02a5-4cfd-ca52-0b99c55eb673"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/24/25 23:11:17 PDT] 当前样本数: (4103422, 70), {0: 1600000, 1: 228953, 2: 50000, 3: 50000, 4: 548809, 5: 50000, 6: 460953, 7: 100000, 8: 369530, 9: 111912, 10: 50000, 11: 154683, 12: 128511, 13: 50000, 14: 150071}\n",
            "[06/24/25 23:11:17 PDT] Undersample to: {0: 800000, 1: 200000, 4: 200000, 6: 200000, 8: 200000, 9: 110000, 11: 150000, 12: 120000, 14: 150000}\n",
            "[06/24/25 23:11:17 PDT] 📉 使用 RUS 欠采样\n",
            "[06/24/25 23:11:18 PDT] 欠采样完成: (2480000, 70), {0: 800000, 1: 200000, 2: 50000, 3: 50000, 4: 200000, 5: 50000, 6: 200000, 7: 100000, 8: 200000, 9: 110000, 10: 50000, 11: 150000, 12: 120000, 13: 50000, 14: 150000}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "report_dataset = ''\n",
        "report_dataset += f'\\nTrain set: {file_X_train.name}, {file_y_train.name} \\n'\n",
        "report_dataset += f'    shape: {X_train.shape}, {y_train.shape} \\n'\n",
        "report_dataset += f'    labels: {get_label_counts(y_train)} \\n'\n",
        "report_dataset += f'\\nValid set: {file_X_valid.name}, {file_y_valid.name} \\n'\n",
        "report_dataset += f'    shape: {X_valid.shape}, {y_valid.shape} \\n'\n",
        "report_dataset += f'    labels: {get_label_counts(y_valid)} \\n'\n",
        "report_dataset += f'\\nTest set: {file_X_test.name}, {file_y_test.name} \\n'\n",
        "report_dataset += f'    shape: {X_test.shape}, {y_test.shape} \\n'\n",
        "report_dataset += f'    labels: {get_label_counts(y_test)} \\n'\n",
        "\n",
        "print(report_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ni7rOH24Kf4q",
        "outputId": "a33aede6-f659-4a8d-b623-3f2581bf7d60"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train set: train_X_l1pminmax_s2_cgan-b(n128,f70,c15,e100,b512,gen[128,[128, 256, 512],0.0003],dis[64,[256, 128],0.0001]).npy, train_y_l1pminmax_s2_cgan-b(n128,f70,c15,e100,b512,gen[128,[128, 256, 512],0.0003],dis[64,[256, 128],0.0001]).npy \n",
            "    shape: (2480000, 70), (2480000,) \n",
            "    labels: {0: 800000, 1: 200000, 2: 50000, 3: 50000, 4: 200000, 5: 50000, 6: 200000, 7: 100000, 8: 200000, 9: 110000, 10: 50000, 11: 150000, 12: 120000, 13: 50000, 14: 150000} \n",
            "\n",
            "Valid set: valid_X_l1pminmax.npy, valid_y.npy \n",
            "    shape: (474693, 70), (474693,) \n",
            "    labels: {0: 200000, 1: 28619, 2: 61, 3: 23, 4: 68601, 5: 173, 6: 57619, 7: 4151, 8: 46191, 9: 13989, 10: 1099, 11: 19335, 12: 16064, 13: 9, 14: 18759} \n",
            "\n",
            "Test set: test_X_l1pminmax.npy, test_y.npy \n",
            "    shape: (474694, 70), (474694,) \n",
            "    labels: {0: 200000, 1: 28619, 2: 61, 3: 23, 4: 68602, 5: 173, 6: 57619, 7: 4151, 8: 46191, 9: 13989, 10: 1099, 11: 19336, 12: 16064, 13: 8, 14: 18759} \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "OzbmCahI4UPt",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c4c5844-7bd9-4b5c-d766-20d99cd9e42f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/24/25 23:11:19 PDT] 升维之后: (2480000, 70, 1) (474693, 70, 1) (474694, 70, 1)\n",
            "[06/24/25 23:11:19 PDT] After one-hot: (2480000, 15) (474693, 15) (474694, 15)\n"
          ]
        }
      ],
      "source": [
        "## 给特征矩阵X增加纬度，标签向量y进行独热编码\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "if X_train.ndim == 2:  # 只在原始二维时加维度\n",
        "    # 为每个特征矩阵增加一个纬度(CNN 卷积神经网络中的通道)\n",
        "    X_train = np.expand_dims(X_train, 2)\n",
        "    X_valid = np.expand_dims(X_valid, 2)\n",
        "    X_test = np.expand_dims(X_test, 2)\n",
        "\n",
        "    # 对标签数据进行独热编码 one-hot\n",
        "    y_train = to_categorical(y_train)\n",
        "    y_valid = to_categorical(y_valid)\n",
        "    y_test = to_categorical(y_test)\n",
        "\n",
        "print(f\"[{now()}] 升维之后:\", X_train.shape, X_valid.shape, X_test.shape)\n",
        "print(f\"[{now()}] After one-hot:\", y_train.shape, y_valid.shape, y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uTYOH66Wwtm"
      },
      "source": [
        "## Train Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "9W3zg2ezZz5V"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from tensorflow.keras.optimizers import Nadam\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "\n",
        "def train_model(x_train, y_train, x_val, y_val, batch_size=256, epochs=100):\n",
        "    \"\"\"模型训练函数\n",
        "\n",
        "    Args:\n",
        "        x_train (_type_): 训练集特征矩阵\n",
        "        y_train (_type_): 训练集标签\n",
        "        x_val (_type_): 验证集特征矩阵\n",
        "        y_val (_type_): 验证集标签\n",
        "        batch_size (int, optional): _description_. Defaults to 256.\n",
        "        epochs (int, optional): _description_. Defaults to 100.\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. 输入层，指定输入数据的形状 (features, depth)\n",
        "    input_signal = tf.keras.Input(x_train.shape[1:])\n",
        "\n",
        "    # 2. 第一组卷积层，使用 32 个卷积核，大小为 3，激活函数为 ReLU，填充方式为 same, 初始化方法为 he_uniform\n",
        "    x = layers.Conv1D(32, 3, activation='relu', padding='same', kernel_initializer='he_uniform')(input_signal)\n",
        "    x = layers.Conv1D(32, 3, activation='relu', padding='same', kernel_initializer='he_uniform')(x)\n",
        "    x = layers.MaxPooling1D(pool_size=2, strides=2)(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # 3. 第二组卷积层，使用 64 个卷积核，大小为 3，激活函数为 ReLU，填充方式为 same, 初始化方法为 he_uniform\n",
        "    x = layers.Conv1D(64, 3, activation='relu', padding='same', kernel_initializer='he_uniform')(x)\n",
        "    x = layers.Conv1D(64, 3, activation='relu', padding='same', kernel_initializer='he_uniform')(x)\n",
        "    x = layers.MaxPooling1D(pool_size=2, strides=2)(x)\n",
        "    x = layers.Dropout(0.2)(x)\n",
        "    x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # 4. 全连接层, 将卷积层的输出展平\n",
        "    x = layers.Flatten()(x)\n",
        "    x = layers.Dense(32, activation='relu')(x)\n",
        "    x = layers.Dense(y_train.shape[1], activation='softmax')(x)  # y_train.shape[1] 是类别数\n",
        "\n",
        "    # 5. 创建模型\n",
        "    model = models.Model(inputs=input_signal, outputs=x)\n",
        "\n",
        "    # 6. 打印模型信息\n",
        "    print(f\"[{now()}] model summary:\")\n",
        "    model.summary()\n",
        "\n",
        "    # 7. 定义优化器\n",
        "    # learning_rate 指定步长大小，越大则收敛越快，但越不稳定。默认值为 0.001\n",
        "    # 其他参数保持默认值即可\n",
        "    learning_rate = 0.001\n",
        "    nadam = Nadam(learning_rate=learning_rate)\n",
        "\n",
        "    # 8. 定义 callbacks 方法 (每个 epoch 训练结束后调用)\n",
        "    callbacks = [\n",
        "        # ⏹️ 提前停止训练：如果验证集 loss 连续 15 轮没有下降，就停止训练，防止过拟合\n",
        "        EarlyStopping(\n",
        "            monitor='val_loss',          # 监控指标是验证集的 loss\n",
        "            min_delta=1e-6,              #\n",
        "            patience=15,                 # 容忍 15 个 epoch 不进步\n",
        "            restore_best_weights=True    # 自动回滚到验证集 loss 最优的模型参数\n",
        "        ),\n",
        "\n",
        "        # 🔽 自动调整学习率：如果验证集 loss 停滞 10 个 epoch，就将学习率乘以 0.1\n",
        "        ReduceLROnPlateau(\n",
        "            monitor='val_loss',         # 监控验证集 loss\n",
        "            factor=0.1,                 # 学习率降低为原来的 1/10\n",
        "            patience=10                 # 容忍多少个 epoch 无进步\n",
        "        ),\n",
        "\n",
        "        # 💾 模型保存器：在验证集 accuracy 最佳时保存整个模型到文件\n",
        "        ModelCheckpoint(\n",
        "            filepath=model_file_best,  # 模型保存路径\n",
        "            monitor='val_accuracy',     # 保存依据是验证集准确率\n",
        "            save_best_only=True         # 只在 val_accuracy 最佳时才保存\n",
        "        )\n",
        "    ]\n",
        "\n",
        "    # 9. 编译模型\n",
        "    model.compile(loss='categorical_crossentropy',\n",
        "                  optimizer=nadam,\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    # 10. 开始训练模型\n",
        "    time_start = datetime.now()\n",
        "    print(f\"[{now()}] 🏋️‍♂️ Training model...\")\n",
        "    history = model.fit(x_train, y_train,\n",
        "                        batch_size=batch_size, epochs=epochs,\n",
        "                        validation_data=(x_val, y_val),\n",
        "                        verbose=1,\n",
        "                        callbacks=callbacks)\n",
        "    time_end = datetime.now()\n",
        "    print(f\"[{now()}] 🕗 Training completed. Time elapsed: {time_end - time_start}\")\n",
        "\n",
        "    return model, history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FFyUvc4UMLsg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 967
        },
        "outputId": "8d55f689-5c57-4704-8d6a-7e46889ef983"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/24/25 23:11:21 PDT] 🤖 build and train model (cse-cic-ids2018_l1pminmax_s2_cgan-b(n128,f70,c15,e100,b512,gen[128,[128, 256, 512],0.0003],dis[64,[256, 128],0.0001])_rus_CNN-final.keras)...\n",
            "[06/24/25 23:11:21 PDT] model summary:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m, \u001b[38;5;34m1\u001b[0m)          │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d (\u001b[38;5;33mConv1D\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │           \u001b[38;5;34m128\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_1 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │         \u001b[38;5;34m3,104\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d (\u001b[38;5;33mMaxPooling1D\u001b[0m)    │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (\u001b[38;5;33mDropout\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m32\u001b[0m)         │           \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_2 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │         \u001b[38;5;34m6,208\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_3 (\u001b[38;5;33mConv1D\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m35\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │        \u001b[38;5;34m12,352\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_1 (\u001b[38;5;33mMaxPooling1D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m64\u001b[0m)         │           \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalization\u001b[0m)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (\u001b[38;5;33mFlatten\u001b[0m)               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1088\u001b[0m)           │             \u001b[38;5;34m0\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │        \u001b[38;5;34m34,848\u001b[0m │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m15\u001b[0m)             │           \u001b[38;5;34m495\u001b[0m │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)          │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">3,104</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)    │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">6,208</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ conv1d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv1D</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">35</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │        <span style=\"color: #00af00; text-decoration-color: #00af00\">12,352</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ max_pooling1d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling1D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ batch_normalization_1           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)         │           <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalization</span>)            │                        │               │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1088</span>)           │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">34,848</span> │\n",
              "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">15</span>)             │           <span style=\"color: #00af00; text-decoration-color: #00af00\">495</span> │\n",
              "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m57,519\u001b[0m (224.68 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">57,519</span> (224.68 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m57,327\u001b[0m (223.93 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">57,327</span> (223.93 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m192\u001b[0m (768.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">192</span> (768.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/24/25 23:11:21 PDT] 🏋️‍♂️ Training model...\n",
            "Epoch 1/100\n",
            "\u001b[1m9688/9688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m48s\u001b[0m 4ms/step - accuracy: 0.9043 - loss: 0.2390 - val_accuracy: 0.9468 - val_loss: 0.1358 - learning_rate: 0.0010\n",
            "Epoch 2/100\n",
            "\u001b[1m9688/9688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 3ms/step - accuracy: 0.9251 - loss: 0.1697 - val_accuracy: 0.9495 - val_loss: 0.1349 - learning_rate: 0.0010\n",
            "Epoch 3/100\n",
            "\u001b[1m9688/9688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 3ms/step - accuracy: 0.9263 - loss: 0.1668 - val_accuracy: 0.9485 - val_loss: 0.1344 - learning_rate: 0.0010\n",
            "Epoch 4/100\n",
            "\u001b[1m9688/9688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 3ms/step - accuracy: 0.9270 - loss: 0.1652 - val_accuracy: 0.9482 - val_loss: 0.1340 - learning_rate: 0.0010\n",
            "Epoch 5/100\n",
            "\u001b[1m9688/9688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 3ms/step - accuracy: 0.9274 - loss: 0.1641 - val_accuracy: 0.9490 - val_loss: 0.1336 - learning_rate: 0.0010\n",
            "Epoch 6/100\n",
            "\u001b[1m9688/9688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 3ms/step - accuracy: 0.9276 - loss: 0.1635 - val_accuracy: 0.9497 - val_loss: 0.1327 - learning_rate: 0.0010\n",
            "Epoch 7/100\n",
            "\u001b[1m9688/9688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 3ms/step - accuracy: 0.9280 - loss: 0.1626 - val_accuracy: 0.9485 - val_loss: 0.1322 - learning_rate: 0.0010\n",
            "Epoch 8/100\n",
            "\u001b[1m9675/9688\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.9285 - loss: 0.1619"
          ]
        }
      ],
      "source": [
        "model_file = model_file_final\n",
        "\n",
        "if model_file.exists() and not retrain:\n",
        "    print(f\"[{now()}] ⏭️ 模型已存在，不重新训练 ({model_file})\")\n",
        "else:\n",
        "    # 训练模型\n",
        "    print(f\"[{now()}] 🤖 build and train model ({model_file.name})...\")\n",
        "    model, history = train_model(X_train, y_train, X_valid, y_valid)\n",
        "\n",
        "    # 打印history\n",
        "    print(f\"[{now()}] history:\\n  {history.history}\")\n",
        "\n",
        "    # 保存模型\n",
        "    model.save(model_file)\n",
        "    print(f\"[{now()}] 💾 model saved to {model_file}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruMKfG2mXAWr"
      },
      "source": [
        "## Evaluate Model"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 导入 evaluation.ipynb 模块\n",
        "%run /content/drive/MyDrive/NYIT/880/code/utils/evaluation.ipynb\n",
        "\n",
        "def get_model_summary_string(model):\n",
        "    \"\"\"获取 Keras 模型的摘要字符串\"\"\"\n",
        "    stream = StringIO()\n",
        "    model.summary(print_fn=lambda x: stream.write(x + '\\n'))\n",
        "    summary_str = stream.getvalue()\n",
        "    stream.close()\n",
        "    return summary_str\n",
        "\n",
        "for model_file in models_to_evaluate:\n",
        "    print(f\"[{now()}] ⚙️ Loading model from {model_file}\")\n",
        "    model = models.load_model(model_file)\n",
        "\n",
        "    # 模型信息\n",
        "    report_model_info = f\"[{now()}] 🤖 Model: {model_file.name}\\n\"\n",
        "    report_model_info += f\"[{now()}] ================= 🧠 Model Info =================\\n\"\n",
        "    report_model_info += f\"Model summary: {get_model_summary_string(model)}\"\n",
        "    print(report_model_info)\n",
        "\n",
        "    # 内建的 evaluate 评估\n",
        "    # report_model_info += f\"[{now()}] ================= 🔶 tf.keras.Model.evaluate() =================\\n\"\n",
        "    # results = model.evaluate(X_test, y_test, verbose=1, return_dict=True)\n",
        "    # report_model_info += f\"{results}\\n\"\n",
        "\n",
        "    # 获取预测结果（概率），再转为类别索引\n",
        "    y_pred_probs = model.predict(X_test, verbose=1)  # 得到的一个矩阵，每行代表一个样本属于每个类别的概率. 比如[0.1, 0.7, 0.2]就表示该样本有0.1的概率属于类别0, 0.7的概率属于类别1\n",
        "    y_pred = np.argmax(y_pred_probs, axis=1)  # 对每行提取值最大的列(提取出每个样本的预测类别)\n",
        "    y_true = np.argmax(y_test, axis=1)  # 提取出每个样本的真实类别\n",
        "\n",
        "    # 生成评估报告\n",
        "    report_eval = generate_evaluation_report(y_true=y_true, y_predict=y_pred,\n",
        "                                             label_mapping=label_mapping,\n",
        "                                             figure_output=report_folder / 'png' / f'{model_file.stem}_confusion_matrix.png',\n",
        "                                             figure_show=True)\n",
        "    print(report_eval)\n",
        "\n",
        "    # 保存报告\n",
        "    report_file = report_folder / f'{model_file.stem}_report.txt'\n",
        "    report_all = report_dataset + '\\n' + report_model_info + '\\n' + report_eval\n",
        "    with open(report_file, 'w') as f:\n",
        "        f.write(report_all)\n",
        "        print(f\"[{now()}] 💾 Evaluation report saved to {report_file}\")\n",
        "    print('============================================================')"
      ],
      "metadata": {
        "id": "q3_hHyk9tjyp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "print(f'[{now()}] ⛔ 运行结束. shutdown now...')\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "YppT-rp0qI8F"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "L4",
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}