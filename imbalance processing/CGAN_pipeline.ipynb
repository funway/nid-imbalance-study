{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/funway/nid-imbalance-study/blob/main/imbalance%20processing/CGAN_pipeline.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv4skFSheBzA"
      },
      "source": [
        "# ä½¿ç”¨ ROS+CGAN å¯¹è®­ç»ƒé›†è¿›è¡Œè¿‡é‡‡æ ·\n",
        "ğŸš€ NYIT 880 | ğŸ§‘ğŸ»â€ğŸ’» funway\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR21AFUgeYXb"
      },
      "source": [
        "## Modules import & Globals setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NKNH9UjKeZmU",
        "outputId": "b735f6b0-4194-4a6f-99cd-41d1ccfe9b12"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "[06/25/25 06:14:22] ğŸ·ï¸ Label mapping: {'Benign': 0, 'Bot': 1, 'Brute Force -Web': 2, 'Brute Force -XSS': 3, 'DDOS attack-HOIC': 4, 'DDOS attack-LOIC-UDP': 5, 'DDoS attacks-LOIC-HTTP': 6, 'DoS attacks-GoldenEye': 7, 'DoS attacks-Hulk': 8, 'DoS attacks-SlowHTTPTest': 9, 'DoS attacks-Slowloris': 10, 'FTP-BruteForce': 11, 'Infilteration': 12, 'SQL Injection': 13, 'SSH-Bruteforce': 14}\n",
            "å¯¼å…¥ utility.ipynb æ¨¡å—. version 1.0.1\n"
          ]
        }
      ],
      "source": [
        "### Modules ###\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "\n",
        "import os, sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "## mount google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "\n",
        "### Globals ###\n",
        "## æ•°æ®æ–‡ä»¶ç›®å½•\n",
        "dataset = 'cse-cic-ids2018'\n",
        "project_folder = Path('/content/drive/MyDrive/NYIT/880')\n",
        "preprocessed_folder = project_folder / 'data/preprocessed'\n",
        "scaled_folder = preprocessed_folder / 'scaled'\n",
        "splits_folder = preprocessed_folder / 'splits'\n",
        "balanced_folder = project_folder / 'data/balanced'\n",
        "\n",
        "## Label åˆ—çš„æ‰€æœ‰å¯èƒ½å€¼(æœ‰åº)\n",
        "unique_labels = ['Benign', 'Bot', 'Brute Force -Web', 'Brute Force -XSS', 'DDOS attack-HOIC', 'DDOS attack-LOIC-UDP', 'DDoS attacks-LOIC-HTTP', 'DoS attacks-GoldenEye', 'DoS attacks-Hulk', 'DoS attacks-SlowHTTPTest', 'DoS attacks-Slowloris', 'FTP-BruteForce', 'Infilteration', 'SQL Injection', 'SSH-Bruteforce']\n",
        "label_mapping = {label: idx for idx, label in enumerate(unique_labels)}\n",
        "print(f\"[{datetime.now().strftime('%x %X')}] ğŸ·ï¸ Label mapping: {label_mapping}\")\n",
        "\n",
        "# å®šä¹‰æå°‘æ•°ç±». [13, 3, 2] å±äºæå°‘æ•°, [5, 10, 7] å±äºå°‘æ•°\n",
        "minority_labels = [13, 3, 2, 5, 10, 7]\n",
        "\n",
        "### å…¨å±€éšæœºæ•°ç§å­ ###\n",
        "np.random.seed(42)\n",
        "tf.random.set_seed(42)\n",
        "op_seed = 42\n",
        "\n",
        "### Utilities ###\n",
        "# å¯¼å…¥ utility.ipynb æ¨¡å—\n",
        "%run /content/drive/MyDrive/NYIT/880/code/utils/utility.ipynb"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r4DGprZsKWAz"
      },
      "source": [
        "## å¯è°ƒå‚æ•°"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "K-SqflxGQ5rK"
      },
      "outputs": [],
      "source": [
        "# æ˜¯å¦å¼ºåˆ¶é‡æ–°è®­ç»ƒ CGAN\n",
        "retrain = False\n",
        "\n",
        "# é€‰æ‹© scaling æ–¹æ³•. å¯é€‰[standard, minmax, robust, l1pminmax]\n",
        "scaling_method = 'minmax'\n",
        "\n",
        "# è¿‡é‡‡æ ·æ–¹æ³•. å¯ä»¥é€‰[cgan-a, cgan-m, cgan-b, ros1+cgan-a, ros2+cgan-b, ...]\n",
        "oversampling_method = 'cgan-m'\n",
        "\n",
        "## CGAN å‚æ•° ##\n",
        "noise_dim = 128     # å™ªå£°ç»´åº¦(ç»™ç”Ÿæˆå™¨çš„)\n",
        "epochs = 500        # å¤šå°‘è½®è®­ç»ƒ\n",
        "batch_size = 512    # æ¯è½®è®­ç»ƒä¸­åˆ’åˆ† batch çš„å¤§å°\n",
        "buffer_size = 200_000  # tf.data éšæœºåˆ‡ç‰‡æ•°æ®æ—¶çš„ç¼“å­˜å¤§å°\n",
        "\n",
        "gen_embed_dim = 128                # ç”Ÿæˆå™¨åµŒå…¥å±‚ç»´åº¦\n",
        "gen_hidden_dims = [128, 256, 512] # ç”Ÿæˆå™¨éšè—å±‚\n",
        "gen_learning_rate = 3e-4          # ç”Ÿæˆå™¨å­¦ä¹ ç‡\n",
        "\n",
        "disc_embed_dim = 64           # åˆ¤åˆ«å™¨åµŒå…¥å±‚ç»´åº¦\n",
        "disc_hidden_dims = [256, 128] # åˆ¤åˆ«å™¨éšè—å±‚\n",
        "disc_learning_rate = 1e-4     # åˆ¤åˆ«å™¨å­¦ä¹ ç‡\n",
        "\n",
        "# ç”Ÿæˆè¿‡é‡‡æ ·æ–‡ä»¶ (ç©ºæ•°ç»„å°±è¡¨ç¤ºä¸ç”Ÿæˆ è¿‡é‡‡æ ·æ–‡ä»¶)\n",
        "resample_outputs = []\n",
        "\n",
        "# åœ¨ç”Ÿæˆè¿‡é‡‡æ ·æ–‡ä»¶ä¹‹å‰, æ˜¯å¦å…ˆç”¨ CGAN åˆ¤åˆ«å™¨åˆ é™¤è´¨é‡å·®çš„æ ·æœ¬\n",
        "# cgan_filter_strategy = 0 è¡¨ç¤ºä¸ä½¿ç”¨ CGAN è¿›è¡Œåˆ é™¤\n",
        "cgan_filter_strategy = 0 # å» utility é‡Œçš„ filter_schemes å–å€¼\n",
        "if cgan_filter_strategy and 'cgan-m' in oversampling_method:\n",
        "    raise Exception('ä¸è¦ä½¿ç”¨ cgan-m å¯¹å¤šæ•°ç±»è¿›è¡Œæ¬ é‡‡æ ·ï¼å› ä¸º cgan-m æ˜¯ä»…é’ˆå¯¹ minority è¿›è¡Œè®­ç»ƒçš„ã€‚')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AM7RomoyKpSt"
      },
      "source": [
        "## åŠ è½½è®­ç»ƒé›†"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "DXZ4E8wJlzLF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38cf6781-7f4a-4fac-8317-fbf56674c594"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/24/25 23:14:38 PDT] train_X_minmax.npy shape: (3797547, 70), train_y.npy shape: (3797547,)\n",
            "Labels: {0: 1600000, 1: 228953, 2: 489, 3: 184, 4: 548809, 5: 1384, 6: 460953, 7: 33206, 8: 369530, 9: 111912, 10: 8792, 11: 154683, 12: 128511, 13: 70, 14: 150071}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "X_file = splits_folder / f'train_X_{scaling_method}.npy'\n",
        "y_file = splits_folder / f'train_y.npy'\n",
        "\n",
        "# åŠ è½½è®­ç»ƒé›†æ–‡ä»¶\n",
        "X = np.load(X_file)\n",
        "y = np.load(y_file)\n",
        "\n",
        "print(f'[{now()}] {X_file.name} shape: {X.shape}, {y_file.name} shape: {y.shape}')\n",
        "print(f'Labels: { {int(k): v for k, v in sorted(Counter(y).items())} }\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75awp9PSfBmN"
      },
      "source": [
        "## CGAN å®šä¹‰\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhavXS3_0bpG"
      },
      "source": [
        "### å®šä¹‰è¾…åŠ©å‡½æ•°ä¸ç±»"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "B9-UoZzDQhsB"
      },
      "outputs": [],
      "source": [
        "from imblearn.under_sampling import RandomUnderSampler\n",
        "\n",
        "def select_cgan_subset(X, y, mode, rus_target=100_000):\n",
        "    \"\"\"\n",
        "    Selects a subset of the data for cGAN training based on the specified mode.\n",
        "    \"\"\"\n",
        "    if mode == 'a':\n",
        "        # è¿”å›å…¨é‡æ•°æ®\n",
        "        return X, y\n",
        "\n",
        "    elif mode == 'm':\n",
        "        # è¿”å›æå°‘æ•°ç±»æ•°æ®\n",
        "        mask = np.isin(y, minority_labels)\n",
        "        return X[mask], y[mask]\n",
        "\n",
        "    elif mode == 'f':\n",
        "        mask = np.isin(y, minority_labels + [9, 12])\n",
        "        return X[mask], y[mask]\n",
        "\n",
        "    elif mode == 'b':\n",
        "        # è¿”å›å¹³è¡¡åçš„è®­ç»ƒæ•°æ® (å°†å¤šæ•°ç±»éƒ½ä¸‹é‡‡æ ·åˆ° 100_000)\n",
        "        strategy = {label: rus_target for label in np.unique(y) if label not in minority_labels}\n",
        "        rus = RandomUnderSampler(sampling_strategy=strategy, random_state=op_seed)\n",
        "        return rus.fit_resample(X, y)\n",
        "\n",
        "    else:\n",
        "        raise ValueError(f\"Invalid mode: {mode}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "OFGJh4Zq1QLh"
      },
      "outputs": [],
      "source": [
        "# Callback: adjust learning rates based on delta_loss\n",
        "class MyReduceLR(tf.keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    å¦‚æœ delta_loss è¿ç»­ {patience} æ¬¡å¤§äº {delta_threshold} æˆ–è€…å°äº -{delta_threshold}ï¼Œåˆ™è°ƒæ•´å­¦ä¹ ç‡ã€‚\n",
        "    \"\"\"\n",
        "    def __init__(self, delta_threshold=1.0, patience=5, factor=0.5, min_lr=1e-8):\n",
        "        super().__init__()\n",
        "        self.delta_threshold = delta_threshold\n",
        "        self.patience = patience\n",
        "        self.factor = factor\n",
        "        self.min_lr = min_lr\n",
        "\n",
        "        self.d_bad = 0\n",
        "        self.g_bad = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        delta = logs.get('delta_loss')\n",
        "        if delta is None:\n",
        "            return\n",
        "\n",
        "        # å¦‚æœ d_loss æ¯” g_loss å°ï¼Œå¹¶ä¸”ç›¸å·®å¤§äº delta_threshold\n",
        "        if delta <= -self.delta_threshold:\n",
        "            self.d_bad += 1\n",
        "        else:\n",
        "            self.d_bad = 0\n",
        "\n",
        "        # å¦‚æœ d_loss æ¯” g_loss å¤§ï¼Œå¹¶ä¸”ç›¸å·®å¤§äº delta_threshold\n",
        "        if delta >= self.delta_threshold:\n",
        "            self.g_bad += 1\n",
        "        else:\n",
        "            self.g_bad = 0\n",
        "\n",
        "        # Apply LR adjustments\n",
        "        if self.d_bad >= self.patience:\n",
        "            old_lr = float(self.model.d_optimizer.learning_rate)\n",
        "            new_lr = max(old_lr * self.factor, self.min_lr)\n",
        "            if new_lr < old_lr:\n",
        "              self.model.d_optimizer.learning_rate.assign(new_lr)\n",
        "              self.d_bad = 0\n",
        "              print(f\"\\n[{now()}] Adjust discriminator\\'s learning rate to {new_lr}\")\n",
        "        if self.g_bad >= self.patience:\n",
        "            old_lr = float(self.model.g_optimizer.learning_rate)\n",
        "            new_lr = max(old_lr * self.factor, self.min_lr)\n",
        "            if new_lr < old_lr:\n",
        "              self.model.g_optimizer.learning_rate.assign(new_lr)\n",
        "              self.g_bad = 0\n",
        "              print(f\"\\n[{now()}] Adjust generator\\'s learning rate to {new_lr}\")\n",
        "\n",
        "\n",
        "# Callback: early stopping when balanced\n",
        "class MyEarlyStopping(tf.keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    å¦‚æœ CGAN çš„ |delta_loss| è¿ç»­ {patience} æ¬¡åœ¨ {balance_tolerance} ä¹‹é—´ï¼Œè¡¨ç¤º CGAN å·²ç»æ”¶æ•›ï¼Œåœæ­¢è®­ç»ƒã€‚\n",
        "    \"\"\"\n",
        "    def __init__(self, balance_tolerance=0.1, patience=10):\n",
        "        super().__init__()\n",
        "        self.balance_tolerance = balance_tolerance\n",
        "        self.patience = patience\n",
        "        self.balance = 0\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        logs = logs or {}\n",
        "        delta = logs.get('delta_loss')\n",
        "        # print(f\"[{datetime.now().strftime('%x %X')}] {logs}\", file=sys.stderr)\n",
        "        # d = self.model.disc_loss_metric.result().numpy()\n",
        "        # g = self.model.gen_loss_metric.result().numpy()\n",
        "        # dl = self.model.delta_loss_metric.result().numpy()\n",
        "        # print(f\"[{datetime.now().strftime('%x %X')}] {d, g, dl}\", file=sys.stderr)\n",
        "\n",
        "        if delta is None:\n",
        "            return\n",
        "\n",
        "        # Check balance condition\n",
        "        if abs(delta) <= self.balance_tolerance:\n",
        "            self.balance += 1\n",
        "        else:\n",
        "            self.balance = 0\n",
        "\n",
        "        if self.balance >= self.patience:\n",
        "            self.model.stop_training = True\n",
        "            print(f\"\\n[{datetime.now().strftime('%x %X')}] Early stopping triggered. CGAN is balanced.\")\n",
        "\n",
        "\n",
        "class BalanceCallback(tf.keras.callbacks.Callback):\n",
        "    def __init__(self, delta_threshold=0.8, extra_steps=2):\n",
        "        super().__init__()\n",
        "        self.delta_threshold = delta_threshold\n",
        "        self.extra_steps = extra_steps\n",
        "\n",
        "    def on_train_batch_end(self, batch, logs=None):\n",
        "        logs = logs or {}\n",
        "        d_loss = logs.get('discriminator_loss')\n",
        "        g_loss = logs.get('generator_loss')\n",
        "\n",
        "        if d_loss is not None and g_loss is not None:\n",
        "            delta_loss = d_loss - g_loss\n",
        "\n",
        "            if delta_loss < -self.delta_threshold:\n",
        "                # D-loss å¤ªä½ï¼Œéœ€è¦æš‚åœ\n",
        "                self.model.pause_D = True\n",
        "                self.model.extra_steps_G = self.extra_steps\n",
        "                # tf.print('æš‚åœ D')\n",
        "            elif delta_loss > self.delta_threshold:\n",
        "                # G-loss å¤ªä½ï¼Œéœ€è¦æš‚åœ\n",
        "                self.model.pause_G = True\n",
        "                self.model.extra_steps_D = self.extra_steps\n",
        "                # tf.print('æš‚åœ G')\n",
        "            else:\n",
        "                self.model.pause_D = False\n",
        "                self.model.pause_G = False\n",
        "                self.model.extra_steps_D = 0\n",
        "                self.model.extra_steps_G = 0\n",
        "\n",
        "class SaveOnInterval(tf.keras.callbacks.Callback):\n",
        "    \"\"\"\n",
        "    æ¯ Interval è½®çš„æ—¶å€™ï¼Œä¿å­˜ä¸€æ¬¡æ¨¡å‹\n",
        "    \"\"\"\n",
        "    def __init__(self, interval, dest_gen_file, dest_dis_file):\n",
        "        super().__init__()\n",
        "        self.interval = interval\n",
        "        self.dest_gen_file = dest_gen_file\n",
        "        self.dest_dis_file = dest_dis_file\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "        # epoch ä» 0 å¼€å§‹ç®—ï¼Œæ•… +1\n",
        "        epoch_num = epoch + 1\n",
        "        max_epochs = self.params['epochs']\n",
        "\n",
        "        if epoch_num % self.interval == 0 and epoch_num < max_epochs:\n",
        "            gen_file = self.dest_gen_file.with_name(self.dest_gen_file.name.replace(f'e{max_epochs},', f'e{epoch_num},'))\n",
        "            dis_file = self.dest_dis_file.with_name(self.dest_dis_file.name.replace(f'e{max_epochs},', f'e{epoch_num},'))\n",
        "\n",
        "            self.model.generator.save(gen_file)\n",
        "            self.model.discriminator.save(dis_file)\n",
        "\n",
        "            print(f'\\n[{now()}] ğŸ’¾ Saved model at epoch {epoch_num} to: {gen_file}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lUwtAak0tyuJ"
      },
      "source": [
        "### å®šä¹‰ cGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ySil5tAjiOvy"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, Model\n",
        "\n",
        "# ---------------- Generator Definition ----------------\n",
        "def build_generator(num_classes: int,\n",
        "                    feature_dim: int,\n",
        "                    noise_dim: int = 128,\n",
        "                    embed_dim: int = 64,\n",
        "                    hidden_dims: list = [128, 256, 512]\n",
        "                    ) -> Model:\n",
        "\n",
        "    print(f\"[{datetime.now().strftime('%x %X')}] Building generator with: num_classes={num_classes}, feature_dim={feature_dim}, noise_dim={noise_dim}, embed_dim={embed_dim}, hidden_dims={hidden_dims}\")\n",
        "\n",
        "    # å®šä¹‰è¾“å…¥å±‚\n",
        "    noise_input = layers.Input(shape=(noise_dim,), name='noise_input')         # å™ªå£°è¾“å…¥ï¼Œè¾“å…¥ä¸€ä¸ª noise_dim ç»´çš„å‘é‡ä½œä¸ºå™ªå£°\n",
        "    label_input = layers.Input(shape=(1,), dtype='int32', name='label_input')  # ç±»åˆ«è¾“å…¥, è¾“å…¥ä¸€ä¸ªæ•´æ•°ä½œä¸ºç±»åˆ«æ ‡ç­¾\n",
        "\n",
        "    # ç±»åˆ«åµŒå…¥å±‚\n",
        "    label_embedding = layers.Embedding(input_dim=num_classes, output_dim=embed_dim)(label_input)  # å°†ç±»åˆ«æ ‡ç­¾æ˜ å°„ä¸ºåµŒå…¥å‘é‡(embed_dim ç»´)\n",
        "    label_embedding = layers.Flatten()(label_embedding)  # å°†åµŒå…¥å‘é‡å±•å¹³, (1, embed_dim) -> (embed_dim,)\n",
        "\n",
        "    # åˆå¹¶ å™ªå£°å‘é‡ å’Œ ç±»åˆ«åµŒå…¥å‘é‡\n",
        "    # åˆå¹¶åçš„å‘é‡ç»´åº¦ä¸º (noise_dim + embed_dim,)\n",
        "    x = layers.Concatenate()([noise_input, label_embedding])\n",
        "\n",
        "    # éšè—å±‚ (æ¿€æ´»å‡½æ•°ä¸º relu)\n",
        "    for dim in hidden_dims:\n",
        "        x = layers.Dense(dim, activation='relu')(x)\n",
        "        x = layers.BatchNormalization()(x)\n",
        "\n",
        "    # è¾“å‡ºå±‚çš„æ¿€æ´»å‡½æ•°è¦è·Ÿæ•°æ®é¢„å¤„ç†çš„ scaling æ–¹æ³•é€‚é…\n",
        "    activation = 'sigmoid' if scaling_method in ['minmax', 'l1pminmax'] else 'linear'\n",
        "\n",
        "    # è¾“å‡ºå±‚ (è¾“å‡º feature_dim ç»´çš„ç‰¹å¾å‘é‡)\n",
        "    output = layers.Dense(feature_dim, activation=activation, name='generated_data')(x)\n",
        "\n",
        "    # è¿”å›ç”Ÿæˆå™¨æ¨¡å‹\n",
        "    # è¯¥æ¨¡å‹æ¥å—ä¸¤ä¸ªè¾“å…¥ï¼Œä¸€ä¸ªæ˜¯å™ªå£°å‘é‡ï¼Œä¸€ä¸ªæ˜¯ç±»åˆ«æ ‡ç­¾. è¾“å‡ºç”Ÿæˆçš„ç‰¹å¾å‘é‡\n",
        "    return Model([noise_input, label_input], output, name=\"Generator\")\n",
        "\n",
        "\n",
        "# ---------------- Discriminator Definition ----------------\n",
        "def build_discriminator(num_classes: int, feature_dim: int,\n",
        "                        embed_dim: int = 64,\n",
        "                        hidden_dims: list = [256, 128]\n",
        "                        ) -> Model:\n",
        "\n",
        "    print(f\"[{datetime.now().strftime('%x %X')}] Building discriminator with: num_classes={num_classes}, feature_dim={feature_dim}, embed_dim={embed_dim}, hidden_dims={hidden_dims}\")\n",
        "\n",
        "    # å®šä¹‰è¾“å…¥å±‚\n",
        "    data_input = layers.Input(shape=(feature_dim,), name='data_input')  # è¾“å…¥ä¸€ä¸ªç‰¹å¾å‘é‡ (feature_dim ç»´)\n",
        "    label_input = layers.Input(shape=(1,), dtype='int32', name='label_input')  # è¾“å…¥ä¸€ä¸ªç±»åˆ«æ ‡ç­¾ (æ•´æ•°)\n",
        "\n",
        "    # ç±»åˆ«åµŒå…¥å±‚\n",
        "    label_embedding = layers.Embedding(input_dim=num_classes, output_dim=embed_dim)(label_input) # å°†ç±»åˆ«æ ‡ç­¾æ˜ å°„ä¸ºåµŒå…¥å‘é‡(embed_dim ç»´)\n",
        "    label_embedding = layers.Flatten()(label_embedding)  # å°†åµŒå…¥å‘é‡å±•å¹³, (1, embed_dim) -> (embed_dim,)\n",
        "\n",
        "    # åˆå¹¶ ç‰¹å¾å‘é‡ å’Œ ç±»åˆ«åµŒå…¥å‘é‡\n",
        "    # åˆå¹¶åçš„å‘é‡ç»´åº¦ä¸º (feature_dim + embed_dim,)\n",
        "    x = layers.Concatenate()([data_input, label_embedding])\n",
        "\n",
        "    # åˆ¤åˆ«å™¨ç½‘ç»œç»“æ„\n",
        "    for dim in hidden_dims:\n",
        "        x = layers.Dense(dim)(x)\n",
        "        x = layers.LeakyReLU(0.2)(x)\n",
        "        x = layers.Dropout(0.4)(x)\n",
        "\n",
        "    # è¾“å‡ºå±‚ (sigmoid æ¿€æ´»å‡½æ•°. è¾“å‡ºä¸º [0, 1] åŒºé—´, æ¥è¿‘0: åˆ¤æ–­ä¸ºå‡æ•°æ®, æ¥è¿‘1: åˆ¤æ–­ä¸ºçœŸå®æ•°æ®)\n",
        "    output = layers.Dense(1, activation='sigmoid', name='validity')(x)\n",
        "\n",
        "    # è¿”å›åˆ¤åˆ«å™¨æ¨¡å‹\n",
        "    # è¯¥æ¨¡å‹æ¥å—ä¸¤ä¸ªè¾“å…¥ï¼Œä¸€ä¸ªæ˜¯ç‰¹å¾å‘é‡ï¼Œä¸€ä¸ªæ˜¯ç±»åˆ«æ ‡ç­¾. è¾“å‡ºåˆ¤åˆ«ç»“æœ(çœŸ/å‡ çš„æ¦‚ç‡)\n",
        "    return Model([data_input, label_input], output, name=\"Discriminator\")\n",
        "\n",
        "\n",
        "# ---------------- CGAN Model with Custom train_step ----------------\n",
        "class ConditionalGAN(Model):\n",
        "    def __init__(self,\n",
        "                 generator,\n",
        "                 discriminator,\n",
        "                 seen_labels: list = [],\n",
        "                 **kwargs):\n",
        "        super(ConditionalGAN, self).__init__(**kwargs)\n",
        "        self.generator = generator\n",
        "        self.discriminator = discriminator\n",
        "        self.seen_labels = tf.constant(seen_labels, dtype=tf.int32)\n",
        "\n",
        "        self.noise_dim = generator.input_shape[0][1]\n",
        "\n",
        "        self.pause_D = False\n",
        "        self.pause_G = False\n",
        "        self.extra_steps_D = 0\n",
        "        self.extra_steps_G = 0\n",
        "\n",
        "        self.gen_loss_metric = tf.keras.metrics.Mean(name=\"generator_loss\")\n",
        "        self.disc_loss_metric = tf.keras.metrics.Mean(name=\"discriminator_loss\")\n",
        "        self.delta_loss_metric = tf.keras.metrics.Mean(name=\"delta_loss\")\n",
        "        pass\n",
        "\n",
        "    @property\n",
        "    def metrics(self):\n",
        "        return [self.disc_loss_metric,\n",
        "                self.gen_loss_metric,\n",
        "                self.delta_loss_metric,\n",
        "                ]\n",
        "\n",
        "    def compile(self, g_optimizer, d_optimizer, loss_fn):\n",
        "        super().compile()\n",
        "        self.g_optimizer = g_optimizer\n",
        "        self.d_optimizer = d_optimizer\n",
        "        self.loss_fn = loss_fn\n",
        "\n",
        "    def _train_D(self, real_x, real_y):\n",
        "        real_size = tf.shape(real_x)[0]\n",
        "\n",
        "        ## è®­ç»ƒåˆ¤åˆ«å™¨ ##\n",
        "        # ç”Ÿæˆä¸€ç»„ (real_size,1) çš„å‡æ ‡ç­¾\n",
        "        # æ–¹æ³•1: ä» seen_labels ä¸­éšæœºå–å€¼ (ä¼˜ç‚¹æ˜¯å®Œå…¨éšæœºï¼Œå¯¹å°‘æ•°ç±»åˆ«å‹å¥½ã€‚ç¼ºç‚¹æ˜¯æ›´éš¾æ”¶æ•›)\n",
        "        idx = tf.random.uniform((real_size,), maxval=tf.shape(self.seen_labels)[0], dtype=tf.int32)\n",
        "        fake_y = tf.expand_dims(tf.gather(self.seen_labels, idx), axis=1)  # shape (real_size,1)\n",
        "        # æ–¹æ³•2: ç”± real_y æ‰“ä¹±é¡ºåºè·å¾— (ä¼˜ç‚¹æ˜¯ç®€å•ï¼Œä¿æŒåŸç±»åˆ«æ¯”ä¾‹ï¼Œæœ‰åŠ©äºæ”¶æ•›)\n",
        "        # fake_y = tf.random.shuffle(real_y)\n",
        "\n",
        "        with tf.GradientTape() as tape_d:\n",
        "            self.discriminator.trainable = True\n",
        "            fake_x = self.generator([tf.random.normal((real_size, self.noise_dim)), fake_y], training=False)\n",
        "\n",
        "            real_validity = self.discriminator([real_x, real_y], training=True)\n",
        "            fake_validity = self.discriminator([fake_x, fake_y], training=True)\n",
        "\n",
        "            d_loss_real = self.loss_fn(tf.ones_like(real_validity), real_validity)  # å¯¹çœŸå®æ ·æœ¬çš„æŸå¤±\n",
        "            d_loss_fake = self.loss_fn(tf.zeros_like(fake_validity), fake_validity)  # å¯¹ç”Ÿæˆæ ·æœ¬çš„æŸå¤±\n",
        "            d_loss = 0.5 * (d_loss_real + d_loss_fake)  # å¹³å‡æŸå¤±\n",
        "\n",
        "        # è®¡ç®—æ¢¯åº¦ï¼Œæ›´æ–°åˆ¤åˆ«å™¨å‚æ•°\n",
        "        grads_d = tape_d.gradient(d_loss, self.discriminator.trainable_weights)\n",
        "        self.d_optimizer.apply_gradients( zip(grads_d, self.discriminator.trainable_weights) )\n",
        "\n",
        "        self.disc_loss_metric.update_state(d_loss)\n",
        "        pass\n",
        "\n",
        "    def _train_G(self, real_x, real_y):\n",
        "        real_size = tf.shape(real_x)[0]\n",
        "\n",
        "        ## è®­ç»ƒç”Ÿæˆå™¨ ##\n",
        "        idx = tf.random.uniform((real_size,), maxval=tf.shape(self.seen_labels)[0], dtype=tf.int32)\n",
        "        fake_y = tf.expand_dims(tf.gather(self.seen_labels, idx), axis=1)  # shape (real_size,1)\n",
        "        # fake_y = tf.random.shuffle(real_y)\n",
        "\n",
        "        with tf.GradientTape() as tape_g:\n",
        "            self.discriminator.trainable = False\n",
        "            # è¦åœ¨ç”Ÿæˆå™¨çš„ tap å†…éƒ¨è°ƒç”¨ç”Ÿæˆå™¨\n",
        "            fake_x = self.generator([tf.random.normal((real_size, self.noise_dim)), fake_y], training=True)\n",
        "\n",
        "            fake_validity = self.discriminator([fake_x, fake_y], training=False)\n",
        "            g_loss = self.loss_fn(tf.ones_like(fake_validity), fake_validity)\n",
        "\n",
        "        # è®¡ç®—æ¢¯åº¦ï¼Œæ›´æ–°ç”Ÿæˆå™¨å‚æ•°\n",
        "        grads_g = tape_g.gradient(g_loss, self.generator.trainable_weights)\n",
        "        self.g_optimizer.apply_gradients( zip(grads_g, self.generator.trainable_weights) )\n",
        "        self.discriminator.trainable = True\n",
        "\n",
        "        self.gen_loss_metric.update_state(g_loss)\n",
        "        pass\n",
        "\n",
        "\n",
        "    # fit æ–¹æ³•ä¼šè‡ªåŠ¨è°ƒç”¨ train_step, æ¯æ¬¡ä¼ é€’ batch_size çš„ data ç»™å®ƒ\n",
        "    def train_step(self, data):\n",
        "        real_x, real_y = data\n",
        "\n",
        "        if not self.pause_D:\n",
        "            for _ in range(1 + self.extra_steps_D):\n",
        "                self._train_D(real_x, real_y)\n",
        "\n",
        "        if not self.pause_G:\n",
        "            for _ in range(1 + self.extra_steps_G):\n",
        "                self._train_G(real_x, real_y)\n",
        "\n",
        "        batch_delta = self.disc_loss_metric.result() - self.gen_loss_metric.result()\n",
        "        self.delta_loss_metric.update_state(batch_delta)\n",
        "\n",
        "        return {\n",
        "            \"discriminator_loss\": self.disc_loss_metric.result(),\n",
        "            \"generator_loss\": self.gen_loss_metric.result(),\n",
        "            \"delta_loss\": self.delta_loss_metric.result(),\n",
        "            # 'g_loss': g_loss,\n",
        "            # 'd_loss': d_loss,\n",
        "            # 'd_loss_real': d_loss_real,\n",
        "            # 'd_loss_fake': d_loss_fake,\n",
        "            }\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDAkD88lY-gE"
      },
      "source": [
        "## 1ï¸âƒ£ åˆ©ç”¨ ROS æå‰è¡¥å……æå°‘æ•°ç±»æ ·æœ¬\n",
        "- å…ˆç”¨ ROS éšæœºå¤åˆ¶çš„æ–¹å¼ï¼Œå°†æå°‘æ•°ç±»æ ·æœ¬æ‰©å±•åˆ°å¯æ¥å—çš„ç¨‹åº¦åå†è¿›è¡Œ oversampling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "vfzi08QuZwQV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "95d00cb7-2e58-457b-985c-8eb0a0aadcd3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/24/25 23:14:40 PDT] No need to apply ROS oversampling.\n"
          ]
        }
      ],
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "oversample_to = {}\n",
        "# åˆ¤æ–­ oversampling_method å­—ç¬¦ä¸²å¼€å¤´æ˜¯å¦ä¸º ros\n",
        "if oversampling_method.startswith('ros'):\n",
        "    ros_scheme = int(oversampling_method[3])\n",
        "    oversample_to = ros_schemes[ros_scheme]\n",
        "    print(f'[{now()}] Applying ROS oversampling to: {oversample_to}')\n",
        "\n",
        "    oversampler = RandomOverSampler(sampling_strategy=oversample_to, random_state=op_seed)\n",
        "    X, y = oversampler.fit_resample(X, y)\n",
        "\n",
        "    print(f'[{now()}] After ROS oversampling:')\n",
        "    print(f'  X.shape: {X.shape}, y.shape: {y.shape}')\n",
        "    print(f'  Labels: { {int(k): v for k, v in sorted(Counter(y).items())} }\\n')\n",
        "else:\n",
        "    print(f'[{now()}] No need to apply ROS oversampling.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG0WobNKt5iS"
      },
      "source": [
        "## 2ï¸âƒ£ åˆå§‹åŒ– å¹¶ è®­ç»ƒ cGAN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7TAUxg7ZEtcC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe61f78d-3918-4af7-8f81-08d3aef9a21f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[06/24/25 23:14:42 PDT] ğŸš€ Training cGAN [cgan-m(n128,f70,c15,e500,b512,gen[128,[128, 256, 512],0.0003],dis[64,[256, 128],0.0001])_generator]...\n",
            "[06/24/25 23:14:42 PDT] Selected X.shape: (44125, 70), y.shape: (44125,)\n",
            "[06/24/25 23:14:42 PDT] Selected labels: {2: 489, 3: 184, 5: 1384, 7: 33206, 10: 8792, 13: 70}\n",
            "[06/25/25 06:14:44] Building generator with: num_classes=15, feature_dim=70, noise_dim=128, embed_dim=128, hidden_dims=[128, 256, 512]\n",
            "[06/25/25 06:14:45] Building discriminator with: num_classes=15, feature_dim=70, embed_dim=64, hidden_dims=[256, 128]\n",
            "Epoch 1/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 71ms/step - delta_loss: -0.4150 - discriminator_loss: 0.5059 - generator_loss: 1.1715\n",
            "Epoch 2/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -2.1886 - discriminator_loss: 0.1846 - generator_loss: 2.6399\n",
            "Epoch 3/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -3.1044 - discriminator_loss: 0.2432 - generator_loss: 3.2759\n",
            "Epoch 4/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -2.6856 - discriminator_loss: 0.3416 - generator_loss: 2.9608\n",
            "Epoch 5/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -2.4278 - discriminator_loss: 0.2977 - generator_loss: 2.6871\n",
            "Epoch 6/500\n",
            "\u001b[1m77/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -2.2389 - discriminator_loss: 0.3285 - generator_loss: 2.5532\n",
            "[06/24/25 23:15:16 PDT] Adjust discriminator's learning rate to 4.999999873689376e-05\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -2.2369 - discriminator_loss: 0.3304 - generator_loss: 2.5510\n",
            "Epoch 7/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -2.1433 - discriminator_loss: 0.3709 - generator_loss: 2.5182\n",
            "Epoch 8/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -2.1656 - discriminator_loss: 0.3968 - generator_loss: 2.5696\n",
            "Epoch 9/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -2.0961 - discriminator_loss: 0.3884 - generator_loss: 2.4809\n",
            "Epoch 10/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -2.0573 - discriminator_loss: 0.3964 - generator_loss: 2.4320\n",
            "Epoch 11/500\n",
            "\u001b[1m86/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9507 - discriminator_loss: 0.4038 - generator_loss: 2.3604\n",
            "[06/24/25 23:15:17 PDT] Adjust discriminator's learning rate to 2.499999936844688e-05\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9508 - discriminator_loss: 0.4039 - generator_loss: 2.3607\n",
            "Epoch 12/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9993 - discriminator_loss: 0.4134 - generator_loss: 2.4136\n",
            "Epoch 13/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -2.0222 - discriminator_loss: 0.4185 - generator_loss: 2.4405\n",
            "Epoch 14/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9741 - discriminator_loss: 0.4228 - generator_loss: 2.4111\n",
            "Epoch 15/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -2.0466 - discriminator_loss: 0.4209 - generator_loss: 2.4463\n",
            "Epoch 16/500\n",
            "\u001b[1m73/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.8873 - discriminator_loss: 0.4071 - generator_loss: 2.2783\n",
            "[06/24/25 23:15:19 PDT] Adjust discriminator's learning rate to 1.249999968422344e-05\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.8846 - discriminator_loss: 0.4072 - generator_loss: 2.2792\n",
            "Epoch 17/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9248 - discriminator_loss: 0.4208 - generator_loss: 2.3432\n",
            "Epoch 18/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9509 - discriminator_loss: 0.4219 - generator_loss: 2.3645\n",
            "Epoch 19/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9221 - discriminator_loss: 0.4189 - generator_loss: 2.3344\n",
            "Epoch 20/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9367 - discriminator_loss: 0.4101 - generator_loss: 2.3367\n",
            "Epoch 21/500\n",
            "\u001b[1m73/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.8301 - discriminator_loss: 0.4070 - generator_loss: 2.2559\n",
            "[06/24/25 23:15:21 PDT] Adjust discriminator's learning rate to 6.24999984211172e-06\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.8341 - discriminator_loss: 0.4077 - generator_loss: 2.2658\n",
            "Epoch 22/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -2.0347 - discriminator_loss: 0.4174 - generator_loss: 2.4447\n",
            "Epoch 23/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -2.0734 - discriminator_loss: 0.4245 - generator_loss: 2.4874\n",
            "Epoch 24/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -2.0654 - discriminator_loss: 0.4240 - generator_loss: 2.4839\n",
            "Epoch 25/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -2.0596 - discriminator_loss: 0.4267 - generator_loss: 2.4878\n",
            "Epoch 26/500\n",
            "\u001b[1m75/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -2.0658 - discriminator_loss: 0.4266 - generator_loss: 2.5201\n",
            "[06/24/25 23:15:22 PDT] Adjust discriminator's learning rate to 3.12499992105586e-06\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -2.0701 - discriminator_loss: 0.4264 - generator_loss: 2.5221\n",
            "Epoch 27/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -2.1088 - discriminator_loss: 0.4308 - generator_loss: 2.5390\n",
            "Epoch 28/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -2.0298 - discriminator_loss: 0.4279 - generator_loss: 2.4446\n",
            "Epoch 29/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9459 - discriminator_loss: 0.4306 - generator_loss: 2.3854\n",
            "Epoch 30/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9422 - discriminator_loss: 0.4306 - generator_loss: 2.3805\n",
            "Epoch 31/500\n",
            "\u001b[1m86/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -2.0240 - discriminator_loss: 0.4285 - generator_loss: 2.4275\n",
            "[06/24/25 23:15:24 PDT] Adjust discriminator's learning rate to 1.56249996052793e-06\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -2.0234 - discriminator_loss: 0.4285 - generator_loss: 2.4271\n",
            "Epoch 32/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9603 - discriminator_loss: 0.4203 - generator_loss: 2.3839\n",
            "Epoch 33/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9580 - discriminator_loss: 0.4276 - generator_loss: 2.3844\n",
            "Epoch 34/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9763 - discriminator_loss: 0.4269 - generator_loss: 2.4080\n",
            "Epoch 35/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9897 - discriminator_loss: 0.4267 - generator_loss: 2.4066\n",
            "Epoch 36/500\n",
            "\u001b[1m85/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9835 - discriminator_loss: 0.4204 - generator_loss: 2.3968\n",
            "[06/24/25 23:15:26 PDT] Adjust discriminator's learning rate to 7.81249980263965e-07\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9832 - discriminator_loss: 0.4205 - generator_loss: 2.3970\n",
            "Epoch 37/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9581 - discriminator_loss: 0.4252 - generator_loss: 2.3855\n",
            "Epoch 38/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9588 - discriminator_loss: 0.4260 - generator_loss: 2.3900\n",
            "Epoch 39/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9487 - discriminator_loss: 0.4265 - generator_loss: 2.3752\n",
            "Epoch 40/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9601 - discriminator_loss: 0.4274 - generator_loss: 2.3785\n",
            "Epoch 41/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9426 - discriminator_loss: 0.4270 - generator_loss: 2.3869\n",
            "[06/24/25 23:15:27 PDT] Adjust discriminator's learning rate to 3.906249901319825e-07\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9428 - discriminator_loss: 0.4271 - generator_loss: 2.3870\n",
            "Epoch 42/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9488 - discriminator_loss: 0.4240 - generator_loss: 2.3800\n",
            "Epoch 43/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9598 - discriminator_loss: 0.4339 - generator_loss: 2.3787\n",
            "Epoch 44/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.8936 - discriminator_loss: 0.4456 - generator_loss: 2.3523\n",
            "Epoch 45/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9366 - discriminator_loss: 0.4442 - generator_loss: 2.3744\n",
            "Epoch 46/500\n",
            "\u001b[1m71/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9365 - discriminator_loss: 0.4435 - generator_loss: 2.3801\n",
            "[06/24/25 23:15:29 PDT] Adjust discriminator's learning rate to 1.9531249506599124e-07\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9366 - discriminator_loss: 0.4437 - generator_loss: 2.3809\n",
            "Epoch 47/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9810 - discriminator_loss: 0.4422 - generator_loss: 2.4016\n",
            "Epoch 48/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9715 - discriminator_loss: 0.4415 - generator_loss: 2.3965\n",
            "Epoch 49/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9432 - discriminator_loss: 0.4454 - generator_loss: 2.3915\n",
            "Epoch 50/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9606 - discriminator_loss: 0.4430 - generator_loss: 2.4021\n",
            "Epoch 51/500\n",
            "\u001b[1m74/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9266 - discriminator_loss: 0.4400 - generator_loss: 2.3818\n",
            "[06/24/25 23:15:30 PDT] Adjust discriminator's learning rate to 9.765624753299562e-08\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9292 - discriminator_loss: 0.4402 - generator_loss: 2.3842\n",
            "Epoch 52/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9513 - discriminator_loss: 0.4417 - generator_loss: 2.3896\n",
            "Epoch 53/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9102 - discriminator_loss: 0.4409 - generator_loss: 2.3704\n",
            "Epoch 54/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -2.0116 - discriminator_loss: 0.4380 - generator_loss: 2.4182\n",
            "Epoch 55/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9278 - discriminator_loss: 0.4444 - generator_loss: 2.3878\n",
            "Epoch 56/500\n",
            "\u001b[1m74/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9714 - discriminator_loss: 0.4451 - generator_loss: 2.4052\n",
            "[06/24/25 23:15:32 PDT] Adjust discriminator's learning rate to 4.882812376649781e-08\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9695 - discriminator_loss: 0.4448 - generator_loss: 2.4043\n",
            "Epoch 57/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9363 - discriminator_loss: 0.4396 - generator_loss: 2.3884\n",
            "Epoch 58/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9257 - discriminator_loss: 0.4410 - generator_loss: 2.3736\n",
            "Epoch 59/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9572 - discriminator_loss: 0.4463 - generator_loss: 2.4011\n",
            "Epoch 60/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9111 - discriminator_loss: 0.4453 - generator_loss: 2.3677\n",
            "Epoch 61/500\n",
            "\u001b[1m85/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9808 - discriminator_loss: 0.4410 - generator_loss: 2.4123\n",
            "[06/24/25 23:15:34 PDT] Adjust discriminator's learning rate to 2.4414061883248905e-08\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9805 - discriminator_loss: 0.4410 - generator_loss: 2.4120\n",
            "Epoch 62/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9293 - discriminator_loss: 0.4429 - generator_loss: 2.3749\n",
            "Epoch 63/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9846 - discriminator_loss: 0.4423 - generator_loss: 2.4158\n",
            "Epoch 64/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9569 - discriminator_loss: 0.4411 - generator_loss: 2.3996\n",
            "Epoch 65/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9522 - discriminator_loss: 0.4462 - generator_loss: 2.3895\n",
            "Epoch 66/500\n",
            "\u001b[1m73/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9477 - discriminator_loss: 0.4433 - generator_loss: 2.3853\n",
            "[06/24/25 23:15:35 PDT] Adjust discriminator's learning rate to 1.2207030941624453e-08\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9467 - discriminator_loss: 0.4435 - generator_loss: 2.3859\n",
            "Epoch 67/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9381 - discriminator_loss: 0.4458 - generator_loss: 2.3923\n",
            "Epoch 68/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9522 - discriminator_loss: 0.4410 - generator_loss: 2.3964\n",
            "Epoch 69/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.8922 - discriminator_loss: 0.4441 - generator_loss: 2.3629\n",
            "Epoch 70/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9561 - discriminator_loss: 0.4426 - generator_loss: 2.3968\n",
            "Epoch 71/500\n",
            "\u001b[1m85/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9043 - discriminator_loss: 0.4438 - generator_loss: 2.3695\n",
            "[06/24/25 23:15:37 PDT] Adjust discriminator's learning rate to 1e-08\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9050 - discriminator_loss: 0.4439 - generator_loss: 2.3700\n",
            "Epoch 72/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9308 - discriminator_loss: 0.4444 - generator_loss: 2.3815\n",
            "Epoch 73/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9499 - discriminator_loss: 0.4457 - generator_loss: 2.3977\n",
            "Epoch 74/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9610 - discriminator_loss: 0.4377 - generator_loss: 2.3881\n",
            "Epoch 75/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9476 - discriminator_loss: 0.4440 - generator_loss: 2.3844\n",
            "Epoch 76/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9588 - discriminator_loss: 0.4428 - generator_loss: 2.3961\n",
            "Epoch 77/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9607 - discriminator_loss: 0.4385 - generator_loss: 2.3910\n",
            "Epoch 78/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9404 - discriminator_loss: 0.4435 - generator_loss: 2.3912\n",
            "Epoch 79/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9654 - discriminator_loss: 0.4346 - generator_loss: 2.3953\n",
            "Epoch 80/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9440 - discriminator_loss: 0.4405 - generator_loss: 2.3937\n",
            "Epoch 81/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9855 - discriminator_loss: 0.4433 - generator_loss: 2.4127\n",
            "Epoch 82/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9447 - discriminator_loss: 0.4427 - generator_loss: 2.3818\n",
            "Epoch 83/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9401 - discriminator_loss: 0.4403 - generator_loss: 2.3868\n",
            "Epoch 84/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9371 - discriminator_loss: 0.4444 - generator_loss: 2.3864\n",
            "Epoch 85/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9374 - discriminator_loss: 0.4415 - generator_loss: 2.3861\n",
            "Epoch 86/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9724 - discriminator_loss: 0.4340 - generator_loss: 2.3952\n",
            "Epoch 87/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9384 - discriminator_loss: 0.4465 - generator_loss: 2.3905\n",
            "Epoch 88/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9305 - discriminator_loss: 0.4419 - generator_loss: 2.3904\n",
            "Epoch 89/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9486 - discriminator_loss: 0.4424 - generator_loss: 2.3982\n",
            "Epoch 90/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9424 - discriminator_loss: 0.4427 - generator_loss: 2.3806\n",
            "Epoch 91/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9704 - discriminator_loss: 0.4443 - generator_loss: 2.3951\n",
            "Epoch 92/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.8935 - discriminator_loss: 0.4430 - generator_loss: 2.3581\n",
            "Epoch 93/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9649 - discriminator_loss: 0.4390 - generator_loss: 2.3966\n",
            "Epoch 94/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9386 - discriminator_loss: 0.4420 - generator_loss: 2.3828\n",
            "Epoch 95/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9273 - discriminator_loss: 0.4470 - generator_loss: 2.3794\n",
            "Epoch 96/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9398 - discriminator_loss: 0.4430 - generator_loss: 2.3845\n",
            "Epoch 97/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9572 - discriminator_loss: 0.4418 - generator_loss: 2.3872\n",
            "Epoch 98/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9138 - discriminator_loss: 0.4426 - generator_loss: 2.3634\n",
            "Epoch 99/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9631 - discriminator_loss: 0.4446 - generator_loss: 2.4099\n",
            "Epoch 100/500\n",
            "\u001b[1m71/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”â”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9569 - discriminator_loss: 0.4435 - generator_loss: 2.3883\n",
            "[06/24/25 23:15:47 PDT] ğŸ’¾ Saved model at epoch 100 to: /content/drive/MyDrive/NYIT/880/data/balanced/models/minmax/cgan-m(n128,f70,c15,e100,b512,gen[128,[128, 256, 512],0.0003],dis[64,[256, 128],0.0001])_generator.keras\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - delta_loss: -1.9547 - discriminator_loss: 0.4436 - generator_loss: 2.3894\n",
            "Epoch 101/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9120 - discriminator_loss: 0.4425 - generator_loss: 2.3696\n",
            "Epoch 102/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9433 - discriminator_loss: 0.4420 - generator_loss: 2.3806\n",
            "Epoch 103/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9452 - discriminator_loss: 0.4459 - generator_loss: 2.3915\n",
            "Epoch 104/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9438 - discriminator_loss: 0.4389 - generator_loss: 2.3746\n",
            "Epoch 105/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9456 - discriminator_loss: 0.4475 - generator_loss: 2.3883\n",
            "Epoch 106/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9555 - discriminator_loss: 0.4449 - generator_loss: 2.3859\n",
            "Epoch 107/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9720 - discriminator_loss: 0.4415 - generator_loss: 2.4034\n",
            "Epoch 108/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9508 - discriminator_loss: 0.4419 - generator_loss: 2.3881\n",
            "Epoch 109/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9328 - discriminator_loss: 0.4479 - generator_loss: 2.3871\n",
            "Epoch 110/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9455 - discriminator_loss: 0.4439 - generator_loss: 2.3811\n",
            "Epoch 111/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9497 - discriminator_loss: 0.4425 - generator_loss: 2.3829\n",
            "Epoch 112/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9528 - discriminator_loss: 0.4424 - generator_loss: 2.3945\n",
            "Epoch 113/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9588 - discriminator_loss: 0.4426 - generator_loss: 2.3960\n",
            "Epoch 114/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9483 - discriminator_loss: 0.4423 - generator_loss: 2.3767\n",
            "Epoch 115/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9437 - discriminator_loss: 0.4479 - generator_loss: 2.3871\n",
            "Epoch 116/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9325 - discriminator_loss: 0.4416 - generator_loss: 2.3951\n",
            "Epoch 117/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9348 - discriminator_loss: 0.4433 - generator_loss: 2.3841\n",
            "Epoch 118/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9475 - discriminator_loss: 0.4434 - generator_loss: 2.3901\n",
            "Epoch 119/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9491 - discriminator_loss: 0.4403 - generator_loss: 2.3897\n",
            "Epoch 120/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9629 - discriminator_loss: 0.4465 - generator_loss: 2.4011\n",
            "Epoch 121/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9156 - discriminator_loss: 0.4452 - generator_loss: 2.3676\n",
            "Epoch 122/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9449 - discriminator_loss: 0.4487 - generator_loss: 2.3923\n",
            "Epoch 123/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9194 - discriminator_loss: 0.4464 - generator_loss: 2.3830\n",
            "Epoch 124/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9424 - discriminator_loss: 0.4456 - generator_loss: 2.3871\n",
            "Epoch 125/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9461 - discriminator_loss: 0.4374 - generator_loss: 2.3852\n",
            "Epoch 126/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9467 - discriminator_loss: 0.4435 - generator_loss: 2.3901\n",
            "Epoch 127/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9418 - discriminator_loss: 0.4423 - generator_loss: 2.3847\n",
            "Epoch 128/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9043 - discriminator_loss: 0.4453 - generator_loss: 2.3667\n",
            "Epoch 129/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9493 - discriminator_loss: 0.4416 - generator_loss: 2.3931\n",
            "Epoch 130/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9169 - discriminator_loss: 0.4416 - generator_loss: 2.3643\n",
            "Epoch 131/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9220 - discriminator_loss: 0.4412 - generator_loss: 2.3797\n",
            "Epoch 132/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9461 - discriminator_loss: 0.4451 - generator_loss: 2.3933\n",
            "Epoch 133/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9316 - discriminator_loss: 0.4432 - generator_loss: 2.3902\n",
            "Epoch 134/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9445 - discriminator_loss: 0.4391 - generator_loss: 2.3905\n",
            "Epoch 135/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9352 - discriminator_loss: 0.4441 - generator_loss: 2.3958\n",
            "Epoch 136/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9510 - discriminator_loss: 0.4403 - generator_loss: 2.3955\n",
            "Epoch 137/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9584 - discriminator_loss: 0.4397 - generator_loss: 2.3890\n",
            "Epoch 138/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.8982 - discriminator_loss: 0.4456 - generator_loss: 2.3646\n",
            "Epoch 139/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9711 - discriminator_loss: 0.4409 - generator_loss: 2.3941\n",
            "Epoch 140/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9545 - discriminator_loss: 0.4454 - generator_loss: 2.3939\n",
            "Epoch 141/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9322 - discriminator_loss: 0.4410 - generator_loss: 2.3752\n",
            "Epoch 142/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9720 - discriminator_loss: 0.4478 - generator_loss: 2.4033\n",
            "Epoch 143/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9248 - discriminator_loss: 0.4459 - generator_loss: 2.3830\n",
            "Epoch 144/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9488 - discriminator_loss: 0.4400 - generator_loss: 2.3953\n",
            "Epoch 145/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9302 - discriminator_loss: 0.4455 - generator_loss: 2.3761\n",
            "Epoch 146/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9297 - discriminator_loss: 0.4452 - generator_loss: 2.3888\n",
            "Epoch 147/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9513 - discriminator_loss: 0.4445 - generator_loss: 2.4075\n",
            "Epoch 148/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9315 - discriminator_loss: 0.4412 - generator_loss: 2.3798\n",
            "Epoch 149/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9463 - discriminator_loss: 0.4451 - generator_loss: 2.3914\n",
            "Epoch 150/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9423 - discriminator_loss: 0.4443 - generator_loss: 2.3842\n",
            "Epoch 151/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9523 - discriminator_loss: 0.4413 - generator_loss: 2.3928\n",
            "Epoch 152/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9539 - discriminator_loss: 0.4445 - generator_loss: 2.3923\n",
            "Epoch 153/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9423 - discriminator_loss: 0.4422 - generator_loss: 2.3891\n",
            "Epoch 154/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9543 - discriminator_loss: 0.4429 - generator_loss: 2.3999\n",
            "Epoch 155/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9326 - discriminator_loss: 0.4417 - generator_loss: 2.3759\n",
            "Epoch 156/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9344 - discriminator_loss: 0.4449 - generator_loss: 2.3779\n",
            "Epoch 157/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9269 - discriminator_loss: 0.4421 - generator_loss: 2.3852\n",
            "Epoch 158/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9530 - discriminator_loss: 0.4422 - generator_loss: 2.3954\n",
            "Epoch 159/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9417 - discriminator_loss: 0.4416 - generator_loss: 2.3831\n",
            "Epoch 160/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9655 - discriminator_loss: 0.4414 - generator_loss: 2.4033\n",
            "Epoch 161/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9541 - discriminator_loss: 0.4483 - generator_loss: 2.3961\n",
            "Epoch 162/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9910 - discriminator_loss: 0.4349 - generator_loss: 2.4056\n",
            "Epoch 163/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9301 - discriminator_loss: 0.4397 - generator_loss: 2.3803\n",
            "Epoch 164/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9202 - discriminator_loss: 0.4449 - generator_loss: 2.3743\n",
            "Epoch 165/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9617 - discriminator_loss: 0.4401 - generator_loss: 2.4010\n",
            "Epoch 166/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9733 - discriminator_loss: 0.4461 - generator_loss: 2.4119\n",
            "Epoch 167/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9202 - discriminator_loss: 0.4428 - generator_loss: 2.3779\n",
            "Epoch 168/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9949 - discriminator_loss: 0.4395 - generator_loss: 2.4169\n",
            "Epoch 169/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9487 - discriminator_loss: 0.4388 - generator_loss: 2.3924\n",
            "Epoch 170/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9617 - discriminator_loss: 0.4418 - generator_loss: 2.3987\n",
            "Epoch 171/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9102 - discriminator_loss: 0.4413 - generator_loss: 2.3653\n",
            "Epoch 172/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9473 - discriminator_loss: 0.4436 - generator_loss: 2.3858\n",
            "Epoch 173/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9316 - discriminator_loss: 0.4433 - generator_loss: 2.3857\n",
            "Epoch 174/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9437 - discriminator_loss: 0.4429 - generator_loss: 2.3835\n",
            "Epoch 175/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9497 - discriminator_loss: 0.4414 - generator_loss: 2.3925\n",
            "Epoch 176/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9485 - discriminator_loss: 0.4376 - generator_loss: 2.3913\n",
            "Epoch 177/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9608 - discriminator_loss: 0.4414 - generator_loss: 2.4029\n",
            "Epoch 178/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9515 - discriminator_loss: 0.4422 - generator_loss: 2.3987\n",
            "Epoch 179/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9299 - discriminator_loss: 0.4398 - generator_loss: 2.3728\n",
            "Epoch 180/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9603 - discriminator_loss: 0.4425 - generator_loss: 2.3973\n",
            "Epoch 181/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9939 - discriminator_loss: 0.4427 - generator_loss: 2.4123\n",
            "Epoch 182/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9543 - discriminator_loss: 0.4429 - generator_loss: 2.4066\n",
            "Epoch 183/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9384 - discriminator_loss: 0.4459 - generator_loss: 2.3894\n",
            "Epoch 184/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9636 - discriminator_loss: 0.4416 - generator_loss: 2.3895\n",
            "Epoch 185/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9304 - discriminator_loss: 0.4470 - generator_loss: 2.3769\n",
            "Epoch 186/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9573 - discriminator_loss: 0.4442 - generator_loss: 2.4030\n",
            "Epoch 187/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9264 - discriminator_loss: 0.4440 - generator_loss: 2.3836\n",
            "Epoch 188/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9134 - discriminator_loss: 0.4478 - generator_loss: 2.3681\n",
            "Epoch 189/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9549 - discriminator_loss: 0.4402 - generator_loss: 2.3894\n",
            "Epoch 190/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9520 - discriminator_loss: 0.4415 - generator_loss: 2.4020\n",
            "Epoch 191/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9309 - discriminator_loss: 0.4432 - generator_loss: 2.3837\n",
            "Epoch 192/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9655 - discriminator_loss: 0.4456 - generator_loss: 2.4033\n",
            "Epoch 193/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9576 - discriminator_loss: 0.4457 - generator_loss: 2.3999\n",
            "Epoch 194/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9560 - discriminator_loss: 0.4406 - generator_loss: 2.4013\n",
            "Epoch 195/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9553 - discriminator_loss: 0.4443 - generator_loss: 2.3921\n",
            "Epoch 196/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9410 - discriminator_loss: 0.4365 - generator_loss: 2.3823\n",
            "Epoch 197/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9566 - discriminator_loss: 0.4415 - generator_loss: 2.3917\n",
            "Epoch 198/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9445 - discriminator_loss: 0.4464 - generator_loss: 2.3915\n",
            "Epoch 199/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9202 - discriminator_loss: 0.4412 - generator_loss: 2.3821\n",
            "Epoch 200/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9357 - discriminator_loss: 0.4378 - generator_loss: 2.3749\n",
            "[06/24/25 23:16:20 PDT] ğŸ’¾ Saved model at epoch 200 to: /content/drive/MyDrive/NYIT/880/data/balanced/models/minmax/cgan-m(n128,f70,c15,e200,b512,gen[128,[128, 256, 512],0.0003],dis[64,[256, 128],0.0001])_generator.keras\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - delta_loss: -1.9358 - discriminator_loss: 0.4378 - generator_loss: 2.3750\n",
            "Epoch 201/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9524 - discriminator_loss: 0.4437 - generator_loss: 2.3988\n",
            "Epoch 202/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9087 - discriminator_loss: 0.4375 - generator_loss: 2.3595\n",
            "Epoch 203/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9253 - discriminator_loss: 0.4410 - generator_loss: 2.3801\n",
            "Epoch 204/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9275 - discriminator_loss: 0.4425 - generator_loss: 2.3810\n",
            "Epoch 205/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9220 - discriminator_loss: 0.4430 - generator_loss: 2.3740\n",
            "Epoch 206/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9607 - discriminator_loss: 0.4402 - generator_loss: 2.3865\n",
            "Epoch 207/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9558 - discriminator_loss: 0.4430 - generator_loss: 2.3961\n",
            "Epoch 208/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9313 - discriminator_loss: 0.4469 - generator_loss: 2.3865\n",
            "Epoch 209/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9539 - discriminator_loss: 0.4402 - generator_loss: 2.3964\n",
            "Epoch 210/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9480 - discriminator_loss: 0.4392 - generator_loss: 2.3842\n",
            "Epoch 211/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9605 - discriminator_loss: 0.4270 - generator_loss: 2.3844\n",
            "Epoch 212/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9308 - discriminator_loss: 0.4419 - generator_loss: 2.3828\n",
            "Epoch 213/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9371 - discriminator_loss: 0.4460 - generator_loss: 2.3937\n",
            "Epoch 214/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9546 - discriminator_loss: 0.4437 - generator_loss: 2.3918\n",
            "Epoch 215/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9718 - discriminator_loss: 0.4427 - generator_loss: 2.4174\n",
            "Epoch 216/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9218 - discriminator_loss: 0.4414 - generator_loss: 2.3737\n",
            "Epoch 217/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9570 - discriminator_loss: 0.4449 - generator_loss: 2.3968\n",
            "Epoch 218/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9632 - discriminator_loss: 0.4365 - generator_loss: 2.3972\n",
            "Epoch 219/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9418 - discriminator_loss: 0.4402 - generator_loss: 2.3805\n",
            "Epoch 220/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9323 - discriminator_loss: 0.4471 - generator_loss: 2.3836\n",
            "Epoch 221/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9107 - discriminator_loss: 0.4529 - generator_loss: 2.3789\n",
            "Epoch 222/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9416 - discriminator_loss: 0.4538 - generator_loss: 2.3912\n",
            "Epoch 223/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9265 - discriminator_loss: 0.4531 - generator_loss: 2.3908\n",
            "Epoch 224/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9342 - discriminator_loss: 0.4528 - generator_loss: 2.3891\n",
            "Epoch 225/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9052 - discriminator_loss: 0.4537 - generator_loss: 2.3685\n",
            "Epoch 226/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9506 - discriminator_loss: 0.4528 - generator_loss: 2.3876\n",
            "Epoch 227/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9449 - discriminator_loss: 0.4586 - generator_loss: 2.3933\n",
            "Epoch 228/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9426 - discriminator_loss: 0.4542 - generator_loss: 2.3976\n",
            "Epoch 229/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9475 - discriminator_loss: 0.4526 - generator_loss: 2.3877\n",
            "Epoch 230/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9715 - discriminator_loss: 0.4481 - generator_loss: 2.4040\n",
            "Epoch 231/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9420 - discriminator_loss: 0.4577 - generator_loss: 2.3950\n",
            "Epoch 232/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9329 - discriminator_loss: 0.4540 - generator_loss: 2.3920\n",
            "Epoch 233/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9261 - discriminator_loss: 0.4537 - generator_loss: 2.3863\n",
            "Epoch 234/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9581 - discriminator_loss: 0.4504 - generator_loss: 2.3958\n",
            "Epoch 235/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9448 - discriminator_loss: 0.4539 - generator_loss: 2.3969\n",
            "Epoch 236/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9495 - discriminator_loss: 0.4529 - generator_loss: 2.4007\n",
            "Epoch 237/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9453 - discriminator_loss: 0.4462 - generator_loss: 2.3916\n",
            "Epoch 238/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9367 - discriminator_loss: 0.4577 - generator_loss: 2.3827\n",
            "Epoch 239/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9433 - discriminator_loss: 0.4504 - generator_loss: 2.3895\n",
            "Epoch 240/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9476 - discriminator_loss: 0.4508 - generator_loss: 2.3888\n",
            "Epoch 241/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9485 - discriminator_loss: 0.4552 - generator_loss: 2.4072\n",
            "Epoch 242/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9655 - discriminator_loss: 0.4488 - generator_loss: 2.3903\n",
            "Epoch 243/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9359 - discriminator_loss: 0.4532 - generator_loss: 2.3864\n",
            "Epoch 244/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9159 - discriminator_loss: 0.4514 - generator_loss: 2.3745\n",
            "Epoch 245/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9164 - discriminator_loss: 0.4530 - generator_loss: 2.3887\n",
            "Epoch 246/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9457 - discriminator_loss: 0.4531 - generator_loss: 2.4038\n",
            "Epoch 247/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9468 - discriminator_loss: 0.4495 - generator_loss: 2.3824\n",
            "Epoch 248/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9755 - discriminator_loss: 0.4485 - generator_loss: 2.4127\n",
            "Epoch 249/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9765 - discriminator_loss: 0.4461 - generator_loss: 2.4102\n",
            "Epoch 250/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9277 - discriminator_loss: 0.4535 - generator_loss: 2.3823\n",
            "Epoch 251/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9547 - discriminator_loss: 0.4543 - generator_loss: 2.4076\n",
            "Epoch 252/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9081 - discriminator_loss: 0.4519 - generator_loss: 2.3805\n",
            "Epoch 253/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9334 - discriminator_loss: 0.4537 - generator_loss: 2.3949\n",
            "Epoch 254/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9445 - discriminator_loss: 0.4474 - generator_loss: 2.3882\n",
            "Epoch 255/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9363 - discriminator_loss: 0.4525 - generator_loss: 2.3897\n",
            "Epoch 256/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9291 - discriminator_loss: 0.4511 - generator_loss: 2.3898\n",
            "Epoch 257/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9096 - discriminator_loss: 0.4540 - generator_loss: 2.3792\n",
            "Epoch 258/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9494 - discriminator_loss: 0.4498 - generator_loss: 2.3818\n",
            "Epoch 259/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9270 - discriminator_loss: 0.4540 - generator_loss: 2.3851\n",
            "Epoch 260/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9362 - discriminator_loss: 0.4552 - generator_loss: 2.3944\n",
            "Epoch 261/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.8923 - discriminator_loss: 0.4573 - generator_loss: 2.3768\n",
            "Epoch 262/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9530 - discriminator_loss: 0.4555 - generator_loss: 2.4154\n",
            "Epoch 263/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9407 - discriminator_loss: 0.4548 - generator_loss: 2.3959\n",
            "Epoch 264/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9440 - discriminator_loss: 0.4519 - generator_loss: 2.3952\n",
            "Epoch 265/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.8893 - discriminator_loss: 0.4557 - generator_loss: 2.3697\n",
            "Epoch 266/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9641 - discriminator_loss: 0.4499 - generator_loss: 2.3979\n",
            "Epoch 267/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9591 - discriminator_loss: 0.4514 - generator_loss: 2.3927\n",
            "Epoch 268/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9769 - discriminator_loss: 0.4532 - generator_loss: 2.4074\n",
            "Epoch 269/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9581 - discriminator_loss: 0.4564 - generator_loss: 2.4138\n",
            "Epoch 270/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9585 - discriminator_loss: 0.4538 - generator_loss: 2.3961\n",
            "Epoch 271/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9711 - discriminator_loss: 0.4496 - generator_loss: 2.4109\n",
            "Epoch 272/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9712 - discriminator_loss: 0.4546 - generator_loss: 2.4149\n",
            "Epoch 273/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9578 - discriminator_loss: 0.4489 - generator_loss: 2.4002\n",
            "Epoch 274/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9486 - discriminator_loss: 0.4553 - generator_loss: 2.4041\n",
            "Epoch 275/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9309 - discriminator_loss: 0.4526 - generator_loss: 2.3848\n",
            "Epoch 276/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9507 - discriminator_loss: 0.4481 - generator_loss: 2.3901\n",
            "Epoch 277/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9398 - discriminator_loss: 0.4532 - generator_loss: 2.3848\n",
            "Epoch 278/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9450 - discriminator_loss: 0.4557 - generator_loss: 2.4020\n",
            "Epoch 279/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9406 - discriminator_loss: 0.4524 - generator_loss: 2.3942\n",
            "Epoch 280/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9568 - discriminator_loss: 0.4532 - generator_loss: 2.3922\n",
            "Epoch 281/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9323 - discriminator_loss: 0.4571 - generator_loss: 2.3912\n",
            "Epoch 282/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9469 - discriminator_loss: 0.4542 - generator_loss: 2.3909\n",
            "Epoch 283/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9428 - discriminator_loss: 0.4520 - generator_loss: 2.3941\n",
            "Epoch 284/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9304 - discriminator_loss: 0.4584 - generator_loss: 2.3835\n",
            "Epoch 285/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9338 - discriminator_loss: 0.4572 - generator_loss: 2.3844\n",
            "Epoch 286/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9450 - discriminator_loss: 0.4520 - generator_loss: 2.3872\n",
            "Epoch 287/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9539 - discriminator_loss: 0.4512 - generator_loss: 2.4086\n",
            "Epoch 288/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9321 - discriminator_loss: 0.4557 - generator_loss: 2.3787\n",
            "Epoch 289/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9334 - discriminator_loss: 0.4581 - generator_loss: 2.3928\n",
            "Epoch 290/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9329 - discriminator_loss: 0.4541 - generator_loss: 2.3856\n",
            "Epoch 291/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9452 - discriminator_loss: 0.4534 - generator_loss: 2.3964\n",
            "Epoch 292/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9225 - discriminator_loss: 0.4541 - generator_loss: 2.3879\n",
            "Epoch 293/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9398 - discriminator_loss: 0.4564 - generator_loss: 2.3938\n",
            "Epoch 294/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9515 - discriminator_loss: 0.4570 - generator_loss: 2.4050\n",
            "Epoch 295/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9471 - discriminator_loss: 0.4532 - generator_loss: 2.3987\n",
            "Epoch 296/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9712 - discriminator_loss: 0.4471 - generator_loss: 2.4019\n",
            "Epoch 297/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9424 - discriminator_loss: 0.4568 - generator_loss: 2.4044\n",
            "Epoch 298/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9553 - discriminator_loss: 0.4568 - generator_loss: 2.4039\n",
            "Epoch 299/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9173 - discriminator_loss: 0.4523 - generator_loss: 2.3714\n",
            "Epoch 300/500\n",
            "\u001b[1m74/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37mâ”â”â”\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9468 - discriminator_loss: 0.4579 - generator_loss: 2.4063\n",
            "[06/24/25 23:16:54 PDT] ğŸ’¾ Saved model at epoch 300 to: /content/drive/MyDrive/NYIT/880/data/balanced/models/minmax/cgan-m(n128,f70,c15,e300,b512,gen[128,[128, 256, 512],0.0003],dis[64,[256, 128],0.0001])_generator.keras\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - delta_loss: -1.9472 - discriminator_loss: 0.4576 - generator_loss: 2.4067\n",
            "Epoch 301/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9256 - discriminator_loss: 0.4543 - generator_loss: 2.3895\n",
            "Epoch 302/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9518 - discriminator_loss: 0.4558 - generator_loss: 2.4130\n",
            "Epoch 303/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9559 - discriminator_loss: 0.4576 - generator_loss: 2.4014\n",
            "Epoch 304/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9128 - discriminator_loss: 0.4573 - generator_loss: 2.3801\n",
            "Epoch 305/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9145 - discriminator_loss: 0.4552 - generator_loss: 2.3810\n",
            "Epoch 306/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9307 - discriminator_loss: 0.4533 - generator_loss: 2.3845\n",
            "Epoch 307/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9457 - discriminator_loss: 0.4514 - generator_loss: 2.3947\n",
            "Epoch 308/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9540 - discriminator_loss: 0.4495 - generator_loss: 2.3985\n",
            "Epoch 309/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9305 - discriminator_loss: 0.4556 - generator_loss: 2.3875\n",
            "Epoch 310/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9577 - discriminator_loss: 0.4564 - generator_loss: 2.4062\n",
            "Epoch 311/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9291 - discriminator_loss: 0.4568 - generator_loss: 2.4005\n",
            "Epoch 312/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9621 - discriminator_loss: 0.4519 - generator_loss: 2.4106\n",
            "Epoch 313/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9640 - discriminator_loss: 0.4533 - generator_loss: 2.4149\n",
            "Epoch 314/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9408 - discriminator_loss: 0.4557 - generator_loss: 2.4017\n",
            "Epoch 315/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9467 - discriminator_loss: 0.4525 - generator_loss: 2.3906\n",
            "Epoch 316/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9566 - discriminator_loss: 0.4565 - generator_loss: 2.4065\n",
            "Epoch 317/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.8907 - discriminator_loss: 0.4590 - generator_loss: 2.3675\n",
            "Epoch 318/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9161 - discriminator_loss: 0.4544 - generator_loss: 2.3742\n",
            "Epoch 319/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9261 - discriminator_loss: 0.4543 - generator_loss: 2.3847\n",
            "Epoch 320/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9451 - discriminator_loss: 0.4518 - generator_loss: 2.3998\n",
            "Epoch 321/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9761 - discriminator_loss: 0.4488 - generator_loss: 2.4123\n",
            "Epoch 322/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9297 - discriminator_loss: 0.4522 - generator_loss: 2.3991\n",
            "Epoch 323/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9701 - discriminator_loss: 0.4484 - generator_loss: 2.4152\n",
            "Epoch 324/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9417 - discriminator_loss: 0.4523 - generator_loss: 2.3868\n",
            "Epoch 325/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9711 - discriminator_loss: 0.4497 - generator_loss: 2.4185\n",
            "Epoch 326/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9520 - discriminator_loss: 0.4537 - generator_loss: 2.4001\n",
            "Epoch 327/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9443 - discriminator_loss: 0.4486 - generator_loss: 2.3956\n",
            "Epoch 328/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9377 - discriminator_loss: 0.4549 - generator_loss: 2.4008\n",
            "Epoch 329/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9212 - discriminator_loss: 0.4556 - generator_loss: 2.3820\n",
            "Epoch 330/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9360 - discriminator_loss: 0.4516 - generator_loss: 2.3921\n",
            "Epoch 331/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9342 - discriminator_loss: 0.4500 - generator_loss: 2.3941\n",
            "Epoch 332/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9355 - discriminator_loss: 0.4539 - generator_loss: 2.3912\n",
            "Epoch 333/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9587 - discriminator_loss: 0.4514 - generator_loss: 2.3922\n",
            "Epoch 334/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9220 - discriminator_loss: 0.4505 - generator_loss: 2.3818\n",
            "Epoch 335/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9715 - discriminator_loss: 0.4550 - generator_loss: 2.4142\n",
            "Epoch 336/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9782 - discriminator_loss: 0.4478 - generator_loss: 2.4087\n",
            "Epoch 337/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9681 - discriminator_loss: 0.4545 - generator_loss: 2.4221\n",
            "Epoch 338/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9232 - discriminator_loss: 0.4541 - generator_loss: 2.3925\n",
            "Epoch 339/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9443 - discriminator_loss: 0.4492 - generator_loss: 2.3952\n",
            "Epoch 340/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9282 - discriminator_loss: 0.4554 - generator_loss: 2.3978\n",
            "Epoch 341/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9305 - discriminator_loss: 0.4515 - generator_loss: 2.3942\n",
            "Epoch 342/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9638 - discriminator_loss: 0.4478 - generator_loss: 2.4084\n",
            "Epoch 343/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9538 - discriminator_loss: 0.4527 - generator_loss: 2.3965\n",
            "Epoch 344/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9543 - discriminator_loss: 0.4520 - generator_loss: 2.4085\n",
            "Epoch 345/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9480 - discriminator_loss: 0.4491 - generator_loss: 2.3974\n",
            "Epoch 346/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9216 - discriminator_loss: 0.4546 - generator_loss: 2.3878\n",
            "Epoch 347/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9573 - discriminator_loss: 0.4567 - generator_loss: 2.4079\n",
            "Epoch 348/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9375 - discriminator_loss: 0.4529 - generator_loss: 2.3958\n",
            "Epoch 349/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9438 - discriminator_loss: 0.4484 - generator_loss: 2.3919\n",
            "Epoch 350/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9714 - discriminator_loss: 0.4463 - generator_loss: 2.4115\n",
            "Epoch 351/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9124 - discriminator_loss: 0.4557 - generator_loss: 2.3866\n",
            "Epoch 352/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9199 - discriminator_loss: 0.4498 - generator_loss: 2.3792\n",
            "Epoch 353/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9155 - discriminator_loss: 0.4536 - generator_loss: 2.3787\n",
            "Epoch 354/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9529 - discriminator_loss: 0.4545 - generator_loss: 2.3979\n",
            "Epoch 355/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9445 - discriminator_loss: 0.4540 - generator_loss: 2.4045\n",
            "Epoch 356/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9718 - discriminator_loss: 0.4488 - generator_loss: 2.4161\n",
            "Epoch 357/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9639 - discriminator_loss: 0.4533 - generator_loss: 2.4005\n",
            "Epoch 358/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9550 - discriminator_loss: 0.4439 - generator_loss: 2.3972\n",
            "Epoch 359/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9631 - discriminator_loss: 0.4510 - generator_loss: 2.4022\n",
            "Epoch 360/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9557 - discriminator_loss: 0.4535 - generator_loss: 2.4115\n",
            "Epoch 361/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9472 - discriminator_loss: 0.4555 - generator_loss: 2.4149\n",
            "Epoch 362/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9591 - discriminator_loss: 0.4471 - generator_loss: 2.3990\n",
            "Epoch 363/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9681 - discriminator_loss: 0.4528 - generator_loss: 2.4047\n",
            "Epoch 364/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9182 - discriminator_loss: 0.4503 - generator_loss: 2.3835\n",
            "Epoch 365/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9828 - discriminator_loss: 0.4524 - generator_loss: 2.4207\n",
            "Epoch 366/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9673 - discriminator_loss: 0.4514 - generator_loss: 2.4048\n",
            "Epoch 367/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9498 - discriminator_loss: 0.4467 - generator_loss: 2.3987\n",
            "Epoch 368/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9619 - discriminator_loss: 0.4493 - generator_loss: 2.4105\n",
            "Epoch 369/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9478 - discriminator_loss: 0.4527 - generator_loss: 2.3957\n",
            "Epoch 370/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9412 - discriminator_loss: 0.4469 - generator_loss: 2.3954\n",
            "Epoch 371/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9503 - discriminator_loss: 0.4514 - generator_loss: 2.4127\n",
            "Epoch 372/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9567 - discriminator_loss: 0.4512 - generator_loss: 2.4086\n",
            "Epoch 373/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9366 - discriminator_loss: 0.4559 - generator_loss: 2.4046\n",
            "Epoch 374/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9321 - discriminator_loss: 0.4535 - generator_loss: 2.3926\n",
            "Epoch 375/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9688 - discriminator_loss: 0.4473 - generator_loss: 2.4146\n",
            "Epoch 376/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9721 - discriminator_loss: 0.4515 - generator_loss: 2.4075\n",
            "Epoch 377/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9345 - discriminator_loss: 0.4463 - generator_loss: 2.3884\n",
            "Epoch 378/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9212 - discriminator_loss: 0.4552 - generator_loss: 2.3944\n",
            "Epoch 379/500\n",
            "\u001b[1m87/87\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - delta_loss: -1.9622 - discriminator_loss: 0.4547 - generator_loss: 2.4140\n",
            "Epoch 380/500\n",
            "\u001b[1m 1/87\u001b[0m \u001b[37mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[1m30s\u001b[0m 349ms/step - delta_loss: -2.0417 - discriminator_loss: 0.4111 - generator_loss: 2.4528"
          ]
        }
      ],
      "source": [
        "feature_dim = X.shape[1]\n",
        "num_classes = len(np.unique(y))  # ç»Ÿä¸€ä½¿ç”¨å…¨å°ºå¯¸çš„ num_classes, å³ä½¿æ˜¯åªå¯¹å°‘æ•°ç±»è®­ç»ƒ CGAN. è¿™æ ·å°±ä¸ç”¨åšé¢å¤–çš„æ ‡ç­¾æ˜ å°„ã€‚\n",
        "\n",
        "# å®šä¹‰æ¨¡å‹ä¿å­˜ç›®å½•ä¸æ–‡ä»¶å\n",
        "save_dir = balanced_folder / 'models' / scaling_method\n",
        "os.makedirs(save_dir, exist_ok=True)\n",
        "generator_file = save_dir / (\n",
        "    f'{oversampling_method}('\n",
        "    f'n{noise_dim},f{feature_dim},c{num_classes},e{epochs},b{batch_size},'\n",
        "    f'gen[{gen_embed_dim},{gen_hidden_dims},{gen_learning_rate}],'\n",
        "    f'dis[{disc_embed_dim},{disc_hidden_dims},{disc_learning_rate}]'\n",
        "    ')_generator.keras'\n",
        ")\n",
        "discriminator_file = save_dir / (generator_file.stem[:-10] + '_discriminator.keras')\n",
        "cgan_file = save_dir / (generator_file.stem[:-10] + '_cgan.keras')\n",
        "\n",
        "\n",
        "if generator_file.exists() and not retrain:\n",
        "    # å¦‚æœå·²ç»å­˜åœ¨é¢„è®­ç»ƒçš„ç”Ÿæˆå™¨æ¨¡å‹ï¼Œåˆ™ç›´æ¥åŠ è½½\n",
        "    print(f\"[{now()}] ğŸ“¡ Loading pre-trained generator from {generator_file}\")\n",
        "    generator = tf.keras.models.load_model(generator_file)\n",
        "\n",
        "    print(f\"[{now()}] ğŸ“¡ Loading pre-trained discriminator from {discriminator_file}\")\n",
        "    discriminator = tf.keras.models.load_model(discriminator_file)\n",
        "else:\n",
        "    print(f\"[{now()}] ğŸš€ Training cGAN [{generator_file.stem}]...\")\n",
        "\n",
        "    ## 1ï¸âƒ£ é€‰æ‹©è¦ç”¨æ¥è®­ç»ƒçš„æ ·æœ¬\n",
        "    selected_X, selected_y = select_cgan_subset(X, y, oversampling_method[-1])\n",
        "    print(f\"[{now()}] Selected X.shape: {selected_X.shape}, y.shape: {selected_y.shape}\")\n",
        "    print(f\"[{now()}] Selected labels: { {int(k): v for k, v in sorted(Counter(selected_y).items())} }\")\n",
        "\n",
        "    train_dataset = tf.data.Dataset.from_tensor_slices((selected_X, selected_y))\n",
        "    train_dataset = train_dataset.shuffle(buffer_size).batch(batch_size).prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "    ## 2ï¸âƒ£ åˆå§‹åŒ– cGAN\n",
        "    generator = build_generator(num_classes, feature_dim, noise_dim=noise_dim, embed_dim=gen_embed_dim, hidden_dims=gen_hidden_dims)\n",
        "    discriminator = build_discriminator(num_classes, feature_dim, embed_dim=disc_embed_dim, hidden_dims=disc_hidden_dims)\n",
        "    cgan = ConditionalGAN(generator, discriminator, seen_labels=np.unique(selected_y))\n",
        "    cgan.compile(\n",
        "        g_optimizer=tf.keras.optimizers.Adam(learning_rate=gen_learning_rate, beta_1=0.5),\n",
        "        d_optimizer=tf.keras.optimizers.Adam(learning_rate=disc_learning_rate, beta_1=0.5),\n",
        "        loss_fn=tf.keras.losses.BinaryCrossentropy(from_logits=False)\n",
        "        )\n",
        "\n",
        "    ## 3ï¸âƒ£ è®­ç»ƒ cGAN\n",
        "    cgan.fit(train_dataset, epochs=epochs,\n",
        "             callbacks=[\n",
        "                 MyReduceLR(delta_threshold=0.8, patience=5, factor=0.5),\n",
        "                #  BalanceCallback(delta_threshold=0.3, extra_steps=3),\n",
        "                 SaveOnInterval(interval=100, dest_gen_file=generator_file, dest_dis_file=discriminator_file),\n",
        "                 MyEarlyStopping(),\n",
        "                 ]\n",
        "             )\n",
        "\n",
        "    ## 4ï¸âƒ£ ä¿å­˜æ¨¡å‹\n",
        "    generator.save(generator_file)\n",
        "    discriminator.save(discriminator_file)\n",
        "    cgan.build(input_shape=[(None, noise_dim), (None, 1)]) # cgan è¦ build åå†ä¿å­˜ï¼Œä¸ç„¶ä¼šæœ‰å‘Šè­¦\n",
        "    cgan.save(cgan_file)\n",
        "    print(f\"[{now()}] âœ… Saved generator model to {generator_file}\")  # å…¶å®åªéœ€è¦ä¿å­˜ generator å°±å¤Ÿäº†\n",
        "\n",
        "    # æ¸…ç†ä¸´æ—¶èµ„æº\n",
        "    del train_dataset\n",
        "    del selected_X\n",
        "    del selected_y"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dFXZWgCjzaj-"
      },
      "source": [
        "## 3ï¸âƒ£ ä½¿ç”¨ cGAN ç”Ÿæˆæ–°æ ·æœ¬"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ld29W8B7aIdk"
      },
      "outputs": [],
      "source": [
        "def generate_samples(generator: tf.keras.Model, target_class: int, num_samples: int):\n",
        "    \"\"\"\n",
        "    Generates samples using the generator for a specific target class.\n",
        "\n",
        "    Args:\n",
        "        generator (tensorflow.keras.Model): The generator model.\n",
        "        target_class (int): The target class for which to generate samples.\n",
        "        num_samples (int): The number of samples to generate.\n",
        "\n",
        "    Returns:\n",
        "        numpy.ndarray: Generated samples as a NumPy array.\n",
        "    \"\"\"\n",
        "    noise_dim = generator.input_shape[0][1]\n",
        "\n",
        "    # éšæœºç”Ÿæˆä¸€ç»„å™ªå£°å‘é‡ shape=(num_samples, noise_dim)\n",
        "    noise = np.random.normal(0, 1, size=(num_samples, noise_dim))\n",
        "\n",
        "    # éšæœºç”Ÿæˆä¸€ç»„ç±»åˆ«æ ‡ç­¾ shape=(num_samples, 1), å…¨éƒ¨ä¸º target_class\n",
        "    labels = np.full((num_samples, 1), fill_value=target_class, dtype=np.int32)\n",
        "\n",
        "    # ä½¿ç”¨ç”Ÿæˆå™¨ç”Ÿæˆæ•°æ®\n",
        "    generated_data = generator.predict([noise, labels], verbose=0)\n",
        "    return generated_data\n",
        "\n",
        "\n",
        "def cgan_undersample(cgan_discriminator: tf.keras.Model, sampling_strategy: dict, X: np.ndarray, y: np.ndarray):\n",
        "    \"\"\"\n",
        "    ä½¿ç”¨ CGAN åˆ¤åˆ«å™¨å¯¹ç›®æ ‡æ•°æ®é›†è¿›è¡Œæ¬ é‡‡æ ·ï¼Œåˆ é™¤è¯„åˆ†ä½çš„æ•°æ®\n",
        "    \"\"\"\n",
        "    print(f'[{now()}] ğŸ“‰ CGAN Undersampling ...')\n",
        "    print(f'  original X.shaep: {X.shape}')\n",
        "    print(f'  original labels_counts: {get_label_counts(y)}')\n",
        "    print(f'  undersample to: {sampling_strategy}')\n",
        "\n",
        "    keep_idxs = []\n",
        "\n",
        "    for cls, target_n in sampling_strategy.items():\n",
        "        idxs = np.where(y == cls)[0]\n",
        "\n",
        "        # nothing to drop if already <= target\n",
        "        if len(idxs) <= target_n:\n",
        "            keep_idxs.extend(idxs.tolist())\n",
        "            print(f'[{now()}] Skipping class [{cls}]: {len(idxs)} â‰¤ {target_n}')\n",
        "            continue\n",
        "\n",
        "        # Score all samples of this cls\n",
        "        X_cls = X[idxs]\n",
        "        y_cls = y[idxs].reshape(-1, 1)\n",
        "        scores = cgan_discriminator([X_cls, y_cls], training=False)\n",
        "        scores = tf.reshape(scores, [-1]).numpy()\n",
        "\n",
        "        # æŒ‰ç…§åˆ¤åˆ«å™¨è¯„åˆ†é™åºæ’åˆ—ï¼Œå–å‰ n ä¸ªä¿ç•™(ä¿ç•™è¯„åˆ†é«˜çš„)\n",
        "        top_idxs = idxs[np.argsort(scores)[::-1][:target_n]]\n",
        "        # æŒ‰ç…§åˆ¤åˆ«å™¨è¯„åˆ†å‡åºæ’åˆ—ï¼Œå–å‰ n ä¸ªä¿ç•™(ä¿ç•™è¯„åˆ†ä½çš„)\n",
        "        # top_idxs = idxs[np.argsort(scores)[:target_n]]\n",
        "\n",
        "        keep_idxs.extend(top_idxs.tolist())\n",
        "        print(f'[{now()}] Dropping {len(idxs) - target_n} samples for class [{cls}]: {len(idxs)} -> {target_n}')\n",
        "\n",
        "    # For any classes not in sampling_strategy, keep all\n",
        "    all_classes = set(np.unique(y))\n",
        "    leftover = all_classes - set(sampling_strategy.keys())\n",
        "    for cls in leftover:\n",
        "        keep_idxs.extend(np.where(y == cls)[0].tolist())\n",
        "\n",
        "    # produce final undersampled arrays\n",
        "    keep_idxs = np.sort(keep_idxs)\n",
        "    X_res = X[keep_idxs]\n",
        "    y_res = y[keep_idxs]\n",
        "\n",
        "    print(f'[{now()}] ğŸ“‰ After CGAN Undersampling:')\n",
        "    print(f'  X_res.shape: {X_res.shape}')\n",
        "    print(f'  Labels: {get_label_counts(y_res)}')\n",
        "\n",
        "    return X_res, y_res\n",
        "\n",
        "def cgan_oversample(cgan_generator: tf.keras.Model, sampling_strategy: dict, X: np.ndarray, y: np.ndarray):\n",
        "    current_counts = get_label_counts(y)\n",
        "\n",
        "    print(f'[{now()}] ğŸ“ˆ CGAN Oversampling ...')\n",
        "    print(f'  original X.shaep: {X.shape}')\n",
        "    print(f'  original labels_counts: {current_counts}')\n",
        "    print(f'  oversample to: {sampling_strategy}')\n",
        "\n",
        "    all_X = [X]\n",
        "    all_y = [y]\n",
        "\n",
        "    for cls, desired_n in sampling_strategy.items():\n",
        "        current_n = current_counts.get(cls, 0)\n",
        "        n_to_gen = desired_n - current_n\n",
        "        if n_to_gen > 0:\n",
        "            print(f'[{now()}] Generating {n_to_gen} samples for class [{cls}]: {current_n} -> {desired_n}')\n",
        "            gen_samples = generate_samples(cgan_generator, cls, n_to_gen)\n",
        "            all_X.append(gen_samples)\n",
        "            all_y.append(np.full(n_to_gen, cls, dtype=np.int32))\n",
        "        else:\n",
        "            print(f'[{now()}] Skipping class [{cls}]: {current_n} â‰¥ {desired_n}')\n",
        "\n",
        "    X_res = np.concatenate(all_X)\n",
        "    y_res = np.concatenate(all_y)\n",
        "\n",
        "    print(f'[{now()}] ğŸ“ˆ After CGAN Oversampling:')\n",
        "    print(f'  X_res.shape: {X_res.shape}')\n",
        "    print(f'  Labels: {get_label_counts(y_res)}')\n",
        "\n",
        "    return X_res, y_res\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8z8xldy80_sI"
      },
      "outputs": [],
      "source": [
        "if len(resample_outputs) == 0:\n",
        "    print(f'[{now()}] ä¸ç”Ÿæˆè¿‡é‡‡æ ·çš„æ•°æ®æ–‡ä»¶')\n",
        "\n",
        "for resample_scheme in resample_outputs:\n",
        "    print(f'[{now()}] ğŸš€ resample_scheme: {resample_scheme}')\n",
        "\n",
        "    # CGAN æ¬ é‡‡æ ·(å¦‚æœéœ€è¦çš„è¯)\n",
        "    if cgan_filter_strategy:\n",
        "        print(f'[{now()}] ğŸŸ¡ Apply CGAN Undersampling.')\n",
        "        X_filtered, y_filtered = cgan_undersample(discriminator, cgan_filter_schemes[cgan_filter_strategy], X, y)\n",
        "    else:\n",
        "        X_filtered, y_filtered = X, y\n",
        "\n",
        "    # CGAN è¿‡é‡‡æ ·\n",
        "    print(f'[{now()}] ğŸŸ¢ Apply CGAN Oversampling.')\n",
        "    resample_to = resample_schemes[resample_scheme]\n",
        "    X_resampled, y_resampled = cgan_oversample(generator, resample_to, X_filtered, y_filtered)\n",
        "\n",
        "    # ä¿å­˜æ–‡ä»¶\n",
        "    filename = generator_file.stem[:-10] + (f'f{cgan_filter_strategy}.npy' if cgan_filter_strategy else '.npy')\n",
        "    X_resampled_file = balanced_folder / f'train_X_{scaling_method}_s{resample_scheme}_{filename}'\n",
        "    y_resampled_file = balanced_folder / f'train_y_{scaling_method}_s{resample_scheme}_{filename}'\n",
        "    np.save(X_resampled_file, X_resampled)\n",
        "    np.save(y_resampled_file, y_resampled)\n",
        "    print(f\"[{now()}] ğŸ’¾ Saved resampled data to {X_resampled_file} & {y_resampled_file.name}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import runtime\n",
        "print(f'[{now()}] â›” è¿è¡Œç»“æŸ. shutdown now...')\n",
        "runtime.unassign()"
      ],
      "metadata": {
        "id": "TGNmz5P2dIq1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "qR21AFUgeYXb",
        "AM7RomoyKpSt",
        "lUwtAak0tyuJ"
      ],
      "gpuType": "L4",
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}