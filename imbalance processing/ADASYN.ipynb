{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/funway/nid-imbalance-study/blob/main/imbalance%20processing/ADASYN.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pv4skFSheBzA"
      },
      "source": [
        "# 使用 ADASYN 对训练集进行过采样\n",
        "\n",
        "* ADASYN 对 少数样本 进行过采样\n",
        "* ADASYN 会根据“被误分类的难度”决定在哪些点附近生成更多少数类样本 (重点加强容易误分类的区域)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gqqi2ITUeOmB"
      },
      "source": [
        "## Google Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 329
        },
        "id": "fzbTh_MiPCpH",
        "outputId": "e6f0125d-eb45-44f4-ee48-682402c70b12"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "MessageError",
          "evalue": "Error: credential propagation was unsuccessful",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mMessageError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-c36f5579453e>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36mmount\u001b[0;34m(mountpoint, force_remount, timeout_ms, readonly)\u001b[0m\n\u001b[1;32m     98\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m120000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreadonly\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m   \u001b[0;34m\"\"\"Mount your Google Drive at the specified mountpoint path.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 100\u001b[0;31m   return _mount(\n\u001b[0m\u001b[1;32m    101\u001b[0m       \u001b[0mmountpoint\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    102\u001b[0m       \u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/drive.py\u001b[0m in \u001b[0;36m_mount\u001b[0;34m(mountpoint, force_remount, timeout_ms, ephemeral, readonly)\u001b[0m\n\u001b[1;32m    135\u001b[0m   )\n\u001b[1;32m    136\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mephemeral\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m     _message.blocking_request(\n\u001b[0m\u001b[1;32m    138\u001b[0m         \u001b[0;34m'request_auth'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m         \u001b[0mrequest\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'authType'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m'dfs_ephemeral'\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mblocking_request\u001b[0;34m(request_type, request, timeout_sec, parent)\u001b[0m\n\u001b[1;32m    174\u001b[0m       \u001b[0mrequest_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrequest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexpect_reply\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m   )\n\u001b[0;32m--> 176\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m    101\u001b[0m     ):\n\u001b[1;32m    102\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0;34m'error'\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 103\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mMessageError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'error'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    104\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mreply\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    105\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mMessageError\u001b[0m: Error: credential propagation was unsuccessful"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qR21AFUgeYXb"
      },
      "source": [
        "## Modules import & Globals setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NKNH9UjKeZmU"
      },
      "outputs": [],
      "source": [
        "### Modules ###\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "### Globals ###\n",
        "\n",
        "## Label 特征的数值化编码\n",
        "label_mapping = {\n",
        "    \"Benign\": 0,\n",
        "    \"Bot\": 1,\n",
        "    \"Brute Force -Web\": 2,\n",
        "    \"Brute Force -XSS\": 3,\n",
        "    \"DDOS attack-HOIC\": 4,\n",
        "    \"DDOS attack-LOIC-UDP\": 5,\n",
        "    \"DDoS attacks-LOIC-HTTP\": 6,\n",
        "    \"DoS attacks-GoldenEye\": 7,\n",
        "    \"DoS attacks-Hulk\": 8,\n",
        "    \"DoS attacks-SlowHTTPTest\": 9,\n",
        "    \"DoS attacks-Slowloris\": 10,\n",
        "    \"FTP-BruteForce\": 11,\n",
        "    \"Infilteration\": 12,\n",
        "    \"SQL Injection\": 13,\n",
        "    \"SSH-Bruteforce\": 14\n",
        "}\n",
        "\n",
        "## 计划尝试三种样本分布模式\n",
        "resample_schemes = {\n",
        "    # 模式1. (标签0:非0标签总和) ≈ (160:157); 非0标签按大概比例增强\n",
        "    1: {\n",
        "        0: 1600000,  # 保持不变\n",
        "        1: 200000,   # ⤵️ 228953\n",
        "        2: 20000,    # ⤴️ 489\n",
        "        3: 20000,    # ⤴️ 184\n",
        "        4: 200000,   # ⤵️ 548809\n",
        "        5: 20000,    # ⤴️ 1384\n",
        "        6: 200000,   # ⤵️ 460953\n",
        "        7: 100000,   # ⤴️ 33206\n",
        "        8: 200000,   # ⤵️ 369530\n",
        "        9: 111912,   # ⤴️ 111912\n",
        "        10: 50000,   # ⤴️ 8792\n",
        "        11: 154683,  # ⤴️ 154683\n",
        "        12: 128511,  # ⤴️ 128511\n",
        "        13: 20000,   # ⤴️ 70\n",
        "        14: 150071   # ⤴️ 150071\n",
        "    },\n",
        "    # 模式2. (标签0:最多非0标签样本) ≈ (3:2)\n",
        "    2: {\n",
        "        0: 300000,   # ⤵️ 1600000\n",
        "        1: 200000,   # ⤵️ 228953\n",
        "        2: 20000,    # ⤴️ 489\n",
        "        3: 20000,    # ⤴️ 184\n",
        "        4: 200000,   # ⤵️ 548809\n",
        "        5: 20000,    # ⤴️ 1384\n",
        "        6: 200000,   # ⤵️ 460953\n",
        "        7: 100000,   # ⤴️ 33206\n",
        "        8: 200000,   # ⤵️ 369530\n",
        "        9: 111912,   # ⤴️ 111912\n",
        "        10: 50000,   # ⤴️ 8792\n",
        "        11: 154683,  # ⤴️ 154683\n",
        "        12: 128511,  # ⤴️ 128511\n",
        "        13: 20000,   # ⤴️ 70\n",
        "        14: 150071   # ⤴️ 150071\n",
        "    },\n",
        "    # 模式3. (标签0:非0标签总和) = (1:1); 每种非0标签都占 114300 个样本\n",
        "    3: {\n",
        "        0: 1600000,\n",
        "        **{k: 114300 for k in range(1, 15)}\n",
        "    },\n",
        "    # 模式4. 所有标签都 20万样本\n",
        "    4: {\n",
        "       **{k: 200000 for k in range(0, 15)}\n",
        "    },\n",
        "}\n",
        "\n",
        "## 数据目录\n",
        "datasets_folder = Path('/content/drive/MyDrive/NYIT/870/datasets')\n",
        "dataset = 'CSE-CIC-IDS2018'\n",
        "preprocessed_folder = datasets_folder / 'preprocessed' / dataset\n",
        "balanced_folder = datasets_folder / 'balanced' / dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K-SqflxGQ5rK"
      },
      "outputs": [],
      "source": [
        "scaling_method = 'standard'\n",
        "# scaling_method = 'minmax'\n",
        "\n",
        "resample_scheme = 3\n",
        "resample_to = resample_schemes[resample_scheme]\n",
        "\n",
        "oversampling_method = 'ROS+ADASYN'\n",
        "\n",
        "# undersampling_method = 'NM'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DXZ4E8wJlzLF"
      },
      "outputs": [],
      "source": [
        "X_file = preprocessed_folder / f'integrated/train_X_{scaling_method}.npy'\n",
        "y_file = preprocessed_folder / f'integrated/train_label_{scaling_method}.npy'\n",
        "\n",
        "# 加载训练集文件\n",
        "X = np.load(X_file)\n",
        "y = np.load(y_file)\n",
        "\n",
        "labels_counts = sorted(Counter(y).items())\n",
        "labels_counts = dict(labels_counts)\n",
        "\n",
        "print(f\"[{datetime.now().strftime('%x %X')}] {X_file.name} shape: {X.shape}\")\n",
        "print(f'Labels: { {int(k): v for k, v in labels_counts.items()} }\\n')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 利用 ROS 提前补充极少数类样本\n",
        "- 先用 ROS 随机复制的方式，将极少数类样本扩展到可接受的程度后再进行 oversampling\n"
      ],
      "metadata": {
        "id": "fDAkD88lY-gE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "oversample_to = {}\n",
        "# 判断 oversampling_method 字符串开头是否为 ROS\n",
        "if oversampling_method.startswith('ROS'):\n",
        "    if oversampling_method.startswith('ROS+'):\n",
        "        oversample_to = {2: 1000, 3: 500, 13: 500}\n",
        "    elif oversampling_method.startswith('ROS1+'):\n",
        "        oversample_to = {2: 1000, 3: 1000, 13: 1000}\n",
        "\n",
        "    oversampler = RandomOverSampler(sampling_strategy=oversample_to, random_state=42)\n",
        "    X, y = oversampler.fit_resample(X, y)\n",
        "\n",
        "    print(f'[{datetime.now().strftime(\"%x %X\")}] After ROS oversampling:')\n",
        "    print(f'  X.shape: {X.shape}, y.shape: {y.shape}')\n",
        "    print(f'  Labels: { {int(k): v for k, v in sorted(Counter(y).items())} }\\n')\n",
        "else:\n",
        "    print(f'[{datetime.now().strftime(\"%x %X\")}] No need to ROS oversampling.')"
      ],
      "metadata": {
        "id": "vfzi08QuZwQV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75awp9PSfBmN"
      },
      "source": [
        "## ADASYN Oversampling\n",
        "\n",
        "\n",
        "*   ADASYN 的计算复杂度太高了, 大概是 `O(m*n*d)`, 其中 m 是样本数量, n 是目标样本数量, d 是特征维度。\n",
        "*   如果是 70 个特征维度, 将一个标签从 30000 过采样到 100000, 耗时大概 30 分钟。。。那从10万到20万岂不是要180分钟。。。\n",
        "*   所以 ADASYN 不适合对本来就有一定规模的样本进行过采样(我觉得超过5万的就不要用 ADASYN 了)。\n",
        "* 另外, ADASYN 很容易报错 `RuntimeError: Not any neigbours belong to the majority class. This case will induce a NaN case with a division by zero. ADASYN is not suited for this specific dataset. Use SMOTE instead.` 在某个少数类样本的周围邻居中没找到多数类样本。\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ySil5tAjiOvy"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import ADASYN\n",
        "\n",
        "### 分阶段过采样函数 #############################################################\n",
        "# 70 → 350 → 1750 → 8750 → 43750 这样\n",
        "def staged_oversample_adasyn(X, y, label, target, factor=5, random_state=42, n_neighbors=5):\n",
        "    current_X, current_y = X, y\n",
        "    current_count = Counter(current_y)[label]\n",
        "\n",
        "    # 如果目标数量比当前数量小或相等，直接返回\n",
        "    if current_count >= target:\n",
        "        return X, y\n",
        "\n",
        "    while current_count < target:\n",
        "        next_target = min(current_count * factor, target)  # 下一个阶段的目标\n",
        "        print(f\"  当前数量: {current_count}, 下阶段目标: {next_target}\")\n",
        "\n",
        "        oversampler = ADASYN(\n",
        "            sampling_strategy={label: next_target},\n",
        "            random_state=random_state,\n",
        "            n_neighbors=n_neighbors\n",
        "        )\n",
        "        try:\n",
        "          current_X, current_y = oversampler.fit_resample(current_X, current_y)\n",
        "          if current_count == Counter(current_y)[label]:\n",
        "              raise Exception('无法新增样本! (标签的样本分布不适合使用该过采样方法)')\n",
        "          current_count = Counter(current_y)[label]\n",
        "        except Exception as e:\n",
        "          print(f\"  ERROR: {e}\")\n",
        "          break\n",
        "\n",
        "        # 最后一次得到近似 target 值后就可以退出了。\n",
        "        if next_target == target:\n",
        "            break\n",
        "\n",
        "    return current_X, current_y\n",
        "### 分阶段过采样函数 #############################################################\n",
        "\n",
        "# 1. 指定需要过采样的标签与目标\n",
        "oversample_to = {}\n",
        "labels_counts = dict(sorted(Counter(y).items()))\n",
        "for label, target in resample_to.items():\n",
        "    if labels_counts[label] < resample_to[label]:\n",
        "        oversample_to[label] = target\n",
        "# oversample_to = {5: 20000}\n",
        "print(f\"[{datetime.now().strftime('%x %X')}] oversample_to: {oversample_to}\\n\")\n",
        "\n",
        "# 2. 执行过采样\n",
        "start_time = datetime.now()\n",
        "\n",
        "# #########################\n",
        "# 2-1. 一次性过采样\n",
        "# oversampler = ADASYN(sampling_strategy=oversample_to, random_state=42, n_neighbors=5)\n",
        "# X_resampled, y_resampled = oversampler.fit_resample(X, y)\n",
        "# #########################\n",
        "\n",
        "# #########################\n",
        "# 2-2. 逐标签过采样\n",
        "# 先提取出不需要过采样的数据\n",
        "mask = ~np.isin(y, list(oversample_to.keys()))\n",
        "X_resampled = X[mask]\n",
        "y_resampled = y[mask]\n",
        "print(f'X_resampled.shape(before): {X_resampled.shape}')\n",
        "print(f'Labels(before): { {int(k): v for k, v in sorted(Counter(y_resampled).items())} }\\n')\n",
        "\n",
        "for label, target in oversample_to.items():\n",
        "    print(f'[{datetime.now().strftime(\"%x %X\")}] Oversampling label[{label}]: {labels_counts[label]} -> {oversample_to[label]} ...')\n",
        "    st = datetime.now()\n",
        "\n",
        "    # 2-2-1. 不分阶段过采样\n",
        "    try:\n",
        "      sampler = ADASYN(sampling_strategy={label: target}, random_state=42, n_neighbors=5)\n",
        "      X_, y_ = sampler.fit_resample(X, y)\n",
        "    except Exception as e:\n",
        "      print(f'  ERROR: {e}')\n",
        "      X_, y_ = X, y\n",
        "\n",
        "    # 2-2-2. 分阶段过采样\n",
        "    # X_, y_ = staged_oversample_adasyn(X, y, label, target, factor=5, random_state=42, n_neighbors=5)\n",
        "\n",
        "    et = datetime.now()\n",
        "    print(f\"  Time elapsed: {et - st}. [{st.strftime('%Y%m%d %X')} -> {et.strftime('%Y%m%d %X')}]\")\n",
        "\n",
        "    # 从结果中提取出当前标签的样本（包括 原始样本 + 新增样本）\n",
        "    mask_current_label = (y_ == label)\n",
        "    X_current_label = X_[mask_current_label]\n",
        "    y_current_label = y_[mask_current_label]\n",
        "\n",
        "    # 拼接到最终结果\n",
        "    X_resampled = np.vstack((X_resampled, X_current_label))\n",
        "    y_resampled = np.hstack((y_resampled, y_current_label))\n",
        "\n",
        "    print(f'  X_resampled.shape: {X_resampled.shape}')\n",
        "    print(f'  Labels: { {int(k): v for k, v in sorted(Counter(y_resampled).items())} }\\n')\n",
        "# #########################\n",
        "\n",
        "# 3. 查看结果\n",
        "print(f\"[{datetime.now().strftime('%x %X')}] After oversampling:\")\n",
        "end_time = datetime.now()\n",
        "print(f\"  Time elapsed: {end_time - start_time}. [{start_time.strftime('%x %X')} -> {end_time.strftime('%x %X')}]\")\n",
        "print(f'  X_resampled.shape: {X_resampled.shape}')\n",
        "print(f'  Labels: { {int(k): v for k, v in sorted(Counter(y_resampled).items())} }\\n')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ahMCdpX0cR5h"
      },
      "outputs": [],
      "source": [
        "# 保存文件\n",
        "X_resampled_file = balanced_folder / f'{X_file.stem}_s{resample_scheme}_{oversampling_method}.npy'\n",
        "y_resampled_file = balanced_folder / f'{y_file.stem}_s{resample_scheme}_{oversampling_method}.npy'\n",
        "\n",
        "# 判断实际 oversample 之后的标签样本数，有没有达到目标数量 oversample_to 的 95%，没有的话，设置变量 incomplete 为 true\n",
        "incomplete = False\n",
        "incomplete_ratio = 0.95\n",
        "labels_counts = sorted(Counter(y_resampled).items())\n",
        "labels_counts = dict(labels_counts)\n",
        "for label, target in oversample_to.items():\n",
        "    if labels_counts[label] <= target * incomplete_ratio:\n",
        "        incomplete = True\n",
        "        # 文件名后面有个 + 号表示过采样不完全，需要使用额外的简单过采样进行数据补全\n",
        "        X_resampled_file = X_resampled_file.with_name(X_resampled_file.stem + '+.npy')\n",
        "        y_resampled_file = y_resampled_file.with_name(y_resampled_file.stem + '+.npy')\n",
        "        break\n",
        "\n",
        "np.save(X_resampled_file, X_resampled)\n",
        "np.save(y_resampled_file, y_resampled)\n",
        "\n",
        "print(f\"[{datetime.now().strftime('%x %X')}] ✅ Saved to {X_resampled_file} & {y_resampled_file.name}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tp6vxC9t_q25"
      },
      "source": [
        "## SMOTE 补全数据\n",
        "\n",
        "因为基于 **邻居** 样本的过采样算法, 可能会因为找不到邻居而导致无法新增数据。<br/>\n",
        "所以在最后用 SMOTE 算法进行兜底, 补全不足 oversample_to 目标的样本。\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8tAavlt9AcNo"
      },
      "outputs": [],
      "source": [
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "if not incomplete:\n",
        "    print(f\"[{datetime.now().strftime('%x %X')}] ✅ 无需补全数据\")\n",
        "else:\n",
        "    print(f\"[{datetime.now().strftime('%x %X')}] ⚠️ 需要补全数据\")\n",
        "\n",
        "    # 加载训练集文件\n",
        "    # X_resampled_file = balanced_folder / f'{X_file.stem}_s{resample_scheme}_BLSMOTE.npy'\n",
        "    # y_resampled_file = balanced_folder / f'{y_file.stem}_s{resample_scheme}_BLSMOTE.npy'\n",
        "\n",
        "    # X_resampled = np.load(X_resampled_file)\n",
        "    # y_resampled = np.load(y_resampled_file)\n",
        "\n",
        "    labels_counts = sorted(Counter(y_resampled).items())\n",
        "    labels_counts = dict(labels_counts)\n",
        "\n",
        "    print(f'X_resampled.shape: {X_resampled.shape}')\n",
        "    print(f'Labels: { {int(k): v for k, v in labels_counts.items()} }\\n')\n",
        "\n",
        "    # 指定需要补充过采样的标签与目标\n",
        "    oversample_to = {}\n",
        "    for label, target in resample_to.items():\n",
        "        if labels_counts[label] < resample_to[label]:\n",
        "            oversample_to[label] = target\n",
        "    print(f'oversample_to: {oversample_to}\\n')\n",
        "\n",
        "    # 使用 SMOTE 过采样\n",
        "    sampler = SMOTE(sampling_strategy=oversample_to, random_state=42)\n",
        "    X_completed, y_completed = sampler.fit_resample(X_resampled, y_resampled)\n",
        "\n",
        "    # 打印结果\n",
        "    print(f'  X_completed.shape: {X_completed.shape}')\n",
        "    print(f'  Labels: { {int(k): v for k, v in sorted(Counter(y_completed).items())} }\\n')\n",
        "\n",
        "    # 保存结果\n",
        "    X_completed_file = X_resampled_file.with_name(X_resampled_file.stem + 'SMOTE.npy')\n",
        "    y_completed_file = y_resampled_file.with_name(y_resampled_file.stem + 'SMOTE.npy')\n",
        "\n",
        "    np.save(X_completed_file, X_completed)\n",
        "    np.save(y_completed_file, y_completed)\n",
        "\n",
        "    print(f\"[{datetime.now().strftime('%x %X')}] ✅ Saved to {X_completed_file} & {y_completed_file.name}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "gqqi2ITUeOmB"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}