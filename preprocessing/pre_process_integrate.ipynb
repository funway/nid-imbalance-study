{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/funway/nid-imbalance-study/blob/main/preprocessing/pre_process_integrate.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rRCpfJDkXDpy"
      },
      "source": [
        "# 整合数据集，生成 train, validate, test 数据文件"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0717hcCbetqR"
      },
      "source": [
        "## Google Colab Env"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "awc4G7nAWs2P",
        "outputId": "7d7e9623-9096-43cc-a168-14fc8f6c4caa"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rYw5iXhnZAhb"
      },
      "source": [
        "## Modules import & Globals setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3JR-k2HaYy1M"
      },
      "outputs": [],
      "source": [
        "### Modules ###\n",
        "from pathlib import Path\n",
        "from datetime import datetime\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "### Globals ###\n",
        "## 数据文件\n",
        "dataset = 'CSE-CIC-IDS2018'\n",
        "dataset_folder = f'/content/drive/MyDrive/NYIT/870/datasets/original/{dataset}/'\n",
        "preprocessed_folder = f'/content/drive/MyDrive/NYIT/870/datasets/preprocessed/{dataset}/'\n",
        "balanced_folder = f'/content/drive/MyDrive/NYIT/870/datasets/balanced/{dataset}/'\n",
        "\n",
        "# 支持: standard, minmax, robust, l1pstandard, l1pminmax\n",
        "# scaling_method = 'standard'\n",
        "scaling_method = 'l1pstandard'\n",
        "\n",
        "# Label 特征的数值化编码\n",
        "label_mapping = {\n",
        "    \"Benign\": 0,\n",
        "    \"Bot\": 1,\n",
        "    \"Brute Force -Web\": 2,\n",
        "    \"Brute Force -XSS\": 3,\n",
        "    \"DDOS attack-HOIC\": 4,\n",
        "    \"DDOS attack-LOIC-UDP\": 5,\n",
        "    \"DDoS attacks-LOIC-HTTP\": 6,\n",
        "    \"DoS attacks-GoldenEye\": 7,\n",
        "    \"DoS attacks-Hulk\": 8,\n",
        "    \"DoS attacks-SlowHTTPTest\": 9,\n",
        "    \"DoS attacks-Slowloris\": 10,\n",
        "    \"FTP-BruteForce\": 11,\n",
        "    \"Infilteration\": 12,\n",
        "    \"SQL Injection\": 13,\n",
        "    \"SSH-Bruteforce\": 14\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dgV6-J25wx0s"
      },
      "source": [
        "## Integration\n",
        "* 将预处理后生成的多个特征文件 X.npy 与对应的 label.npy 标签文件, 整合到两个文件 all_X.npy 以及 all_label.npy 中\n",
        "* 同时对 all_X 进行一次纵向裁剪，删除全零值的特征列。理论上删除全零特征列的动作应该是在整合之后、scaling 之前。但我们这里是先 scaling 后整合，所以删除全零列也就放在这里的整合之后了。\n",
        "* 经过验证，原始文件，StandardScaler 和 MinmaxScaler 处理后的全零列都是 [32 34 56 57 58 59 60 61] 这8列。\n",
        "* robust, l1pstandard, l1pminmax 处理后的全零列是 [32 33 34 50 56 57 58 59 60 61]，多了 33, 50 两列。\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hoLW98Lkengx",
        "outputId": "32d52452-41c4-4c51-e8c2-a58288603a55"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "label_file: /content/drive/MyDrive/NYIT/870/datasets/preprocessed/CSE-CIC-IDS2018/separated/Wednesday-28-02-2018_TrafficForML_CICFlowMeter_label.npy\n",
            "X_file: /content/drive/MyDrive/NYIT/870/datasets/preprocessed/CSE-CIC-IDS2018/separated/Wednesday-28-02-2018_TrafficForML_CICFlowMeter_X_l1pstandard.npy\n",
            "  X.shape: (606902, 78)\n",
            "  labels.shape: (606902,)\n",
            "label_file: /content/drive/MyDrive/NYIT/870/datasets/preprocessed/CSE-CIC-IDS2018/separated/Wednesday-21-02-2018_TrafficForML_CICFlowMeter_label.npy\n",
            "X_file: /content/drive/MyDrive/NYIT/870/datasets/preprocessed/CSE-CIC-IDS2018/separated/Wednesday-21-02-2018_TrafficForML_CICFlowMeter_X_l1pstandard.npy\n",
            "  X.shape: (1048575, 78)\n",
            "  labels.shape: (1048575,)\n",
            "label_file: /content/drive/MyDrive/NYIT/870/datasets/preprocessed/CSE-CIC-IDS2018/separated/Wednesday-14-02-2018_TrafficForML_CICFlowMeter_label.npy\n",
            "X_file: /content/drive/MyDrive/NYIT/870/datasets/preprocessed/CSE-CIC-IDS2018/separated/Wednesday-14-02-2018_TrafficForML_CICFlowMeter_X_l1pstandard.npy\n",
            "  X.shape: (1044751, 78)\n",
            "  labels.shape: (1044751,)\n",
            "label_file: /content/drive/MyDrive/NYIT/870/datasets/preprocessed/CSE-CIC-IDS2018/separated/Thursday-22-02-2018_TrafficForML_CICFlowMeter_label.npy\n",
            "X_file: /content/drive/MyDrive/NYIT/870/datasets/preprocessed/CSE-CIC-IDS2018/separated/Thursday-22-02-2018_TrafficForML_CICFlowMeter_X_l1pstandard.npy\n",
            "  X.shape: (1042965, 78)\n",
            "  labels.shape: (1042965,)\n",
            "label_file: /content/drive/MyDrive/NYIT/870/datasets/preprocessed/CSE-CIC-IDS2018/separated/Thursday-15-02-2018_TrafficForML_CICFlowMeter_label.npy\n",
            "X_file: /content/drive/MyDrive/NYIT/870/datasets/preprocessed/CSE-CIC-IDS2018/separated/Thursday-15-02-2018_TrafficForML_CICFlowMeter_X_l1pstandard.npy\n",
            "  X.shape: (1040548, 78)\n",
            "  labels.shape: (1040548,)\n",
            "label_file: /content/drive/MyDrive/NYIT/870/datasets/preprocessed/CSE-CIC-IDS2018/separated/Thursday-01-03-2018_TrafficForML_CICFlowMeter_label.npy\n",
            "X_file: /content/drive/MyDrive/NYIT/870/datasets/preprocessed/CSE-CIC-IDS2018/separated/Thursday-01-03-2018_TrafficForML_CICFlowMeter_X_l1pstandard.npy\n",
            "  X.shape: (328181, 78)\n",
            "  labels.shape: (328181,)\n",
            "label_file: /content/drive/MyDrive/NYIT/870/datasets/preprocessed/CSE-CIC-IDS2018/separated/Thuesday-20-02-2018_TrafficForML_CICFlowMeter_label.npy\n",
            "X_file: /content/drive/MyDrive/NYIT/870/datasets/preprocessed/CSE-CIC-IDS2018/separated/Thuesday-20-02-2018_TrafficForML_CICFlowMeter_X_l1pstandard.npy\n",
            "  X.shape: (1072147, 78)\n",
            "  labels.shape: (1072147,)\n",
            "label_file: /content/drive/MyDrive/NYIT/870/datasets/preprocessed/CSE-CIC-IDS2018/separated/Friday-23-02-2018_TrafficForML_CICFlowMeter_label.npy\n",
            "X_file: /content/drive/MyDrive/NYIT/870/datasets/preprocessed/CSE-CIC-IDS2018/separated/Friday-23-02-2018_TrafficForML_CICFlowMeter_X_l1pstandard.npy\n",
            "  X.shape: (1042867, 78)\n",
            "  labels.shape: (1042867,)\n",
            "label_file: /content/drive/MyDrive/NYIT/870/datasets/preprocessed/CSE-CIC-IDS2018/separated/Friday-16-02-2018_TrafficForML_CICFlowMeter_label.npy\n",
            "X_file: /content/drive/MyDrive/NYIT/870/datasets/preprocessed/CSE-CIC-IDS2018/separated/Friday-16-02-2018_TrafficForML_CICFlowMeter_X_l1pstandard.npy\n",
            "  X.shape: (1048574, 78)\n",
            "  labels.shape: (1048574,)\n",
            "label_file: /content/drive/MyDrive/NYIT/870/datasets/preprocessed/CSE-CIC-IDS2018/separated/Friday-02-03-2018_TrafficForML_CICFlowMeter_label.npy\n",
            "X_file: /content/drive/MyDrive/NYIT/870/datasets/preprocessed/CSE-CIC-IDS2018/separated/Friday-02-03-2018_TrafficForML_CICFlowMeter_X_l1pstandard.npy\n",
            "  X.shape: (1044525, 78)\n",
            "  labels.shape: (1044525,)\n",
            "[04/20/25 04:56:36] After Integration, all_X.shape: (9320035, 78)\n",
            "[04/20/25 04:56:36] After Integration, all_label.shape: (9320035,)\n",
            "标签 0: 6573101 个样本\n",
            "标签 1: 286191 个样本\n",
            "标签 2: 611 个样本\n",
            "标签 3: 230 个样本\n",
            "标签 4: 686012 个样本\n",
            "标签 5: 1730 个样本\n",
            "标签 6: 576191 个样本\n",
            "标签 7: 41508 个样本\n",
            "标签 8: 461912 个样本\n",
            "标签 9: 139890 个样本\n",
            "标签 10: 10990 个样本\n",
            "标签 11: 193354 个样本\n",
            "标签 12: 160639 个样本\n",
            "标签 13: 87 个样本\n",
            "标签 14: 187589 个样本\n",
            "[04/20/25 04:56:37] 全零列: [32 33 34 50 56 57 58 59 60 61]\n",
            "[04/20/25 04:56:39] 删除全零值的列后, all_X.shape: (9320035, 68)\n"
          ]
        }
      ],
      "source": [
        "## 整合数据文件 ##\n",
        "\n",
        "all_X = []  # 用来存储所有的X数据\n",
        "all_label = []  # 用来存储所有的label数据\n",
        "\n",
        "label_reg = '*_label.npy'\n",
        "label_files = list((Path(preprocessed_folder) / 'separated/').rglob(label_reg))\n",
        "\n",
        "for label_file in label_files:\n",
        "    print(f'label_file: {label_file}')\n",
        "    X_file = str(label_file).replace('_label.npy', f'_X_{scaling_method}.npy')\n",
        "    print(f'X_file: {X_file}')\n",
        "\n",
        "    # 读取数据\n",
        "    labels = np.load(label_file)\n",
        "    X = np.load(X_file)\n",
        "    print(f'  X.shape: {X.shape}')\n",
        "    print(f'  labels.shape: {labels.shape}')\n",
        "\n",
        "    # 合并数据\n",
        "    all_X.append(X)\n",
        "    all_label.append(labels)\n",
        "\n",
        "# 将所有数据合并成一个大数组\n",
        "all_label = np.concatenate(all_label, axis=0)\n",
        "all_X = np.concatenate(all_X, axis=0)\n",
        "print(f'[{datetime.now().strftime(\"%x %X\")}] After Integration, all_X.shape: {all_X.shape}')\n",
        "print(f'[{datetime.now().strftime(\"%x %X\")}] After Integration, all_label.shape: {all_label.shape}')\n",
        "\n",
        "# 统计每个标签的数量\n",
        "unique_labels, counts = np.unique(all_label, return_counts=True)\n",
        "# 打印结果\n",
        "for label, count in zip(unique_labels, counts):\n",
        "    print(f\"标签 {label}: {count} 个样本\")\n",
        "\n",
        "# 判断 X 中是否有全零的列，有的话删除\n",
        "zero_columns = np.all(np.isclose(X, 0, atol=1e-15), axis=0)\n",
        "print(f'[{datetime.now().strftime(\"%x %X\")}] 全零列: {np.where(zero_columns == True)[0]}')\n",
        "all_X = all_X[:, ~zero_columns]\n",
        "print(f'[{datetime.now().strftime(\"%x %X\")}] 删除全零值的列后, all_X.shape: {all_X.shape}')\n",
        "\n",
        "# 保存为npy文件\n",
        "# np.save(Path(preprocessed_folder) / f'integrated/all_X_{scaling_method}.npy', all_X)\n",
        "# np.save(Path(preprocessed_folder) / f'integrated/all_label_{scaling_method}.npy', all_label)\n",
        "# print('✅ Saved')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P6U9aj2sw4ZA"
      },
      "source": [
        "## Trim\n",
        "*   由于 CSE-CIC-IDS2018 数据集数据量太大，所以需要对整合后的数据文件进行裁剪\n",
        "*   对于不同标签值的样本，保留的数据从原始数据中随机抽取，而不是顺序抽取\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gsB3dKEmwwC0",
        "outputId": "4df08437-8273-4a62-a298-bdab150d40b4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[04/20/25 04:56:39] 裁剪整合后的数据文件. Before Trim, all_X.shape: (9320035, 68)\n",
            "Triming label: 0\n",
            "Triming label: 1\n",
            "Triming label: 3\n",
            "Keep all data of label: 2\n",
            "Keep all data of label: 4\n",
            "Keep all data of label: 5\n",
            "Keep all data of label: 6\n",
            "Keep all data of label: 7\n",
            "Keep all data of label: 8\n",
            "Keep all data of label: 9\n",
            "Keep all data of label: 10\n",
            "Keep all data of label: 11\n",
            "Keep all data of label: 12\n",
            "Keep all data of label: 13\n",
            "Keep all data of label: 14\n",
            "[04/20/25 04:56:48] 裁剪后的 X 数据形状: (4746934, 68)\n",
            "[04/20/25 04:56:48] 裁剪后的 label 数据形状: (4746934,)\n",
            "标签 0: 2000000 个样本\n",
            "标签 1: 286191 个样本\n",
            "标签 2: 611 个样本\n",
            "标签 3: 230 个样本\n",
            "标签 4: 686012 个样本\n",
            "标签 5: 1730 个样本\n",
            "标签 6: 576191 个样本\n",
            "标签 7: 41508 个样本\n",
            "标签 8: 461912 个样本\n",
            "标签 9: 139890 个样本\n",
            "标签 10: 10990 个样本\n",
            "标签 11: 193354 个样本\n",
            "标签 12: 160639 个样本\n",
            "标签 13: 87 个样本\n",
            "标签 14: 187589 个样本\n",
            "[04/20/25 04:57:02] ✅ Saved\n"
          ]
        }
      ],
      "source": [
        "## 裁剪整合后的数据文件并保存 ##\n",
        "\n",
        "print(f'[{datetime.now().strftime(\"%x %X\")}] 裁剪整合后的数据文件. Before Trim, all_X.shape: {all_X.shape}')\n",
        "\n",
        "# 将 key 值对于的标签裁剪到 ≤ value 的数量\n",
        "# 因为标签 1,3 本来就没有30万条数据, 所以这里并不会对标签 1,3 进行裁剪\n",
        "trim_to = {0:2000000, 1:300000, 3:300000}\n",
        "\n",
        "# 初始化空列表，用来存储裁剪后的数据\n",
        "trimed_X = []\n",
        "trimed_label = []\n",
        "\n",
        "# 获取每个标签对应的索引\n",
        "for label in trim_to:\n",
        "    print(f'Triming label: {label}')\n",
        "\n",
        "    # 找到所有标签为 `label` 的样本\n",
        "    label_indices = np.where(all_label == label)[0]\n",
        "\n",
        "    # 计算需要保留的样本数，如果该标签样本数大于指定的最大值，则裁剪\n",
        "    num_samples_to_keep = min(len(label_indices), trim_to[label])\n",
        "\n",
        "    # 随机选择 num_samples_to_keep 个索引\n",
        "    np.random.seed(42)  # 设置随机数种子，保证每次执行得到的随机序列都一样\n",
        "    sampled_indices = np.random.choice(label_indices, num_samples_to_keep, replace=False)\n",
        "\n",
        "    trimed_X.append(all_X[sampled_indices])\n",
        "    trimed_label.append(all_label[sampled_indices])\n",
        "\n",
        "# 处理未定义在 trim_to 字典中的标签（保留全部数据）\n",
        "remaining_labels = np.unique(all_label)\n",
        "for label in remaining_labels:\n",
        "    if label not in trim_to:\n",
        "        print(f'Keep all data of label: {label}')\n",
        "        label_indices = np.where(all_label == label)[0]\n",
        "        trimed_X.append(all_X[label_indices])\n",
        "        trimed_label.append(all_label[label_indices])\n",
        "\n",
        "# 合并所有裁剪后的数据\n",
        "trimed_X = np.concatenate(trimed_X, axis=0)\n",
        "trimed_label = np.concatenate(trimed_label, axis=0)\n",
        "\n",
        "# 打印裁剪后的数据形状\n",
        "print(f\"[{datetime.now().strftime('%x %X')}] 裁剪后的 X 数据形状: {trimed_X.shape}\")\n",
        "print(f\"[{datetime.now().strftime('%x %X')}] 裁剪后的 label 数据形状: {trimed_label.shape}\")\n",
        "# 统计每个标签的数量\n",
        "unique_labels, counts = np.unique(trimed_label, return_counts=True)\n",
        "# 打印结果\n",
        "for label, count in zip(unique_labels, counts):\n",
        "    print(f\"标签 {label}: {count} 个样本\")\n",
        "\n",
        "# 保存为npy文件\n",
        "np.save(Path(preprocessed_folder) / f'integrated/trimed_X_{scaling_method}.npy', trimed_X)\n",
        "np.save(Path(preprocessed_folder) / f'integrated/trimed_label_{scaling_method}.npy', trimed_label)\n",
        "print(f'[{datetime.now().strftime(\"%x %X\")}] ✅ Saved')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split to Train, Valid, Test\n",
        "*   将整合好的数据集 **按比例** 分割为 **Train**(训练集), **Valid**(验证集), **Test**(测试集)\n",
        "*   比例暂定为 Train: 80%, Valid: 10%, Test: 10%\n",
        "\n"
      ],
      "metadata": {
        "id": "OlZkHQafWsUh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "X_file = Path(preprocessed_folder) / f'integrated/trimed_X_{scaling_method}.npy'\n",
        "label_file = Path(preprocessed_folder) / f'integrated/trimed_label_{scaling_method}.npy'\n",
        "\n",
        "print(f'X_file: {X_file}')\n",
        "print(f'label_file: {label_file}')\n",
        "\n",
        "# Read whole data\n",
        "X = np.load(X_file)\n",
        "label = np.load(label_file)\n",
        "print(f'Original X.shape: {X.shape}')\n",
        "print(f'Original labels: { {int(k): v for k, v in sorted(Counter(label).items())} }\\n')\n",
        "\n",
        "# Split whole => 0.8 : 0.2\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, label, test_size=0.2, random_state=42, stratify=label)\n",
        "\n",
        "# Split 0.2 => 0.1 : 0.1\n",
        "X_valid, X_test, y_valid, y_test = train_test_split(X_test, y_test, test_size=0.5, random_state=42, stratify=y_test)\n",
        "\n",
        "print(f'X_train.shape: {X_train.shape}')\n",
        "print(f'y_train labels: { {int(k): v for k, v in sorted(Counter(y_train).items())} }\\n')\n",
        "np.save(Path(preprocessed_folder) / f'integrated/train_X_{scaling_method}.npy', X_train)\n",
        "np.save(Path(preprocessed_folder) / f'integrated/train_label_{scaling_method}.npy', y_train)\n",
        "\n",
        "print(f'X_valid.shape: {X_valid.shape}')\n",
        "print(f'y_valid labels: { {int(k): v for k, v in sorted(Counter(y_valid).items())} }\\n')\n",
        "np.save(Path(preprocessed_folder) / f'integrated/valid_X_{scaling_method}.npy', X_valid)\n",
        "np.save(Path(preprocessed_folder) / f'integrated/valid_label_{scaling_method}.npy', y_valid)\n",
        "\n",
        "print(f'X_test.shape: {X_test.shape}')\n",
        "print(f'y_test labels: { {int(k): v for k, v in sorted(Counter(y_test).items())} }\\n')\n",
        "np.save(Path(preprocessed_folder) / f'integrated/test_X_{scaling_method}.npy', X_test)\n",
        "np.save(Path(preprocessed_folder) / f'integrated/test_label_{scaling_method}.npy', y_test)\n",
        "\n",
        "print(f'[{datetime.now().strftime(\"%x %X\")}] ✅ Saved')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pjSbUovFYyYD",
        "outputId": "922e7ee4-bb18-4a15-e7e5-2eb8bf764bff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_file: /content/drive/MyDrive/NYIT/870/datasets/preprocessed/CSE-CIC-IDS2018/integrated/trimed_X_l1pstandard.npy\n",
            "label_file: /content/drive/MyDrive/NYIT/870/datasets/preprocessed/CSE-CIC-IDS2018/integrated/trimed_label_l1pstandard.npy\n",
            "Original X.shape: (4746934, 68)\n",
            "Original labels: {0: 2000000, 1: 286191, 2: 611, 3: 230, 4: 686012, 5: 1730, 6: 576191, 7: 41508, 8: 461912, 9: 139890, 10: 10990, 11: 193354, 12: 160639, 13: 87, 14: 187589}\n",
            "\n",
            "X_train.shape: (3797547, 68)\n",
            "y_train labels: {0: 1600000, 1: 228953, 2: 489, 3: 184, 4: 548809, 5: 1384, 6: 460953, 7: 33206, 8: 369530, 9: 111912, 10: 8792, 11: 154683, 12: 128511, 13: 70, 14: 150071}\n",
            "\n",
            "X_valid.shape: (474693, 68)\n",
            "y_valid labels: {0: 200000, 1: 28619, 2: 61, 3: 23, 4: 68601, 5: 173, 6: 57619, 7: 4151, 8: 46191, 9: 13989, 10: 1099, 11: 19335, 12: 16064, 13: 9, 14: 18759}\n",
            "\n",
            "X_test.shape: (474694, 68)\n",
            "y_test labels: {0: 200000, 1: 28619, 2: 61, 3: 23, 4: 68602, 5: 173, 6: 57619, 7: 4151, 8: 46191, 9: 13989, 10: 1099, 11: 19336, 12: 16064, 13: 8, 14: 18759}\n",
            "\n",
            "[04/20/25 04:57:47] ✅ Saved\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "0717hcCbetqR"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}