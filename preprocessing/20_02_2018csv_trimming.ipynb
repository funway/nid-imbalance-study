{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/funway/nid-imbalance-study/blob/main/preprocessing/20_02_2018csv_trimming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trimming Thuesday-20-02-2018_TrafficForML_CICFlowMeter.csv\n",
        "```\n",
        "Thuesday-20-02-2018_TrafficForML_CICFlowMeter.csv\n",
        "  unique labels: [2], {'Benign': 7372557, 'DDoS attacks-LOIC-HTTP': 576191}\n",
        "```\n",
        "\n",
        "The dataset Thuesday-20-02-2018_TrafficForML_CICFlowMeter.csv is too large (more than 3GB) to be handled by Colab free tier in onetime processing. Also,\n",
        " and the performance of the free version of Colab cannot handle such a large amount of data at once. Also, there is too much **Benign** data, as shown above. Therefore, we can trim the CSV file to retain the attack data and some benign data."
      ],
      "metadata": {
        "id": "4-6lX24fC2WN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CW1uhegIB7va",
        "outputId": "07b6398e-067c-4671-ed1f-102330e70474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "原始文件大小: 3867.08 MB\n",
            "当前 chunk 中 Benign 样本数量: 423809\n",
            "当前 chunk 中 Benign 样本数量: 1000000\n",
            "当前 chunk 中 Benign 样本数量: 1000000\n",
            "当前 chunk 中 Benign 样本数量: 1000000\n",
            "当前 chunk 中 Benign 样本数量: 1000000\n",
            "当前 chunk 中 Benign 样本数量: 1000000\n",
            "当前 chunk 中 Benign 样本数量: 1000000\n",
            "当前 chunk 中 Benign 样本数量: 948748\n",
            "总的 Benign 样本数量: 7372557\n",
            "每个 chunk 中 `Benign` 样本的比例: [0.057484669158882055, 0.1356381510512567, 0.1356381510512567, 0.1356381510512567, 0.1356381510512567, 0.1356381510512567, 0.1356381510512567, 0.1286864245335777]\n",
            "该 chunk 中要抽取的 `Benign` 样本数量: 28742\n",
            "该 chunk 中要抽取的 `Benign` 样本数量: 67819\n",
            "该 chunk 中要抽取的 `Benign` 样本数量: 67819\n",
            "该 chunk 中要抽取的 `Benign` 样本数量: 67819\n",
            "该 chunk 中要抽取的 `Benign` 样本数量: 67819\n",
            "该 chunk 中要抽取的 `Benign` 样本数量: 67819\n",
            "该 chunk 中要抽取的 `Benign` 样本数量: 67819\n",
            "该 chunk 中要抽取的 `Benign` 样本数量: 64343\n",
            "最终数据集中不同标签的样本数量: Label\n",
            "DDoS attacks-LOIC-HTTP    576191\n",
            "Benign                    499999\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "if not os.path.exists('/content/drive/MyDrive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "## 先手工将 csv 文件改名成 .bak 后缀 ##\n",
        "\n",
        "csv_file = '/content/drive/MyDrive/NYIT/870/datasets/CSE-CIC-IDS2018/Thuesday-20-02-2018_TrafficForML_CICFlowMeter.csv'\n",
        "bak_file = csv_file + '.bak'\n",
        "output_file = csv_file\n",
        "\n",
        "file_size = os.path.getsize(bak_file)  # 获取文件大小，单位是字节\n",
        "print(f'原始文件大小: {file_size / (1024 ** 2):.2f} MB')\n",
        "\n",
        "chunksize = 1000000  # 每次读取 100,000 行\n",
        "benign_count_per_chunk = []  # 记录每个 chunk 中 Benign 样本的数量\n",
        "total_benign_count = 0  # 总的 Benign 样本数量\n",
        "\n",
        "# 先分块读取一次，统计每个 chunk 中的 Benign 样本数量\n",
        "for chunk in pd.read_csv(bak_file, chunksize=chunksize):\n",
        "    # 统计 `Benign` 样本数量\n",
        "    benign_data = chunk[chunk['Label'] == 'Benign']\n",
        "    benign_count = benign_data.shape[0]\n",
        "    benign_count_per_chunk.append(benign_count)\n",
        "    total_benign_count += benign_count\n",
        "    print(f'当前 chunk 中 Benign 样本数量: {benign_count}')\n",
        "print(f'总的 Benign 样本数量: {total_benign_count}')\n",
        "\n",
        "final_data = pd.DataFrame()  # 用于存储最终数据\n",
        "target_benign_count = 500000  # 我们希望最终 `Benign` 样本的数量是 500,000\n",
        "benign_sample_ratios = [count / total_benign_count for count in benign_count_per_chunk]  # 每个 chunk 中 `Benign` 样本的比例\n",
        "print(f'每个 chunk 中 `Benign` 样本的比例: {benign_sample_ratios}')\n",
        "\n",
        "# 第二次分块读取，从每个 chunk 中等比例抽取 Benign样本\n",
        "# (但后面发现可能没必要，因为它原始文件中，Benign 样本也不是随机分布的，\n",
        "# 除开第一个 chunk 有非 Benign 数据，剩下的 chunks 几乎100%全是 Benign 数据)\n",
        "for chunk, ratio in zip(pd.read_csv(bak_file, chunksize=chunksize), benign_sample_ratios):\n",
        "    benign_data = chunk[chunk['Label'] == 'Benign']\n",
        "    non_benign_data = chunk[chunk['Label'] != 'Benign']\n",
        "\n",
        "    # 计算该 chunk 中要抽取的 `Benign` 样本数量\n",
        "    required_benign_count = int(target_benign_count * ratio)\n",
        "    print(f'该 chunk 中要抽取的 `Benign` 样本数量: {required_benign_count}')\n",
        "\n",
        "    # 从该 chunk 中抽取 `Benign` 样本\n",
        "    sampled_benign_data = benign_data.sample(n=required_benign_count, random_state=42)\n",
        "    final_data = pd.concat([final_data, sampled_benign_data, non_benign_data])\n",
        "    pass\n",
        "\n",
        "print(f\"最终数据集中不同标签的样本数量: {final_data['Label'].value_counts()}\")\n",
        "final_data.to_csv(output_file, index=False)\n",
        "print(f'最终数据已保存到: {output_file}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print('hello')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KvxFOCj-aw4E",
        "outputId": "7c53df76-baab-4e1c-d4ee-1e358b1fa7c5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hello\n"
          ]
        }
      ]
    }
  ]
}